FEATURE EXTRACTION AND DESCRIPTORS - Identifying and Describing Image Features
=============================================================================

TABLE OF CONTENTS:
1. Feature Detection Fundamentals
2. Corner Detection Methods
3. SIFT (Scale-Invariant Feature Transform)
4. SURF (Speeded-Up Robust Features)
5. HOG (Histogram of Oriented Gradients)
6. LBP (Local Binary Patterns)
7. Advanced Feature Descriptors
8. Implementation and Practical Guidelines

=======================================================

1. FEATURE DETECTION FUNDAMENTALS
=================================

1.1 What are Image Features:
---------------------------
Definition:
Distinctive image regions with specific properties
Corners, edges, blobs, textures

Feature Types:
- Point features: Corners, interest points
- Edge features: Boundaries, contours
- Region features: Blobs, uniform areas
- Texture features: Patterns, surfaces

Desirable Properties:
- Repeatability: Detected consistently
- Distinctiveness: Unique appearance
- Locality: Robust to occlusion
- Accuracy: Precise localization
- Efficiency: Fast computation

```python
import numpy as np
import cv2
from scipy import ndimage
from skimage import feature, filters
import matplotlib.pyplot as plt

def create_test_image():
    """Create test image with various features"""
    image = np.zeros((200, 200), dtype=np.uint8)
    
    # Add corners
    cv2.rectangle(image, (50, 50), (100, 100), 255, 2)
    cv2.rectangle(image, (120, 30), (180, 90), 128, -1)
    
    # Add circular feature
    cv2.circle(image, (150, 150), 20, 255, -1)
    
    # Add texture pattern
    for i in range(10, 40, 5):
        cv2.line(image, (10, i), (40, i), 200, 1)
    
    return image

# Create and display test image
test_img = create_test_image()
print(f"Test image shape: {test_img.shape}")
print(f"Intensity range: {test_img.min()} to {test_img.max()}")
```

1.2 Feature Detection Pipeline:
------------------------------
Steps:
1. Preprocessing (smoothing, normalization)
2. Interest point detection
3. Feature description
4. Feature matching (if needed)

Detection vs Description:
Detection: WHERE are the features?
Description: WHAT do the features look like?

Evaluation Metrics:
- Repeatability: Percentage of detected features
- Precision: Correct detections / Total detections
- Recall: Correct detections / Ground truth features
- Accuracy: Localization precision

1.3 Multi-Scale Analysis:
------------------------
Scale Space:
Analyze features at multiple scales
Gaussian scale space: L(x,y,σ) = G(x,y,σ) * I(x,y)

Scale Selection:
Choose characteristic scale for each feature
Maximum response across scales

Octave Sampling:
Efficient scale space sampling
Reduce image size by factor of 2 per octave

```python
def build_scale_space(image, num_octaves=4, scales_per_octave=3, sigma_base=1.6):
    """Build Gaussian scale space"""
    scale_space = []
    
    current_image = image.astype(np.float32)
    
    for octave in range(num_octaves):
        octave_images = []
        
        for scale in range(scales_per_octave):
            # Calculate sigma for this scale
            sigma = sigma_base * (2 ** (scale / scales_per_octave))
            
            # Apply Gaussian smoothing
            smoothed = ndimage.gaussian_filter(current_image, sigma=sigma)
            octave_images.append(smoothed)
        
        scale_space.append(octave_images)
        
        # Downsample for next octave
        current_image = current_image[::2, ::2]
    
    return scale_space

# Build scale space for test image
scale_space = build_scale_space(test_img)
print(f"Scale space levels: {len(scale_space)}")
for i, octave in enumerate(scale_space):
    print(f"Octave {i}: {len(octave)} scales, image size: {octave[0].shape}")
```

=======================================================

2. CORNER DETECTION METHODS
===========================

2.1 Harris Corner Detector:
---------------------------
Mathematical Foundation:
Corner detection using structure tensor
E(u,v) = Σ w(x,y)[I(x+u,y+v) - I(x,y)]²

Structure Tensor:
M = Σ w(x,y) [Ix²    IxIy]
                [IxIy   Iy²]

Harris Response:
R = det(M) - k(trace(M))²
where k ≈ 0.04-0.06

```python
def harris_corner_detection(image, k=0.04, threshold=0.01):
    """Implement Harris corner detection"""
    # Convert to float
    image = image.astype(np.float32)
    
    # Compute image gradients
    Ix = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)
    Iy = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)
    
    # Compute products of derivatives
    Ixx = Ix * Ix
    Iyy = Iy * Iy
    Ixy = Ix * Iy
    
    # Apply Gaussian window
    sigma = 2.0
    Ixx = ndimage.gaussian_filter(Ixx, sigma=sigma)
    Iyy = ndimage.gaussian_filter(Iyy, sigma=sigma)
    Ixy = ndimage.gaussian_filter(Ixy, sigma=sigma)
    
    # Compute Harris response
    det_M = Ixx * Iyy - Ixy * Ixy
    trace_M = Ixx + Iyy
    
    R = det_M - k * (trace_M ** 2)
    
    # Apply threshold
    corners = R > threshold * R.max()
    
    return R, corners

# Test Harris corner detection
harris_response, harris_corners = harris_corner_detection(test_img)

# Find corner coordinates
corner_coords = np.where(harris_corners)
print(f"Harris corners detected: {len(corner_coords[0])}")
print(f"Harris response range: {harris_response.min():.6f} to {harris_response.max():.6f}")
```

2.2 Shi-Tomasi Corner Detector:
------------------------------
Modified Harris Detector:
R = min(λ₁, λ₂)
where λ₁, λ₂ are eigenvalues of M

Better Corner Selection:
More stable than Harris
Good for tracking applications

```python
def shi_tomasi_corner_detection(image, threshold=0.01):
    """Implement Shi-Tomasi corner detection"""
    image = image.astype(np.float32)
    
    # Compute gradients
    Ix = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)
    Iy = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)
    
    # Structure tensor components
    Ixx = ndimage.gaussian_filter(Ix * Ix, sigma=2.0)
    Iyy = ndimage.gaussian_filter(Iy * Iy, sigma=2.0)
    Ixy = ndimage.gaussian_filter(Ix * Iy, sigma=2.0)
    
    # Compute eigenvalues
    rows, cols = image.shape
    R = np.zeros((rows, cols))
    
    for i in range(rows):
        for j in range(cols):
            M = np.array([[Ixx[i,j], Ixy[i,j]], 
                         [Ixy[i,j], Iyy[i,j]]])
            
            eigenvals = np.linalg.eigvals(M)
            R[i,j] = min(eigenvals)
    
    # Apply threshold
    corners = R > threshold * R.max()
    
    return R, corners

# Test Shi-Tomasi detector
st_response, st_corners = shi_tomasi_corner_detection(test_img)
st_coords = np.where(st_corners)
print(f"Shi-Tomasi corners detected: {len(st_coords[0])}")
```

2.3 FAST (Features from Accelerated Segment Test):
-------------------------------------------------
Speed-Optimized Detection:
Compare pixel with circle of 16 pixels
Threshold-based decision

Algorithm:
1. Select pixel p with intensity Ip
2. Consider circle of 16 pixels around p
3. Count pixels with |I - Ip| > threshold
4. If ≥12 consecutive pixels → corner

```python
def fast_corner_detection(image, threshold=20, n_consecutive=12):
    """Simplified FAST corner detection"""
    
    # Circle offsets for 16 pixels (Bresenham circle)
    circle_offsets = [
        (0, 3), (1, 3), (2, 2), (3, 1), (3, 0), (3, -1), (2, -2), (1, -3),
        (0, -3), (-1, -3), (-2, -2), (-3, -1), (-3, 0), (-3, 1), (-2, 2), (-1, 3)
    ]
    
    rows, cols = image.shape
    corners = np.zeros((rows, cols), dtype=bool)
    
    # Check each pixel (avoiding borders)
    for i in range(3, rows - 3):
        for j in range(3, cols - 3):
            center_intensity = image[i, j]
            
            # Get circle pixel intensities
            circle_pixels = []
            for dy, dx in circle_offsets:
                if 0 <= i + dy < rows and 0 <= j + dx < cols:
                    circle_pixels.append(image[i + dy, j + dx])
                else:
                    circle_pixels.append(center_intensity)  # Boundary handling
            
            # Check for consecutive pixels above/below threshold
            brighter = [p > center_intensity + threshold for p in circle_pixels]
            darker = [p < center_intensity - threshold for p in circle_pixels]
            
            # Count consecutive occurrences
            def count_consecutive(binary_list):
                max_consecutive = 0
                current_consecutive = 0
                
                # Check twice to handle circular nature
                for value in binary_list + binary_list:
                    if value:
                        current_consecutive += 1
                        max_consecutive = max(max_consecutive, current_consecutive)
                    else:
                        current_consecutive = 0
                
                return max_consecutive
            
            if (count_consecutive(brighter) >= n_consecutive or 
                count_consecutive(darker) >= n_consecutive):
                corners[i, j] = True
    
    return corners

# Test FAST detection
fast_corners = fast_corner_detection(test_img, threshold=30)
fast_coords = np.where(fast_corners)
print(f"FAST corners detected: {len(fast_coords[0])}")
```

=======================================================

3. SIFT (SCALE-INVARIANT FEATURE TRANSFORM)
===========================================

3.1 SIFT Algorithm Overview:
---------------------------
Four Main Steps:
1. Scale-space extrema detection
2. Keypoint localization
3. Orientation assignment
4. Keypoint descriptor

Key Properties:
- Scale invariant
- Rotation invariant
- Partially illumination invariant
- Robust to viewpoint changes

3.2 Scale-Space Extrema Detection:
---------------------------------
Difference of Gaussians (DoG):
D(x,y,σ) = (G(x,y,kσ) - G(x,y,σ)) * I(x,y)

Extrema Detection:
Compare each pixel with 26 neighbors
(8 in current scale + 9 in scale above + 9 in scale below)

```python
def detect_dog_extrema(scale_space, contrast_threshold=0.03):
    """Detect extrema in Difference of Gaussians"""
    extrema_points = []
    
    for octave_idx, octave in enumerate(scale_space):
        # Compute DoG images
        dog_images = []
        for i in range(len(octave) - 1):
            dog = octave[i+1] - octave[i]
            dog_images.append(dog)
        
        # Find extrema in DoG images
        for scale_idx in range(1, len(dog_images) - 1):
            current = dog_images[scale_idx]
            above = dog_images[scale_idx + 1]
            below = dog_images[scale_idx - 1]
            
            rows, cols = current.shape
            
            for i in range(1, rows - 1):
                for j in range(1, cols - 1):
                    pixel_val = current[i, j]
                    
                    # Check if extremum
                    if abs(pixel_val) < contrast_threshold:
                        continue
                    
                    # Compare with 26 neighbors
                    neighbors = []
                    
                    # Current scale neighbors
                    for di in [-1, 0, 1]:
                        for dj in [-1, 0, 1]:
                            if di != 0 or dj != 0:
                                neighbors.append(current[i+di, j+dj])
                    
                    # Above and below scale neighbors
                    for di in [-1, 0, 1]:
                        for dj in [-1, 0, 1]:
                            neighbors.append(above[i+di, j+dj])
                            neighbors.append(below[i+di, j+dj])
                    
                    # Check if maximum or minimum
                    is_max = pixel_val > max(neighbors)
                    is_min = pixel_val < min(neighbors)
                    
                    if is_max or is_min:
                        extrema_points.append({
                            'octave': octave_idx,
                            'scale': scale_idx,
                            'x': j,
                            'y': i,
                            'response': pixel_val
                        })
    
    return extrema_points

# Detect SIFT keypoints
sift_keypoints = detect_dog_extrema(scale_space)
print(f"SIFT keypoints detected: {len(sift_keypoints)}")
```

3.3 Orientation Assignment:
--------------------------
Gradient Computation:
m(x,y) = √[(L(x+1,y) - L(x-1,y))² + (L(x,y+1) - L(x,y-1))²]
θ(x,y) = atan2(L(x,y+1) - L(x,y-1), L(x+1,y) - L(x-1,y))

Orientation Histogram:
36 bins covering 360°
Weighted by gradient magnitude and Gaussian window

```python
def compute_keypoint_orientations(image, keypoints, sigma_factor=1.5):
    """Compute dominant orientations for keypoints"""
    oriented_keypoints = []
    
    for kp in keypoints:
        x, y = kp['x'], kp['y']
        scale = kp['scale']
        
        # Calculate window size
        sigma = sigma_factor * (2 ** (scale / 3.0))
        radius = int(3 * sigma)
        
        # Check bounds
        if (x - radius < 0 or x + radius >= image.shape[1] or
            y - radius < 0 or y + radius >= image.shape[0]):
            continue
        
        # Extract patch
        patch = image[y-radius:y+radius+1, x-radius:x+radius+1]
        
        # Compute gradients
        gx = cv2.Sobel(patch, cv2.CV_32F, 1, 0, ksize=3)
        gy = cv2.Sobel(patch, cv2.CV_32F, 0, 1, ksize=3)
        
        magnitude = np.sqrt(gx**2 + gy**2)
        orientation = np.arctan2(gy, gx) * 180 / np.pi
        orientation[orientation < 0] += 360
        
        # Create Gaussian weight
        center = radius
        y_grid, x_grid = np.ogrid[:patch.shape[0], :patch.shape[1]]
        gaussian_weight = np.exp(-((x_grid - center)**2 + (y_grid - center)**2) / (2 * sigma**2))
        
        # Weighted magnitude
        weighted_magnitude = magnitude * gaussian_weight
        
        # Build orientation histogram (36 bins)
        hist_bins = 36
        hist = np.zeros(hist_bins)
        
        for i in range(patch.shape[0]):
            for j in range(patch.shape[1]):
                angle = orientation[i, j]
                bin_idx = int(angle // (360 / hist_bins)) % hist_bins
                hist[bin_idx] += weighted_magnitude[i, j]
        
        # Find dominant orientations (peaks above 80% of maximum)
        max_val = np.max(hist)
        peak_threshold = 0.8 * max_val
        
        for bin_idx in range(hist_bins):
            if hist[bin_idx] >= peak_threshold:
                # Refined orientation using parabolic interpolation
                prev_bin = (bin_idx - 1) % hist_bins
                next_bin = (bin_idx + 1) % hist_bins
                
                refined_angle = bin_idx + 0.5 * (hist[prev_bin] - hist[next_bin]) / \
                               (hist[prev_bin] - 2*hist[bin_idx] + hist[next_bin])
                
                refined_angle = refined_angle * (360 / hist_bins)
                if refined_angle < 0:
                    refined_angle += 360
                
                # Create new keypoint with orientation
                oriented_kp = kp.copy()
                oriented_kp['orientation'] = refined_angle
                oriented_keypoints.append(oriented_kp)
    
    return oriented_keypoints

# Compute orientations for SIFT keypoints
if sift_keypoints:
    oriented_keypoints = compute_keypoint_orientations(test_img.astype(np.float32), sift_keypoints[:5])  # Test with first 5
    print(f"Oriented keypoints: {len(oriented_keypoints)}")
    for kp in oriented_keypoints[:3]:
        print(f"Keypoint at ({kp['x']}, {kp['y']}) with orientation {kp['orientation']:.1f}°")
```

3.4 SIFT Descriptor Computation:
-------------------------------
16×16 Neighborhood:
Divided into 4×4 subregions
8-bin orientation histogram per subregion
Total: 4×4×8 = 128-dimensional descriptor

Trilinear Interpolation:
Smooth contribution to neighboring bins
Reduces aliasing effects

```python
def compute_sift_descriptor(image, keypoint, descriptor_size=16, hist_bins=8):
    """Compute 128-dimensional SIFT descriptor"""
    x, y = keypoint['x'], keypoint['y']
    orientation = keypoint.get('orientation', 0)
    scale = keypoint['scale']
    
    # Calculate sampling parameters
    sigma = scale * 2.0
    radius = int(descriptor_size / 2 * np.sqrt(2) * sigma)
    
    # Check bounds
    if (x - radius < 0 or x + radius >= image.shape[1] or
        y - radius < 0 or y + radius >= image.shape[0]):
        return None
    
    # Extract larger patch
    patch = image[y-radius:y+radius+1, x-radius:x+radius+1].astype(np.float32)
    
    # Rotate patch to align with dominant orientation
    angle_rad = -orientation * np.pi / 180
    cos_angle, sin_angle = np.cos(angle_rad), np.sin(angle_rad)
    
    # Compute gradients
    gx = cv2.Sobel(patch, cv2.CV_32F, 1, 0, ksize=3)
    gy = cv2.Sobel(patch, cv2.CV_32F, 0, 1, ksize=3)
    
    magnitude = np.sqrt(gx**2 + gy**2)
    angle = np.arctan2(gy, gx) * 180 / np.pi
    
    # Adjust angles relative to keypoint orientation
    angle = angle - orientation
    angle[angle < 0] += 360
    
    # Initialize descriptor
    descriptor = np.zeros((4, 4, hist_bins))
    
    # Sample points in 16x16 grid
    center = radius
    step = descriptor_size / 4
    
    for i in range(descriptor_size):
        for j in range(descriptor_size):
            # Original coordinates in patch
            patch_y = center + (i - descriptor_size/2) * sigma
            patch_x = center + (j - descriptor_size/2) * sigma
            
            # Check bounds
            if (patch_y < 1 or patch_y >= patch.shape[0] - 1 or
                patch_x < 1 or patch_x >= patch.shape[1] - 1):
                continue
            
            # Bilinear interpolation for gradients
            y_int, x_int = int(patch_y), int(patch_x)
            dy, dx = patch_y - y_int, patch_x - x_int
            
            # Interpolated magnitude and angle
            mag_interp = (magnitude[y_int, x_int] * (1-dy) * (1-dx) +
                         magnitude[y_int+1, x_int] * dy * (1-dx) +
                         magnitude[y_int, x_int+1] * (1-dy) * dx +
                         magnitude[y_int+1, x_int+1] * dy * dx)
            
            ang_interp = (angle[y_int, x_int] * (1-dy) * (1-dx) +
                         angle[y_int+1, x_int] * dy * (1-dx) +
                         angle[y_int, x_int+1] * (1-dy) * dx +
                         angle[y_int+1, x_int+1] * dy * dx)
            
            # Determine which 4x4 subregion
            region_y = int((i) / step)
            region_x = int((j) / step)
            region_y = min(region_y, 3)
            region_x = min(region_x, 3)
            
            # Determine orientation bin
            bin_idx = int(ang_interp / (360 / hist_bins)) % hist_bins
            
            # Add to histogram with Gaussian weighting
            gaussian_weight = np.exp(-((i - descriptor_size/2)**2 + (j - descriptor_size/2)**2) / 
                                   (2 * (descriptor_size/2)**2))
            
            descriptor[region_y, region_x, bin_idx] += mag_interp * gaussian_weight
    
    # Flatten to 128D vector
    descriptor_vector = descriptor.flatten()
    
    # Normalize and clip
    descriptor_vector = descriptor_vector / (np.linalg.norm(descriptor_vector) + 1e-8)
    descriptor_vector = np.clip(descriptor_vector, 0, 0.2)
    descriptor_vector = descriptor_vector / (np.linalg.norm(descriptor_vector) + 1e-8)
    
    return descriptor_vector

# Compute SIFT descriptors
if oriented_keypoints:
    sift_descriptors = []
    for kp in oriented_keypoints:
        desc = compute_sift_descriptor(test_img.astype(np.float32), kp)
        if desc is not None:
            sift_descriptors.append(desc)
    
    print(f"SIFT descriptors computed: {len(sift_descriptors)}")
    if sift_descriptors:
        print(f"Descriptor dimension: {len(sift_descriptors[0])}")
        print(f"First descriptor norm: {np.linalg.norm(sift_descriptors[0]):.6f}")
```

=======================================================

4. SURF (SPEEDED-UP ROBUST FEATURES)
====================================

4.1 SURF Algorithm Overview:
---------------------------
Key Improvements over SIFT:
- Faster computation using integral images
- Hessian-based detector
- Simplified descriptor

Integral Image:
II(x,y) = Σᵢ₌₀ˣ Σⱼ₌₀ʸ I(i,j)
Enables fast box filter computation

4.2 Hessian-Based Detection:
---------------------------
Hessian Matrix:
H = [Lxx  Lxy]
    [Lxy  Lyy]

Approximation with Box Filters:
Faster computation using integral images
Good approximation to Gaussian derivatives

```python
def compute_integral_image(image):
    """Compute integral image for fast box filtering"""
    return np.cumsum(np.cumsum(image, axis=0), axis=1)

def box_filter(integral_img, x, y, width, height):
    """Fast box filter using integral image"""
    x, y = int(x), int(y)
    x1, y1 = x, y
    x2, y2 = min(x + width, integral_img.shape[1] - 1), min(y + height, integral_img.shape[0] - 1)
    
    # Handle boundaries
    x1, y1 = max(0, x1), max(0, y1)
    
    area = (x2 - x1) * (y2 - y1)
    if area == 0:
        return 0
    
    # Sum using integral image
    sum_val = (integral_img[y2, x2] - 
               (integral_img[y1-1, x2] if y1 > 0 else 0) -
               (integral_img[y2, x1-1] if x1 > 0 else 0) +
               (integral_img[y1-1, x1-1] if y1 > 0 and x1 > 0 else 0))
    
    return sum_val / area

def surf_hessian_response(image, scale=9):
    """Compute SURF Hessian response using box filters"""
    integral_img = compute_integral_image(image.astype(np.float64))
    rows, cols = image.shape
    
    # Box filter sizes for approximating Gaussian derivatives
    # at given scale
    lobe_size = scale // 3
    
    hessian_response = np.zeros((rows, cols))
    
    for y in range(lobe_size, rows - lobe_size):
        for x in range(lobe_size, cols - lobe_size):
            # Dxx approximation
            dxx = (box_filter(integral_img, x - lobe_size, y - lobe_size//2, lobe_size, lobe_size) -
                   2 * box_filter(integral_img, x, y - lobe_size//2, lobe_size, lobe_size) +
                   box_filter(integral_img, x + lobe_size, y - lobe_size//2, lobe_size, lobe_size))
            
            # Dyy approximation
            dyy = (box_filter(integral_img, x - lobe_size//2, y - lobe_size, lobe_size, lobe_size) -
                   2 * box_filter(integral_img, x - lobe_size//2, y, lobe_size, lobe_size) +
                   box_filter(integral_img, x - lobe_size//2, y + lobe_size, lobe_size, lobe_size))
            
            # Dxy approximation
            dxy = (box_filter(integral_img, x - lobe_size//2, y - lobe_size//2, lobe_size//2, lobe_size//2) +
                   box_filter(integral_img, x + lobe_size//2, y + lobe_size//2, lobe_size//2, lobe_size//2) -
                   box_filter(integral_img, x - lobe_size//2, y + lobe_size//2, lobe_size//2, lobe_size//2) -
                   box_filter(integral_img, x + lobe_size//2, y - lobe_size//2, lobe_size//2, lobe_size//2))
            
            # Hessian determinant
            det_hessian = dxx * dyy - (0.9 * dxy)**2  # 0.9 is approximation factor
            hessian_response[y, x] = det_hessian
    
    return hessian_response

# Test SURF detection
surf_response = surf_hessian_response(test_img)
surf_threshold = 0.1 * surf_response.max()
surf_keypoints = surf_response > surf_threshold

surf_coords = np.where(surf_keypoints)
print(f"SURF keypoints detected: {len(surf_coords[0])}")
print(f"SURF response range: {surf_response.min():.2f} to {surf_response.max():.2f}")
```

4.3 SURF Descriptor:
-------------------
Wavelet Responses:
Haar wavelet responses in x and y directions
Sum of responses in 4×4 subregions

Descriptor Vector:
64-dimensional (4×4×4 values per subregion)
Σdx, Σdy, Σ|dx|, Σ|dy| for each subregion

```python
def compute_surf_descriptor(image, x, y, orientation=0, size=20):
    """Compute 64-dimensional SURF descriptor"""
    
    # Create sampling grid
    grid_spacing = size / 4
    samples = []
    
    # Collect Haar wavelet responses
    for i in range(4):
        for j in range(4):
            # Center of current subregion
            subregion_x = x + (i - 1.5) * grid_spacing
            subregion_y = y + (j - 1.5) * grid_spacing
            
            # Initialize sums for this subregion
            dx_sum, dy_sum = 0, 0
            abs_dx_sum, abs_dy_sum = 0, 0
            
            # Sample points within subregion
            for si in range(int(grid_spacing)):
                for sj in range(int(grid_spacing)):
                    sample_x = int(subregion_x + si)
                    sample_y = int(subregion_y + sj)
                    
                    # Check bounds
                    if (sample_x < 1 or sample_x >= image.shape[1] - 1 or
                        sample_y < 1 or sample_y >= image.shape[0] - 1):
                        continue
                    
                    # Haar wavelet responses (simplified)
                    dx = (image[sample_y, sample_x + 1] - image[sample_y, sample_x - 1]) / 2
                    dy = (image[sample_y + 1, sample_x] - image[sample_y - 1, sample_x]) / 2
                    
                    # Accumulate responses
                    dx_sum += dx
                    dy_sum += dy
                    abs_dx_sum += abs(dx)
                    abs_dy_sum += abs(dy)
            
            # Add to descriptor
            samples.extend([dx_sum, dy_sum, abs_dx_sum, abs_dy_sum])
    
    # Normalize descriptor
    descriptor = np.array(samples)
    norm = np.linalg.norm(descriptor)
    if norm > 0:
        descriptor = descriptor / norm
    
    return descriptor

# Compute SURF descriptors for detected keypoints
surf_descriptors = []
if len(surf_coords[0]) > 0:
    # Take first few keypoints
    for i in range(min(5, len(surf_coords[0]))):
        y, x = surf_coords[0][i], surf_coords[1][i]
        desc = compute_surf_descriptor(test_img.astype(np.float32), x, y)
        surf_descriptors.append(desc)

print(f"SURF descriptors computed: {len(surf_descriptors)}")
if surf_descriptors:
    print(f"SURF descriptor dimension: {len(surf_descriptors[0])}")
```

=======================================================

5. HOG (HISTOGRAM OF ORIENTED GRADIENTS)
========================================

5.1 HOG Algorithm Overview:
--------------------------
Purpose:
Dense feature descriptor for object detection
Particularly effective for pedestrians

Key Idea:
Local object appearance characterized by distribution of gradient orientations
Computed over dense grid of cells

Pipeline:
1. Gradient computation
2. Cell histograms
3. Block normalization
4. Feature concatenation

```python
def compute_hog_features(image, cell_size=(8, 8), block_size=(2, 2), num_bins=9):
    """Compute HOG features for entire image"""
    
    # Convert to float
    image = image.astype(np.float32)
    
    # Compute gradients
    gx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=1)
    gy = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=1)
    
    magnitude = np.sqrt(gx**2 + gy**2)
    orientation = np.arctan2(gy, gx) * 180 / np.pi
    
    # Convert to unsigned orientations (0-180 degrees)
    orientation[orientation < 0] += 180
    
    rows, cols = image.shape
    cell_rows = rows // cell_size[0]
    cell_cols = cols // cell_size[1]
    
    # Compute cell histograms
    cell_histograms = np.zeros((cell_rows, cell_cols, num_bins))
    
    for i in range(cell_rows):
        for j in range(cell_cols):
            # Extract cell
            start_row = i * cell_size[0]
            end_row = start_row + cell_size[0]
            start_col = j * cell_size[1]
            end_col = start_col + cell_size[1]
            
            cell_mag = magnitude[start_row:end_row, start_col:end_col]
            cell_ori = orientation[start_row:end_row, start_col:end_col]
            
            # Build histogram for this cell
            for y in range(cell_size[0]):
                for x in range(cell_size[1]):
                    mag_val = cell_mag[y, x]
                    ori_val = cell_ori[y, x]
                    
                    # Bilinear interpolation between bins
                    bin_idx = ori_val / (180 / num_bins)
                    bin_left = int(bin_idx) % num_bins
                    bin_right = (bin_left + 1) % num_bins
                    
                    weight_right = bin_idx - int(bin_idx)
                    weight_left = 1 - weight_right
                    
                    cell_histograms[i, j, bin_left] += mag_val * weight_left
                    cell_histograms[i, j, bin_right] += mag_val * weight_right
    
    # Block normalization
    block_rows = cell_rows - block_size[0] + 1
    block_cols = cell_cols - block_size[1] + 1
    
    hog_features = []
    
    for i in range(block_rows):
        for j in range(block_cols):
            # Extract block
            block_hist = cell_histograms[i:i+block_size[0], j:j+block_size[1], :]
            
            # Flatten and normalize
            block_vector = block_hist.flatten()
            
            # L2 normalization with small epsilon
            norm = np.sqrt(np.sum(block_vector**2) + 1e-6)
            normalized_block = block_vector / norm
            
            hog_features.extend(normalized_block)
    
    return np.array(hog_features)

# Compute HOG features for test image
hog_descriptor = compute_hog_features(test_img)
print(f"HOG descriptor dimension: {len(hog_descriptor)}")
print(f"HOG descriptor range: {hog_descriptor.min():.6f} to {hog_descriptor.max():.6f}")

def visualize_hog_cells(image, cell_size=(8, 8), num_bins=9):
    """Visualize HOG cell orientations"""
    # Compute gradients
    gx = cv2.Sobel(image.astype(np.float32), cv2.CV_32F, 1, 0, ksize=1)
    gy = cv2.Sobel(image.astype(np.float32), cv2.CV_32F, 0, 1, ksize=1)
    
    magnitude = np.sqrt(gx**2 + gy**2)
    orientation = np.arctan2(gy, gx)
    
    rows, cols = image.shape
    cell_rows = rows // cell_size[0]
    cell_cols = cols // cell_size[1]
    
    # Create visualization image
    vis_image = np.zeros((rows, cols, 3), dtype=np.uint8)
    vis_image[:, :, 0] = image  # Use original image as background
    
    # Draw dominant orientations for each cell
    for i in range(cell_rows):
        for j in range(cell_cols):
            start_row = i * cell_size[0]
            end_row = start_row + cell_size[0]
            start_col = j * cell_size[1]
            end_col = start_col + cell_size[1]
            
            cell_mag = magnitude[start_row:end_row, start_col:end_col]
            cell_ori = orientation[start_row:end_row, start_col:end_col]
            
            # Find dominant orientation
            mean_ori = np.arctan2(np.mean(np.sin(cell_ori) * cell_mag), 
                                 np.mean(np.cos(cell_ori) * cell_mag))
            mean_mag = np.mean(cell_mag)
            
            if mean_mag > 10:  # Only draw if significant gradient
                center_y = start_row + cell_size[0] // 2
                center_x = start_col + cell_size[1] // 2
                
                length = min(cell_size) // 2
                end_x = int(center_x + length * np.cos(mean_ori))
                end_y = int(center_y + length * np.sin(mean_ori))
                
                cv2.line(vis_image, (center_x, center_y), (end_x, end_y), (0, 255, 0), 1)
    
    return vis_image

# Visualize HOG cells
hog_visualization = visualize_hog_cells(test_img)
print("HOG visualization created")
```

=======================================================

6. LBP (LOCAL BINARY PATTERNS)
==============================

6.1 LBP Algorithm:
-----------------
Basic Principle:
Compare center pixel with neighbors
Create binary pattern from comparisons

Original LBP:
8 neighbors in 3×3 neighborhood
8-bit binary code

Extended LBP:
Variable radius and number of neighbors
LBP(P,R) where P=neighbors, R=radius

```python
def compute_lbp(image, radius=1, n_points=8, method='uniform'):
    """Compute Local Binary Patterns"""
    
    def get_circle_coordinates(radius, n_points):
        """Get coordinates of neighbors on circle"""
        angles = 2 * np.pi * np.arange(n_points) / n_points
        x_coords = radius * np.cos(angles)
        y_coords = radius * np.sin(angles)
        return x_coords, y_coords
    
    def bilinear_interpolation(image, x, y):
        """Bilinear interpolation for non-integer coordinates"""
        x1, y1 = int(x), int(y)
        x2, y2 = x1 + 1, y1 + 1
        
        # Handle boundaries
        if x1 < 0 or x2 >= image.shape[1] or y1 < 0 or y2 >= image.shape[0]:
            return 0
        
        # Weights
        wa = (x2 - x) * (y2 - y)
        wb = (x - x1) * (y2 - y)
        wc = (x2 - x) * (y - y1)
        wd = (x - x1) * (y - y1)
        
        return (wa * image[y1, x1] + wb * image[y1, x2] + 
                wc * image[y2, x1] + wd * image[y2, x2])
    
    # Get neighbor coordinates
    x_coords, y_coords = get_circle_coordinates(radius, n_points)
    
    rows, cols = image.shape
    lbp_image = np.zeros((rows, cols), dtype=np.uint32)
    
    for i in range(radius, rows - radius):
        for j in range(radius, cols - radius):
            center_pixel = image[i, j]
            
            # Initialize binary pattern
            binary_pattern = 0
            
            for k in range(n_points):
                # Neighbor coordinates
                neighbor_x = j + x_coords[k]
                neighbor_y = i + y_coords[k]
                
                # Get neighbor value with interpolation
                neighbor_value = bilinear_interpolation(image, neighbor_x, neighbor_y)
                
                # Compare with center
                if neighbor_value >= center_pixel:
                    binary_pattern |= (1 << k)
            
            lbp_image[i, j] = binary_pattern
    
    # Apply uniform pattern mapping if requested
    if method == 'uniform':
        lbp_image = uniform_pattern_mapping(lbp_image, n_points)
    
    return lbp_image

def uniform_pattern_mapping(lbp_image, n_points):
    """Map to uniform patterns only"""
    
    def is_uniform_pattern(pattern, n_points):
        """Check if pattern is uniform (at most 2 bit transitions)"""
        binary_str = format(pattern, f'0{n_points}b')
        transitions = 0
        
        for i in range(n_points):
            if binary_str[i] != binary_str[(i + 1) % n_points]:
                transitions += 1
        
        return transitions <= 2
    
    # Create mapping table
    uniform_patterns = []
    mapping = np.zeros(2**n_points, dtype=np.uint32)
    
    for pattern in range(2**n_points):
        if is_uniform_pattern(pattern, n_points):
            mapping[pattern] = len(uniform_patterns)
            uniform_patterns.append(pattern)
        else:
            mapping[pattern] = len(uniform_patterns)  # Non-uniform bin
    
    # Apply mapping
    mapped_image = np.zeros_like(lbp_image)
    for i in range(lbp_image.shape[0]):
        for j in range(lbp_image.shape[1]):
            mapped_image[i, j] = mapping[lbp_image[i, j]]
    
    return mapped_image

def compute_lbp_histogram(lbp_image, n_bins=None):
    """Compute histogram of LBP values"""
    if n_bins is None:
        n_bins = int(lbp_image.max()) + 1
    
    histogram = np.histogram(lbp_image, bins=n_bins, range=(0, n_bins))[0]
    
    # Normalize
    histogram = histogram.astype(np.float32)
    histogram = histogram / (np.sum(histogram) + 1e-8)
    
    return histogram

# Compute LBP for test image
lbp_result = compute_lbp(test_img.astype(np.float32), radius=1, n_points=8, method='uniform')
lbp_histogram = compute_lbp_histogram(lbp_result)

print(f"LBP image shape: {lbp_result.shape}")
print(f"LBP value range: {lbp_result.min()} to {lbp_result.max()}")
print(f"LBP histogram dimension: {len(lbp_histogram)}")
print(f"LBP histogram sum: {np.sum(lbp_histogram):.6f}")

# Multi-scale LBP
def compute_multiscale_lbp(image, radii=[1, 2, 3], n_points=8):
    """Compute LBP at multiple scales"""
    multiscale_features = []
    
    for radius in radii:
        lbp = compute_lbp(image, radius=radius, n_points=n_points, method='uniform')
        histogram = compute_lbp_histogram(lbp)
        multiscale_features.extend(histogram)
    
    return np.array(multiscale_features)

# Compute multi-scale LBP
multiscale_lbp = compute_multiscale_lbp(test_img.astype(np.float32))
print(f"Multi-scale LBP dimension: {len(multiscale_lbp)}")
```

=======================================================

7. ADVANCED FEATURE DESCRIPTORS
===============================

7.1 BRIEF (Binary Robust Independent Elementary Features):
---------------------------------------------------------
Binary Descriptor:
Fast to compute and match
128, 256, or 512-bit descriptors

Random Sampling:
Random pairs of pixels in patch
Binary test: I(p) > I(q)

```python
def compute_brief_descriptor(image, keypoints, descriptor_length=256, patch_size=31):
    """Compute BRIEF binary descriptors"""
    
    # Pre-compute random sampling pairs
    np.random.seed(42)  # For reproducibility
    half_patch = patch_size // 2
    
    # Generate random pairs
    pairs = []
    for _ in range(descriptor_length):
        p1 = (np.random.randint(-half_patch, half_patch+1), 
              np.random.randint(-half_patch, half_patch+1))
        p2 = (np.random.randint(-half_patch, half_patch+1), 
              np.random.randint(-half_patch, half_patch+1))
        pairs.append((p1, p2))
    
    descriptors = []
    
    for kp in keypoints:
        x, y = int(kp['x']), int(kp['y'])
        
        # Check if patch fits in image
        if (x - half_patch < 0 or x + half_patch >= image.shape[1] or
            y - half_patch < 0 or y + half_patch >= image.shape[0]):
            continue
        
        # Extract patch
        patch = image[y-half_patch:y+half_patch+1, x-half_patch:x+half_patch+1]
        
        # Smooth patch
        smoothed_patch = ndimage.gaussian_filter(patch.astype(np.float32), sigma=2.0)
        
        # Compute binary descriptor
        descriptor = 0
        
        for i, (p1, p2) in enumerate(pairs):
            y1, x1 = half_patch + p1[1], half_patch + p1[0]
            y2, x2 = half_patch + p2[1], half_patch + p2[0]
            
            if smoothed_patch[y1, x1] > smoothed_patch[y2, x2]:
                descriptor |= (1 << i)
        
        descriptors.append(descriptor)
    
    return descriptors

# Compute BRIEF descriptors
if sift_keypoints:
    brief_descriptors = compute_brief_descriptor(test_img, sift_keypoints[:5])
    print(f"BRIEF descriptors computed: {len(brief_descriptors)}")
    if brief_descriptors:
        print(f"First BRIEF descriptor (binary): {bin(brief_descriptors[0])}")
        print(f"BRIEF descriptor bit length: {brief_descriptors[0].bit_length()}")
```

7.2 ORB (Oriented FAST and Rotated BRIEF):
------------------------------------------
Combines FAST keypoints with BRIEF descriptors
Adds orientation compensation
Rotation invariant

```python
def compute_orb_orientation(image, keypoint, patch_size=31):
    """Compute orientation for ORB keypoint"""
    x, y = int(keypoint['x']), int(keypoint['y'])
    half_patch = patch_size // 2
    
    # Check bounds
    if (x - half_patch < 0 or x + half_patch >= image.shape[1] or
        y - half_patch < 0 or y + half_patch >= image.shape[0]):
        return 0
    
    # Extract patch
    patch = image[y-half_patch:y+half_patch+1, x-half_patch:x+half_patch+1]
    
    # Compute moments
    m01 = m10 = 0
    
    for i in range(patch_size):
        for j in range(patch_size):
            intensity = patch[i, j]
            m01 += i * intensity
            m10 += j * intensity
    
    # Compute orientation
    orientation = np.arctan2(m01, m10) * 180 / np.pi
    if orientation < 0:
        orientation += 360
    
    return orientation

def steered_brief_pairs(orientation, pairs):
    """Rotate BRIEF sampling pairs according to orientation"""
    angle_rad = orientation * np.pi / 180
    cos_angle = np.cos(angle_rad)
    sin_angle = np.sin(angle_rad)
    
    steered_pairs = []
    
    for (p1, p2) in pairs:
        # Rotate p1
        x1_rot = p1[0] * cos_angle - p1[1] * sin_angle
        y1_rot = p1[0] * sin_angle + p1[1] * cos_angle
        
        # Rotate p2
        x2_rot = p2[0] * cos_angle - p2[1] * sin_angle
        y2_rot = p2[0] * sin_angle + p2[1] * cos_angle
        
        steered_pairs.append(((int(x1_rot), int(y1_rot)), (int(x2_rot), int(y2_rot))))
    
    return steered_pairs

# Compute ORB features (FAST + oriented BRIEF)
def compute_orb_descriptors(image, fast_keypoints, descriptor_length=256):
    """Compute ORB descriptors"""
    
    # Convert corner locations to keypoint format
    orb_keypoints = []
    corner_coords = np.where(fast_keypoints)
    
    for i in range(min(10, len(corner_coords[0]))):  # Limit for demo
        kp = {
            'x': corner_coords[1][i],
            'y': corner_coords[0][i]
        }
        orb_keypoints.append(kp)
    
    # Compute orientations
    for kp in orb_keypoints:
        kp['orientation'] = compute_orb_orientation(image, kp)
    
    # Generate base BRIEF pairs
    np.random.seed(42)
    half_patch = 15
    pairs = []
    for _ in range(descriptor_length):
        p1 = (np.random.randint(-half_patch, half_patch+1), 
              np.random.randint(-half_patch, half_patch+1))
        p2 = (np.random.randint(-half_patch, half_patch+1), 
              np.random.randint(-half_patch, half_patch+1))
        pairs.append((p1, p2))
    
    # Compute descriptors
    descriptors = []
    
    for kp in orb_keypoints:
        x, y = int(kp['x']), int(kp['y'])
        orientation = kp['orientation']
        
        # Check bounds
        if (x - half_patch < 0 or x + half_patch >= image.shape[1] or
            y - half_patch < 0 or y + half_patch >= image.shape[0]):
            continue
        
        # Steer BRIEF pairs according to orientation
        steered_pairs = steered_brief_pairs(orientation, pairs)
        
        # Extract patch
        patch = image[y-half_patch:y+half_patch+1, x-half_patch:x+half_patch+1]
        
        # Compute binary descriptor
        descriptor = 0
        
        for i, (p1, p2) in enumerate(steered_pairs):
            y1, x1 = half_patch + p1[1], half_patch + p1[0]
            y2, x2 = half_patch + p2[1], half_patch + p2[0]
            
            # Check bounds
            if (0 <= y1 < patch.shape[0] and 0 <= x1 < patch.shape[1] and
                0 <= y2 < patch.shape[0] and 0 <= x2 < patch.shape[1]):
                
                if patch[y1, x1] > patch[y2, x2]:
                    descriptor |= (1 << i)
        
        descriptors.append(descriptor)
    
    return orb_keypoints, descriptors

# Compute ORB descriptors
orb_keypoints, orb_descriptors = compute_orb_descriptors(test_img, fast_corners)
print(f"ORB keypoints: {len(orb_keypoints)}")
print(f"ORB descriptors: {len(orb_descriptors)}")
if orb_keypoints:
    print(f"Sample ORB keypoint orientation: {orb_keypoints[0]['orientation']:.1f}°")
```

=======================================================

8. IMPLEMENTATION AND PRACTICAL GUIDELINES
==========================================

8.1 Feature Matching:
---------------------
```python
def match_descriptors(desc1, desc2, method='ratio', threshold=0.75):
    """Match descriptors using various methods"""
    
    if method == 'brute_force':
        # Brute force matching
        matches = []
        for i, d1 in enumerate(desc1):
            best_match = -1
            best_distance = float('inf')
            
            for j, d2 in enumerate(desc2):
                distance = np.linalg.norm(d1 - d2)
                if distance < best_distance:
                    best_distance = distance
                    best_match = j
            
            matches.append((i, best_match, best_distance))
        
        return matches
    
    elif method == 'ratio':
        # Lowe's ratio test
        matches = []
        
        for i, d1 in enumerate(desc1):
            distances = []
            for j, d2 in enumerate(desc2):
                distance = np.linalg.norm(d1 - d2)
                distances.append((distance, j))
            
            distances.sort()
            
            if len(distances) >= 2:
                ratio = distances[0][0] / distances[1][0]
                if ratio < threshold:
                    matches.append((i, distances[0][1], distances[0][0]))
        
        return matches

def match_binary_descriptors(desc1, desc2, threshold=50):
    """Match binary descriptors using Hamming distance"""
    matches = []
    
    for i, d1 in enumerate(desc1):
        best_match = -1
        best_distance = float('inf')
        
        for j, d2 in enumerate(desc2):
            # Hamming distance
            hamming_dist = bin(d1 ^ d2).count('1')
            
            if hamming_dist < best_distance:
                best_distance = hamming_dist
                best_match = j
        
        if best_distance < threshold:
            matches.append((i, best_match, best_distance))
    
    return matches

# Example matching
if len(sift_descriptors) >= 2:
    sift_matches = match_descriptors([sift_descriptors[0]], [sift_descriptors[1]], method='brute_force')
    print(f"SIFT matches: {len(sift_matches)}")

if len(orb_descriptors) >= 2:
    orb_matches = match_binary_descriptors([orb_descriptors[0]], [orb_descriptors[1]])
    print(f"ORB matches: {len(orb_matches)}")
```

8.2 Feature Evaluation:
-----------------------
```python
class FeatureEvaluator:
    """Evaluate feature detection and description performance"""
    
    @staticmethod
    def repeatability_score(kp1, kp2, threshold=5):
        """Compute repeatability between two sets of keypoints"""
        if not kp1 or not kp2:
            return 0
        
        matches = 0
        for kp_a in kp1:
            for kp_b in kp2:
                distance = np.sqrt((kp_a['x'] - kp_b['x'])**2 + (kp_a['y'] - kp_b['y'])**2)
                if distance < threshold:
                    matches += 1
                    break
        
        return matches / max(len(kp1), len(kp2))
    
    @staticmethod
    def descriptor_distinctiveness(descriptors):
        """Measure descriptor distinctiveness"""
        if len(descriptors) < 2:
            return 0
        
        distances = []
        for i in range(len(descriptors)):
            for j in range(i+1, len(descriptors)):
                if isinstance(descriptors[i], (int, np.integer)):
                    # Binary descriptor
                    dist = bin(descriptors[i] ^ descriptors[j]).count('1')
                else:
                    # Float descriptor
                    dist = np.linalg.norm(descriptors[i] - descriptors[j])
                distances.append(dist)
        
        return np.mean(distances), np.std(distances)
    
    @staticmethod
    def timing_benchmark(feature_function, image, num_runs=10):
        """Benchmark feature computation time"""
        import time
        
        times = []
        for _ in range(num_runs):
            start_time = time.time()
            result = feature_function(image)
            end_time = time.time()
            times.append(end_time - start_time)
        
        return np.mean(times), np.std(times)

# Evaluate features
evaluator = FeatureEvaluator()

# Repeatability (would need two images for proper evaluation)
print("Feature Evaluation:")
print("-" * 30)

# Descriptor distinctiveness
if sift_descriptors:
    sift_mean_dist, sift_std_dist = evaluator.descriptor_distinctiveness(sift_descriptors)
    print(f"SIFT descriptor distinctiveness: {sift_mean_dist:.4f} ± {sift_std_dist:.4f}")

if orb_descriptors:
    orb_mean_dist, orb_std_dist = evaluator.descriptor_distinctiveness(orb_descriptors)
    print(f"ORB descriptor distinctiveness: {orb_mean_dist:.1f} ± {orb_std_dist:.1f} (Hamming)")

# Timing benchmark
def harris_wrapper(img):
    response, corners = harris_corner_detection(img)
    return corners

harris_time_mean, harris_time_std = evaluator.timing_benchmark(harris_wrapper, test_img, num_runs=5)
print(f"Harris detection time: {harris_time_mean:.4f} ± {harris_time_std:.4f} seconds")
```

8.3 Best Practices:
------------------
Feature Selection Guidelines:
1. Consider invariance requirements (scale, rotation, illumination)
2. Balance computation speed vs. descriptor quality
3. Choose appropriate descriptor dimension
4. Test on representative data

Implementation Tips:
1. Use multi-scale detection for scale invariance
2. Apply non-maximum suppression to reduce redundancy
3. Consider subpixel accuracy for precise localization
4. Optimize critical loops for performance

Quality Assurance:
1. Visualize detected features for validation
2. Test repeatability across transformations
3. Measure descriptor distinctiveness
4. Benchmark computation time

8.4 Common Pitfalls:
-------------------
Detection Issues:
- Too many or too few features detected
- Features concentrated in texture regions
- Poor localization accuracy

Description Problems:
- Non-distinctive descriptors
- Sensitivity to noise or blur
- Large descriptor dimensions

Matching Errors:
- Incorrect threshold selection
- No geometric verification
- Ambiguous matches

8.5 Success Guidelines:
----------------------
1. Understand the mathematical foundations of each method
2. Choose appropriate features for your specific application
3. Consider the trade-offs between accuracy and speed
4. Implement proper evaluation metrics
5. Test robustness to various image transformations
6. Use established libraries for production code
7. Validate results with ground truth when available
8. Document feature parameters and design choices

=======================================================
END OF DOCUMENT 