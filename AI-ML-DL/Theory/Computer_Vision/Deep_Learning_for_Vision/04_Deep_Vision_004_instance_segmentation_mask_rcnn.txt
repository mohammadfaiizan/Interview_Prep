INSTANCE SEGMENTATION: MASK R-CNN AND BEYOND
============================================

Table of Contents:
1. Introduction to Instance Segmentation
2. Mask R-CNN Architecture
3. Instance vs Semantic vs Panoptic Segmentation
4. Training and Loss Functions
5. Advanced Instance Segmentation Methods
6. Panoptic Segmentation
7. Evaluation Metrics
8. Data Annotation and Datasets
9. Python Implementation Examples
10. Applications and Use Cases
11. Performance Analysis
12. Recent Advances and Future Directions

================================================================================
1. INTRODUCTION TO INSTANCE SEGMENTATION
================================================================================

1.1 Problem Definition
---------------------
Instance segmentation is the task of simultaneously detecting and segmenting each distinct object instance in an image:

**Input:** Image I ∈ ℝH×W×C
**Output:** Set of instance masks {(Mi, ci, si)} where:
- Mi ∈ {0,1}H×W: Binary mask for instance i
- ci ∈ {1,2,...,K}: Class label for instance i  
- si ∈ [0,1]: Confidence score for instance i

**Key Characteristics:**
- Distinguishes between different instances of same class
- Provides pixel-level segmentation for each instance
- Combines object detection and semantic segmentation

1.2 Comparison with Related Tasks
--------------------------------
**Object Detection:**
- Output: Bounding boxes + class labels
- Rectangular regions only
- No pixel-level information

**Semantic Segmentation:**
- Output: Dense pixel-level classification
- No instance distinction
- All pixels of same class get same label

**Instance Segmentation:**
- Output: Instance masks + class labels + confidence scores
- Pixel-level precision with instance awareness
- Most comprehensive object understanding

**Panoptic Segmentation:**
- Combines semantic and instance segmentation
- Every pixel gets both semantic and instance labels
- Unified representation of stuff (background) and things (objects)

1.3 Challenges
--------------
**Object Overlap:**
- Instances can partially occlude each other
- Need to separate touching or overlapping objects
- Handle complex spatial relationships

**Scale Variation:**
- Objects appear at different sizes
- Small objects are difficult to segment accurately
- Large objects may exceed network receptive field

**Class Imbalance:**
- Some classes have many instances, others few
- Background vs foreground imbalance
- Rare object detection and segmentation

**Boundary Precision:**
- Accurate delineation of object boundaries
- Sub-pixel precision requirements
- Smooth vs jagged boundaries

1.4 Historical Development
-------------------------
**Timeline:**
- 2014: R-CNN introduces region-based detection
- 2015: Fast R-CNN improves efficiency
- 2016: Faster R-CNN adds Region Proposal Network
- 2017: Mask R-CNN extends to instance segmentation
- 2018: PANet improves feature propagation
- 2019: Mask Scoring R-CNN refines mask quality
- 2020: SOLO introduces direct instance segmentation
- 2021: Mask2Former unifies segmentation tasks

================================================================================
2. MASK R-CNN ARCHITECTURE
================================================================================

2.1 Mask R-CNN Overview
-----------------------
Mask R-CNN extends Faster R-CNN by adding a mask prediction branch:

**Architecture Components:**
1. **Backbone Network:** Feature extraction (ResNet, FPN)
2. **Region Proposal Network (RPN):** Object proposal generation
3. **RoI Head:** Classification, bounding box regression, mask prediction
4. **Mask Head:** FCN for binary mask prediction per class

**Key Innovation:**
- Parallel mask prediction branch
- RoI Align for precise feature extraction
- Class-specific mask prediction

2.2 RoI Align
-------------
**Problem with RoI Pooling:**
- Quantization of RoI coordinates
- Quantization of pooling bins
- Misalignment between RoI and extracted features

**RoI Align Solution:**
- Bilinear interpolation for continuous coordinates
- No quantization in spatial coordinates
- Precise alignment between RoIs and features

**Mathematical Formulation:**
For RoI with coordinates (x₁, y₁, x₂, y₂):
1. Divide into k×k bins without quantization
2. Sample points within each bin using bilinear interpolation
3. Apply pooling (max or average) over sampled points

**Benefits:**
- Improved mask accuracy, especially for small objects
- Better spatial precision
- Essential for pixel-level tasks

2.3 Mask Head Design
-------------------
**Architecture:**
```
RoI Features (7×7×256) → Conv-ReLU-Conv-ReLU-Conv-ReLU-Conv-ReLU → 
Deconv (2×2) → Conv (1×1, K classes) → Output Masks (14×14×K)
```

**Key Design Choices:**
- Small FCN with few layers
- Transposed convolution for upsampling
- Class-specific mask prediction
- Binary mask prediction per class

**Mask Resolution:**
- Typically 14×14 or 28×28
- Trade-off between accuracy and computation
- Higher resolution for better boundary precision

2.4 Multi-task Loss
-------------------
**Combined Loss Function:**
L = L_cls + L_box + L_mask

**Classification Loss:**
L_cls = -log p_u (cross-entropy loss)

**Bounding Box Loss:**
L_box = smooth_L1(t_u - v) for positive RoIs only

**Mask Loss:**
L_mask = -(1/m²) ∑ᵢⱼ [yᵢⱼ log ŷᵢⱼᵏ + (1-yᵢⱼ) log(1-ŷᵢⱼᵏ)]

where:
- m×m is mask resolution
- k is ground-truth class
- yᵢⱼ is ground-truth mask pixel
- ŷᵢⱼᵏ is predicted mask pixel for class k

**Key Properties:**
- Mask loss only computed for positive RoIs
- Average binary cross-entropy over mask pixels
- Decouples mask and class prediction

================================================================================
3. INSTANCE VS SEMANTIC VS PANOPTIC SEGMENTATION
================================================================================

3.1 Semantic Segmentation
-------------------------
**Characteristics:**
- Pixel-level classification
- No instance distinction
- Stuff classes (background, sky, road)
- Dense labeling of entire image

**Representation:**
- Single label map Y ∈ {1,2,...,K}H×W
- Each pixel assigned one class label
- Same class instances merged together

**Applications:**
- Scene understanding
- Autonomous driving (drivable area)
- Medical imaging (organ segmentation)

3.2 Instance Segmentation
-------------------------
**Characteristics:**
- Object detection + segmentation
- Instance-aware labeling
- Thing classes (objects with distinct instances)
- Sparse labeling of object instances

**Representation:**
- Set of instance masks {(Mi, ci, si)}
- Each instance gets separate mask
- Background typically not labeled

**Applications:**
- Object counting and tracking
- Robotics (object manipulation)
- Quality control (defect detection)

3.3 Panoptic Segmentation
-------------------------
**Unified Framework:**
- Combines semantic and instance segmentation
- Every pixel gets semantic and instance label
- Distinguishes "stuff" and "things"

**Representation:**
- Semantic map: S ∈ {1,2,...,K}H×W
- Instance map: I ∈ {0,1,...,N}H×W
- Combined panoptic map: P = S × R + I

where R is large constant (e.g., 1000)

**Rules:**
- Stuff classes: Multiple pixels, same instance ID
- Thing classes: Each instance gets unique ID
- No overlapping instances

3.4 Task Relationships
----------------------
**Hierarchy:**
```
Panoptic Segmentation
├── Semantic Segmentation (stuff)
└── Instance Segmentation (things)
```

**Conversion:**
- Instance → Semantic: Merge instances of same class
- Semantic → Instance: Not possible (no instance info)
- Panoptic → Both: Direct extraction

**Complexity:**
- Semantic: Least complex, pixel classification
- Instance: Medium complexity, detection + segmentation
- Panoptic: Most complex, unified understanding

================================================================================
4. TRAINING AND LOSS FUNCTIONS
================================================================================

4.1 Data Preparation
-------------------
**Annotation Requirements:**
- Bounding boxes for each instance
- Pixel-level masks for each instance
- Class labels for each instance
- Instance IDs for tracking

**Data Augmentation:**
- Horizontal flips (with mask adjustment)
- Multi-scale training
- Color jittering
- Crop and resize (maintaining instance integrity)

**Negative Mining:**
- Hard negative mining for background RoIs
- Balance positive/negative ratio (1:3)
- Focus training on difficult examples

4.2 Training Strategy
--------------------
**Multi-stage Training:**
1. Pre-train backbone on ImageNet
2. Train RPN on detection dataset
3. Train Fast R-CNN with RPN features
4. Fine-tune entire network end-to-end

**End-to-end Training:**
- Joint optimization of all components
- Shared backbone features
- Faster convergence
- Better performance

**Training Schedule:**
- Learning rate schedule (step decay)
- Batch size considerations
- Gradient clipping for stability
- Weight decay for regularization

4.3 Loss Function Design
-----------------------
**Balanced Multi-task Loss:**
L_total = λ₁L_rpn_cls + λ₂L_rpn_box + λ₃L_cls + λ₄L_box + λ₅L_mask

**Loss Weighting:**
- Typically λ₁ = λ₂ = λ₃ = λ₄ = λ₅ = 1
- Can be tuned based on task importance
- Automatic loss balancing methods available

**Mask Loss Variants:**
- Binary cross-entropy (standard)
- Dice loss for better overlap
- Focal loss for hard examples
- Boundary-aware losses

4.4 Advanced Training Techniques
-------------------------------
**Mixed Precision Training:**
- FP16 for forward pass, FP32 for loss computation
- Faster training with lower memory usage
- Gradient scaling to prevent underflow

**Gradient Accumulation:**
- Simulate larger batch sizes
- Accumulate gradients over multiple mini-batches
- Update parameters after accumulation

**Knowledge Distillation:**
- Teacher-student training paradigm
- Transfer knowledge from larger to smaller models
- Improve performance of compact models

================================================================================
5. ADVANCED INSTANCE SEGMENTATION METHODS
================================================================================

5.1 PANet (Path Aggregation Network)
------------------------------------
**Key Improvements:**
1. **Bottom-up Path Augmentation:** Strengthen feature pyramid
2. **Adaptive Feature Pooling:** Aggregate features from all levels
3. **Fully-connected Fusion:** Better mask prediction

**Bottom-up Path:**
- Augment top-down FPN with bottom-up path
- Enhance lower-level features with high-level semantics
- Shorter information path

**Adaptive Feature Pooling:**
- Pool features from multiple FPN levels
- Weighted fusion based on proposal size
- Better multi-scale representation

5.2 Mask Scoring R-CNN
----------------------
**Problem:** Mask quality and classification confidence mismatch

**Solution:** 
- Additional mask scoring head
- Predict IoU between predicted and ground-truth masks
- Re-rank instances based on mask quality

**MaskIoU Head:**
- Takes mask features as input
- Predicts IoU score for each class
- Used during inference for re-ranking

**Benefits:**
- Better correlation between confidence and mask quality
- Improved Average Precision metrics
- More reliable instance ranking

5.3 SOLO (Segmenting Objects by Locations)
------------------------------------------
**Direct Instance Segmentation:**
- No object detection pipeline
- Direct prediction of instance masks
- Location-based instance differentiation

**Grid-based Approach:**
- Divide image into S×S grid
- Each grid cell responsible for center of instances
- Predict instance categories and masks

**Architecture:**
```
Backbone → FPN → Category Branch + Mask Branch
```

**Category Branch:**
- Predict object categories at each grid location
- S×S×C output (C classes)
- Multi-label classification

**Mask Branch:**
- Predict instance masks
- S²×H×W output
- Each channel corresponds to grid location

5.4 SOLOv2
----------
**Improvements over SOLO:**
1. **Dynamic Convolution:** Reduce parameter redundancy
2. **Matrix NMS:** Efficient non-maximum suppression
3. **Better Feature Learning:** Improved training strategies

**Dynamic Convolution:**
- Generate mask kernels dynamically
- Reduce computation and memory
- Better parameter efficiency

**Matrix NMS:**
- Parallel NMS computation
- Sort by confidence scores
- Apply decay based on IoU overlap

================================================================================
6. PANOPTIC SEGMENTATION
================================================================================

6.1 Panoptic Segmentation Definition
------------------------------------
**Unified Representation:**
Every pixel is assigned both semantic and instance labels:
- Semantic label: What class the pixel belongs to
- Instance label: Which instance of that class

**Quality Metric:**
Panoptic Quality (PQ) = SQ × RQ

where:
- SQ (Segmentation Quality): Average IoU of matched segments
- RQ (Recognition Quality): F1-score of matching

6.2 Panoptic FPN
----------------
**Architecture:**
- Shared backbone and FPN
- Separate heads for instance and semantic segmentation
- Fusion module to combine outputs

**Components:**
1. **Instance Head:** Modified Mask R-CNN
2. **Semantic Head:** FCN for dense prediction
3. **Fusion Module:** Resolve conflicts between heads

**Fusion Strategy:**
- Instance predictions have higher priority
- Semantic predictions fill remaining pixels
- Overlapping resolution based on confidence

6.3 UPSNet (Unified Panoptic Segmentation)
------------------------------------------
**Key Innovation:** Panoptic head for joint prediction

**Architecture:**
```
Backbone → FPN → Instance Head
                → Semantic Head  → Panoptic Head → Final Output
                → Panoptic Head
```

**Panoptic Head:**
- Takes instance and semantic features
- Joint optimization for both tasks
- Better feature sharing and consistency

6.4 Panoptic Deeplab
--------------------
**Bottom-up Approach:**
- Dense prediction for all pixels
- Instance center detection
- Instance regression for grouping

**Components:**
1. **Semantic Segmentation:** Dense classification
2. **Instance Center Detection:** Heatmap of object centers
3. **Center Regression:** Offset vectors to instance centers

**Grouping Algorithm:**
- For each pixel, follow regression vector to center
- Group pixels belonging to same center
- Assign instance IDs based on grouping

================================================================================
7. EVALUATION METRICS
================================================================================

7.1 Instance Segmentation Metrics
---------------------------------
**Average Precision (AP):**
- Primary evaluation metric
- Computed over range of IoU thresholds
- AP@0.5, AP@0.75, AP@[0.5:0.95]

**Average Recall (AR):**
- Maximum recall given fixed number of detections
- AR@1, AR@10, AR@100

**Scale-specific Metrics:**
- AP_S: Small objects (area < 32²)
- AP_M: Medium objects (32² < area < 96²)
- AP_L: Large objects (area > 96²)

7.2 Panoptic Segmentation Metrics
---------------------------------
**Panoptic Quality (PQ):**
PQ = (∑(p,g)∈TP IoU(p,g)) / (|TP| + 0.5|FP| + 0.5|FN|)

**Decomposition:**
PQ = SQ × RQ

where:
- SQ = (∑(p,g)∈TP IoU(p,g)) / |TP|
- RQ = |TP| / (|TP| + 0.5|FP| + 0.5|FN|)

**Per-class Metrics:**
- PQ per semantic class
- Separate evaluation for things and stuff

7.3 Boundary Evaluation
-----------------------
**Boundary IoU:**
- Evaluate masks only near object boundaries
- More sensitive to boundary quality
- Important for applications requiring precision

**Average Boundary Precision:**
- Precision/recall for boundary pixels
- Distance-based boundary matching
- Complementary to mask-based metrics

7.4 Computational Metrics
-------------------------
**Inference Speed:**
- FPS (Frames Per Second)
- Processing time per image
- Memory usage during inference

**Model Complexity:**
- Number of parameters
- FLOPs (Floating Point Operations)
- Model size for deployment

================================================================================
8. DATA ANNOTATION AND DATASETS
================================================================================

8.1 Annotation Process
----------------------
**Instance Annotation Pipeline:**
1. Object detection (bounding boxes)
2. Polygon annotation for masks
3. Class label assignment
4. Quality control and verification

**Tools:**
- CVAT (Computer Vision Annotation Tool)
- LabelMe for polygon annotation
- VGG Image Annotator (VIA)
- Supervisely platform

**Quality Considerations:**
- Inter-annotator agreement
- Boundary precision requirements
- Handling of occlusions
- Consistency across dataset

8.2 Major Datasets
------------------
**COCO (Common Objects in Context):**
- 80 object categories
- 330K images, 1.5M instances
- Standard benchmark for instance segmentation
- Detailed annotations with polygon masks

**Cityscapes:**
- Urban street scenes
- 19 semantic classes
- Instance annotations for vehicles and people
- High-resolution images (2048×1024)

**ADE20K:**
- Scene parsing dataset
- 150 semantic categories
- Panoptic annotations available
- Diverse indoor/outdoor scenes

**Open Images:**
- Large-scale dataset (9M images)
- 600 object categories
- Subset with instance segmentation annotations
- Hierarchical label structure

8.3 Synthetic Datasets
----------------------
**Advantages:**
- Perfect ground truth annotations
- Controlled variations (lighting, pose, occlusion)
- Cost-effective data generation
- Domain-specific customization

**Examples:**
- SYNTHIA (synthetic urban scenes)
- Virtual KITTI
- Flying Chairs/Things
- AI2-THOR virtual environments

**Domain Gap:**
- Visual appearance differences
- Texture and lighting variations
- Object arrangements
- Domain adaptation techniques needed

8.4 Weakly Supervised Approaches
--------------------------------
**Image-level Supervision:**
- Only class labels available
- Generate pseudo-masks using CAM/Grad-CAM
- Iterative refinement of masks

**Point Supervision:**
- Single point per instance
- Minimal annotation cost
- Generate masks through expansion

**Scribble Supervision:**
- Rough scribbles indicating objects
- More information than points
- Interactive annotation tools

================================================================================
9. PYTHON IMPLEMENTATION EXAMPLES
================================================================================

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision.models.detection import maskrcnn_resnet50_fpn
from torchvision.models.detection.rpn import AnchorGenerator
import numpy as np
import cv2
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple

class MaskRCNN(nn.Module):
    """Simplified Mask R-CNN implementation"""
    
    def __init__(self, num_classes=91, backbone='resnet50'):
        super(MaskRCNN, self).__init__()
        
        # Use pre-trained Mask R-CNN
        self.model = maskrcnn_resnet50_fpn(
            pretrained=True,
            num_classes=num_classes,
            rpn_anchor_generator=AnchorGenerator(
                sizes=((32, 64, 128, 256, 512),),
                aspect_ratios=((0.5, 1.0, 2.0),)
            )
        )
        
    def forward(self, images, targets=None):
        return self.model(images, targets)

class RoIAlign(nn.Module):
    """RoI Align implementation"""
    
    def __init__(self, output_size, spatial_scale, sampling_ratio=-1):
        super(RoIAlign, self).__init__()
        self.output_size = output_size
        self.spatial_scale = spatial_scale
        self.sampling_ratio = sampling_ratio
        
    def forward(self, features, rois):
        """
        Args:
            features: (N, C, H, W) feature maps
            rois: (K, 5) where each row is (batch_index, x1, y1, x2, y2)
        Returns:
            aligned_features: (K, C, output_size, output_size)
        """
        return torchvision.ops.roi_align(
            features, rois, self.output_size, 
            self.spatial_scale, self.sampling_ratio
        )

class MaskHead(nn.Module):
    """Mask prediction head"""
    
    def __init__(self, in_channels=256, num_classes=80, mask_size=14):
        super(MaskHead, self).__init__()
        
        self.mask_convs = nn.Sequential(
            nn.Conv2d(in_channels, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True),
        )
        
        self.mask_deconv = nn.ConvTranspose2d(256, 256, 2, stride=2)
        self.mask_predictor = nn.Conv2d(256, num_classes, 1)
        
    def forward(self, x):
        x = self.mask_convs(x)
        x = F.relu(self.mask_deconv(x))
        x = self.mask_predictor(x)
        return x

class InstanceSegmentationLoss(nn.Module):
    """Combined loss for instance segmentation"""
    
    def __init__(self, mask_weight=1.0):
        super(InstanceSegmentationLoss, self).__init__()
        self.mask_weight = mask_weight
        
    def forward(self, predictions, targets):
        """
        Args:
            predictions: dict with 'masks', 'boxes', 'labels', 'scores'
            targets: list of dicts with 'masks', 'boxes', 'labels'
        """
        losses = {}
        
        # Classification loss
        if 'loss_classifier' in predictions:
            losses['loss_classifier'] = predictions['loss_classifier']
            
        # Box regression loss  
        if 'loss_box_reg' in predictions:
            losses['loss_box_reg'] = predictions['loss_box_reg']
            
        # Mask loss
        if 'loss_mask' in predictions:
            losses['loss_mask'] = predictions['loss_mask'] * self.mask_weight
            
        # RPN losses
        if 'loss_objectness' in predictions:
            losses['loss_objectness'] = predictions['loss_objectness']
        if 'loss_rpn_box_reg' in predictions:
            losses['loss_rpn_box_reg'] = predictions['loss_rpn_box_reg']
            
        return losses

def mask_loss(pred_masks, target_masks, target_labels):
    """
    Compute mask loss for Mask R-CNN
    
    Args:
        pred_masks: (N, C, H, W) predicted masks
        target_masks: (N, H, W) target masks
        target_labels: (N,) target labels
    """
    # Select masks corresponding to target labels
    N, C, H, W = pred_masks.shape
    selected_masks = pred_masks[range(N), target_labels]
    
    # Binary cross entropy loss
    loss = F.binary_cross_entropy_with_logits(
        selected_masks, target_masks.float(), reduction='mean'
    )
    
    return loss

class SOLO(nn.Module):
    """SOLO: Segmenting Objects by Locations"""
    
    def __init__(self, num_classes=80, num_grids=[40, 36, 24, 16, 12]):
        super(SOLO, self).__init__()
        self.num_classes = num_classes
        self.num_grids = num_grids
        
        # Backbone (simplified)
        self.backbone = torchvision.models.resnet50(pretrained=True)
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])
        
        # FPN
        self.fpn = self._build_fpn()
        
        # Category branch
        self.category_convs = nn.ModuleList([
            self._make_category_head(256, num_classes, grid_size)
            for grid_size in num_grids
        ])
        
        # Mask branch  
        self.mask_convs = nn.ModuleList([
            self._make_mask_head(256, grid_size)
            for grid_size in num_grids
        ])
        
    def _build_fpn(self):
        # Simplified FPN implementation
        return nn.Sequential(
            nn.Conv2d(2048, 256, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        
    def _make_category_head(self, in_channels, num_classes, grid_size):
        return nn.Sequential(
            nn.Conv2d(in_channels, 256, 3, padding=1),
            nn.GroupNorm(32, 256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.GroupNorm(32, 256),  
            nn.ReLU(inplace=True),
            nn.Conv2d(256, num_classes, 3, padding=1),
            nn.AdaptiveAvgPool2d((grid_size, grid_size))
        )
        
    def _make_mask_head(self, in_channels, grid_size):
        return nn.Sequential(
            nn.Conv2d(in_channels, 128, 3, padding=1),
            nn.GroupNorm(32, 128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.GroupNorm(32, 128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, grid_size ** 2, 1)
        )
        
    def forward(self, x):
        # Extract features
        features = self.backbone(x)
        fpn_features = self.fpn(features)
        
        # Category and mask predictions
        category_preds = []
        mask_preds = []
        
        for i, (cat_head, mask_head) in enumerate(zip(self.category_convs, self.mask_convs)):
            # Resize features for different grid sizes
            feat_size = self.num_grids[i] * 2  # Feature map size
            resized_feat = F.interpolate(
                fpn_features, size=(feat_size, feat_size), mode='bilinear'
            )
            
            cat_pred = cat_head(resized_feat)
            mask_pred = mask_head(resized_feat)
            
            category_preds.append(cat_pred)
            mask_preds.append(mask_pred)
            
        return {'category': category_preds, 'mask': mask_preds}

def compute_mask_iou(pred_mask, target_mask):
    """Compute IoU between predicted and target masks"""
    pred_mask = pred_mask > 0.5  # Threshold
    target_mask = target_mask > 0.5
    
    intersection = (pred_mask & target_mask).sum().float()
    union = (pred_mask | target_mask).sum().float()
    
    if union == 0:
        return 1.0 if intersection == 0 else 0.0
    
    return intersection / union

def non_max_suppression_masks(boxes, scores, masks, iou_threshold=0.5):
    """
    Non-maximum suppression for instance segmentation
    
    Args:
        boxes: (N, 4) bounding boxes
        scores: (N,) confidence scores  
        masks: (N, H, W) binary masks
        iou_threshold: IoU threshold for suppression
    
    Returns:
        keep: indices of boxes to keep
    """
    if len(boxes) == 0:
        return torch.tensor([], dtype=torch.long)
    
    # Sort by scores
    _, indices = scores.sort(descending=True)
    
    keep = []
    while len(indices) > 0:
        current = indices[0]
        keep.append(current)
        
        if len(indices) == 1:
            break
        
        # Compute mask IoU with remaining boxes
        current_mask = masks[current]
        remaining_masks = masks[indices[1:]]
        
        ious = torch.tensor([
            compute_mask_iou(current_mask, mask) 
            for mask in remaining_masks
        ])
        
        # Keep boxes with IoU below threshold
        keep_mask = ious <= iou_threshold
        indices = indices[1:][keep_mask]
    
    return torch.tensor(keep, dtype=torch.long)

def visualize_instance_segmentation(image, instances, class_names, 
                                  score_threshold=0.5, alpha=0.5):
    """
    Visualize instance segmentation results
    
    Args:
        image: numpy array (H, W, 3)
        instances: dict with 'boxes', 'masks', 'labels', 'scores'
        class_names: list of class names
        score_threshold: minimum score to display
        alpha: transparency for mask overlay
    """
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    ax.imshow(image)
    
    boxes = instances['boxes'].cpu().numpy()
    masks = instances['masks'].cpu().numpy()
    labels = instances['labels'].cpu().numpy()
    scores = instances['scores'].cpu().numpy()
    
    # Generate colors for each instance
    colors = plt.cm.tab20(np.linspace(0, 1, len(boxes)))
    
    for i in range(len(boxes)):
        if scores[i] < score_threshold:
            continue
            
        box = boxes[i]
        mask = masks[i, 0]  # First channel
        label = labels[i]
        score = scores[i]
        color = colors[i]
        
        # Draw bounding box
        x1, y1, x2, y2 = box
        width = x2 - x1
        height = y2 - y1
        
        rect = plt.Rectangle((x1, y1), width, height, 
                           fill=False, edgecolor=color[:3], linewidth=2)
        ax.add_patch(rect)
        
        # Draw mask
        colored_mask = np.zeros_like(image)
        colored_mask[mask > 0.5] = (np.array(color[:3]) * 255).astype(np.uint8)
        
        # Blend with original image
        mask_area = mask > 0.5
        blended = image.copy()
        blended[mask_area] = (1 - alpha) * blended[mask_area] + alpha * colored_mask[mask_area]
        
        # Update display
        ax.imshow(blended, alpha=alpha)
        
        # Draw label
        class_name = class_names[label] if label < len(class_names) else f"Class_{label}"
        label_text = f"{class_name}: {score:.2f}"
        ax.text(x1, y1 - 5, label_text, fontsize=10, 
               bbox=dict(boxstyle="round,pad=0.3", facecolor=color, alpha=0.7))
    
    ax.axis('off')
    ax.set_title('Instance Segmentation Results')
    plt.tight_layout()
    plt.show()

def evaluate_instance_segmentation(predictions, targets, iou_thresholds=None):
    """
    Evaluate instance segmentation using COCO metrics
    
    Args:
        predictions: list of dicts with 'boxes', 'masks', 'labels', 'scores'
        targets: list of dicts with 'boxes', 'masks', 'labels'
        iou_thresholds: list of IoU thresholds
    
    Returns:
        dict with evaluation metrics
    """
    if iou_thresholds is None:
        iou_thresholds = np.arange(0.5, 1.0, 0.05)
    
    aps = []
    
    for iou_thresh in iou_thresholds:
        tp, fp, fn = 0, 0, 0
        
        for pred, target in zip(predictions, targets):
            pred_masks = pred['masks']
            target_masks = target['masks']
            
            # Match predictions to targets
            matched = set()
            
            for i, pred_mask in enumerate(pred_masks):
                best_iou = 0
                best_match = -1
                
                for j, target_mask in enumerate(target_masks):
                    if j in matched:
                        continue
                        
                    iou = compute_mask_iou(pred_mask, target_mask)
                    if iou > best_iou:
                        best_iou = iou
                        best_match = j
                
                if best_iou >= iou_thresh:
                    tp += 1
                    matched.add(best_match)
                else:
                    fp += 1
            
            fn += len(target_masks) - len(matched)
        
        # Compute precision and recall
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        # Compute AP (simplified)
        ap = precision * recall
        aps.append(ap)
    
    # Average over IoU thresholds
    mean_ap = np.mean(aps)
    
    return {
        'mAP': mean_ap,
        'AP@0.5': aps[0] if len(aps) > 0 else 0,
        'AP@0.75': aps[5] if len(aps) > 5 else 0,
    }

# Example usage
def demonstrate_instance_segmentation():
    """Demonstrate instance segmentation pipeline"""
    
    # Initialize model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = MaskRCNN(num_classes=91)  # COCO classes
    model.to(device)
    model.eval()
    
    # Create dummy input
    batch_size = 2
    images = [torch.randn(3, 480, 640) for _ in range(batch_size)]
    
    # Forward pass
    with torch.no_grad():
        predictions = model(images)
    
    print("Instance Segmentation Results:")
    for i, pred in enumerate(predictions):
        print(f"Image {i+1}:")
        print(f"  Detected {len(pred['boxes'])} instances")
        print(f"  Boxes shape: {pred['boxes'].shape}")
        print(f"  Masks shape: {pred['masks'].shape}")
        print(f"  Labels: {pred['labels']}")
        print(f"  Scores: {pred['scores']}")
    
    # Demonstrate SOLO
    print("\nSOLO Results:")
    solo_model = SOLO(num_classes=80)
    solo_model.eval()
    
    dummy_input = torch.randn(1, 3, 512, 512)
    with torch.no_grad():
        solo_output = solo_model(dummy_input)
    
    print(f"Category predictions: {len(solo_output['category'])} levels")
    print(f"Mask predictions: {len(solo_output['mask'])} levels")
    
    for i, (cat, mask) in enumerate(zip(solo_output['category'], solo_output['mask'])):
        print(f"Level {i}: Category {cat.shape}, Mask {mask.shape}")

if __name__ == "__main__":
    demonstrate_instance_segmentation()
```

================================================================================
10. APPLICATIONS AND USE CASES
================================================================================

10.1 Autonomous Vehicles
-----------------------
**Object Instance Detection:**
- Vehicle counting and tracking
- Pedestrian detection and behavior prediction
- Traffic sign and signal recognition
- Road object segmentation

**Safety Requirements:**
- Real-time processing (<100ms)
- High precision for safety-critical objects
- Robustness to weather conditions
- Multi-sensor fusion capability

10.2 Medical Imaging
-------------------
**Cell and Organ Analysis:**
- Cell counting in microscopy
- Tumor segmentation and measurement
- Organ boundary delineation
- Pathology analysis

**Precision Requirements:**
- Sub-pixel accuracy for measurements
- 3D volumetric segmentation
- Handling of overlapping structures
- Regulatory compliance (FDA)

10.3 Industrial Quality Control
------------------------------
**Defect Detection:**
- Product inspection on assembly lines
- Component counting and verification
- Surface defect segmentation
- Dimensional measurement

**Deployment Constraints:**
- Real-time processing requirements
- Edge computing limitations
- Harsh industrial environments
- Integration with existing systems

10.4 Retail and Inventory
-------------------------
**Product Recognition:**
- Shelf monitoring and restocking
- Customer behavior analysis
- Automated checkout systems
- Inventory management

**Challenges:**
- Dense object arrangements
- Similar product variations
- Lighting condition changes
- Scale and deployment cost

10.5 Agriculture and Environment
-------------------------------
**Crop Monitoring:**
- Plant counting and health assessment
- Fruit detection and yield estimation
- Weed identification and mapping
- Livestock monitoring

**Environmental Monitoring:**
- Wildlife tracking and counting
- Deforestation analysis
- Pollution source identification
- Habitat assessment

================================================================================
11. PERFORMANCE ANALYSIS
================================================================================

11.1 COCO Dataset Benchmarks
----------------------------
**Instance Segmentation Results (mAP):**
- Mask R-CNN (ResNet-50): 35.7%
- Mask R-CNN (ResNet-101): 38.2%
- PANet (ResNet-101): 42.0%
- Mask Scoring R-CNN: 39.8%
- SOLO (ResNet-101): 39.7%
- SOLOv2 (ResNet-101): 42.4%

**Panoptic Segmentation Results (PQ):**
- Panoptic FPN: 39.0%
- UPSNet: 42.5%
- Panoptic Deeplab: 41.4%

11.2 Speed Analysis
------------------
**Inference Time (ms on V100 GPU):**
- Mask R-CNN: 85ms
- PANet: 120ms
- SOLO: 77ms
- SOLOv2: 54ms
- Real-time requirement: <33ms (30 FPS)

**Model Size:**
- Mask R-CNN (ResNet-50): 170MB
- Mask R-CNN (ResNet-101): 255MB
- Mobile deployments: <50MB preferred

11.3 Accuracy vs Speed Trade-offs
---------------------------------
**High Accuracy (Research):**
- Complex architectures with FPN
- Large backbone networks (ResNet-101, ResNeXt)
- Multi-scale training and testing
- Ensemble methods

**Real-time Applications:**
- Lightweight backbones (MobileNet, EfficientNet)
- Single-scale inference
- Model compression techniques
- Hardware acceleration (TensorRT, ONNX)

**Mobile Deployment:**
- Quantization (INT8, FP16)
- Knowledge distillation
- Architecture search for efficiency
- Edge computing optimization

================================================================================
12. RECENT ADVANCES AND FUTURE DIRECTIONS
================================================================================

12.1 Transformer-Based Methods
------------------------------
**DETR (Detection Transformer):**
- End-to-end object detection with transformers
- Set prediction approach
- No hand-crafted components (NMS, anchors)

**Mask2Former:**
- Unified architecture for segmentation tasks
- Masked attention for better feature learning
- State-of-the-art panoptic segmentation

**SegFormer:**
- Transformer encoder with lightweight decoder
- No positional encoding required
- Efficient hierarchical attention

12.2 Real-time Instance Segmentation
------------------------------------
**YOLACT (You Only Look At CoefficienTs):**
- Real-time instance segmentation
- Prototype-based mask generation
- 30+ FPS on GPU

**CenterMask:**
- Anchor-free instance segmentation
- Spatial Attention-Guided Mask (SAG-Mask)
- Real-time performance

**CondInst:**
- Conditional convolutions for mask prediction
- Dynamic instance-aware convolutions
- Better boundary quality

12.3 Weakly Supervised Learning
------------------------------
**Few-shot Instance Segmentation:**
- Learn new categories with few examples
- Meta-learning approaches
- Support-query paradigm

**Unsupervised Instance Segmentation:**
- Discovery of object instances without labels
- Self-supervised learning
- Clustering-based approaches

**Semi-supervised Learning:**
- Utilize unlabeled data effectively
- Pseudo-labeling strategies
- Consistency regularization

12.4 3D Instance Segmentation
-----------------------------
**Point Cloud Segmentation:**
- 3D object detection and segmentation
- LiDAR data processing
- Autonomous driving applications

**RGB-D Segmentation:**
- Depth information integration
- Better occlusion handling
- Indoor scene understanding

**Multi-modal Fusion:**
- Combine RGB, depth, and point cloud
- Complementary information sources
- Robust 3D understanding

12.5 Future Research Directions
------------------------------
**Efficiency Improvements:**
- Neural architecture search for segmentation
- Hardware-aware model design
- Dynamic inference adaptation

**Generalization:**
- Domain adaptation techniques
- Zero-shot instance segmentation
- Cross-dataset transfer learning

**Integration:**
- Unified vision models
- Multi-task learning frameworks
- End-to-end trainable systems

**Applications:**
- Video instance segmentation
- Interactive segmentation
- Augmented reality integration

================================================================================
CONCLUSION
================================================================================

Instance segmentation has evolved from early Mask R-CNN approaches to sophisticated methods achieving impressive accuracy and efficiency. Key developments include:

**Architectural Innovations:**
- RoI Align for precise feature extraction
- Multi-task learning frameworks
- Direct instance prediction methods
- Transformer-based architectures

**Training Advances:**
- Better loss functions and training strategies
- Weakly supervised learning approaches
- Self-supervised pretraining methods
- Efficient annotation techniques

**Performance Improvements:**
- COCO mAP from 35% to 45%+
- Real-time inference capabilities
- Mobile deployment optimizations
- Better boundary accuracy

**Applications:**
- Autonomous driving with safety-critical requirements
- Medical imaging with precision demands
- Industrial automation for quality control
- Consumer applications in mobile devices

**Future Directions:**
- Unified segmentation frameworks
- Better generalization across domains
- Efficient models for edge deployment
- Integration with other vision tasks

The field continues advancing toward more practical, efficient, and generalizable instance segmentation systems that can handle diverse real-world scenarios with minimal supervision.

================================================================================
REFERENCES AND FURTHER READING
================================================================================

1. He, K. et al. "Mask R-CNN" (2017)
2. Liu, S. et al. "Path Aggregation Network for Instance Segmentation" (2018)
3. Huang, Z. et al. "Mask Scoring R-CNN" (2019)
4. Wang, X. et al. "SOLO: Segmenting Objects by Locations" (2020)
5. Wang, X. et al. "SOLOv2: Dynamic and Fast Instance Segmentation" (2020)
6. Kirillov, A. et al. "Panoptic Segmentation" (2019)
7. Kirillov, A. et al. "Panoptic Feature Pyramid Networks" (2019)
8. Xiong, Y. et al. "UPSNet: A Unified Panoptic Segmentation Network" (2019)
9. Cheng, B. et al. "Masked-attention Mask Transformer for Universal Image Segmentation" (2022)
10. Bolya, D. et al. "YOLACT: Real-time Instance Segmentation" (2019) 