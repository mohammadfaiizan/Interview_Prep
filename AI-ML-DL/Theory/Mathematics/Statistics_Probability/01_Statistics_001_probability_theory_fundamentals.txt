PROBABILITY THEORY FUNDAMENTALS - Mathematical Foundations for ML
===============================================================

TABLE OF CONTENTS:
1. Sample Spaces and Events
2. Probability Axioms and Properties
3. Random Variables and Distributions
4. Expectation and Moments
5. Joint and Conditional Probability
6. Independence and Conditional Independence
7. Limit Theorems
8. Applications in Machine Learning

=======================================================

1. SAMPLE SPACES AND EVENTS
===========================

1.1 Sample Space:
----------------
Sample space Ω: Set of all possible outcomes of an experiment

Examples:
- Coin flip: Ω = {H, T}
- Die roll: Ω = {1, 2, 3, 4, 5, 6}
- Real number: Ω = ℝ
- Feature vector: Ω = ℝⁿ

Properties:
- Must be exhaustive (covers all possibilities)
- Outcomes must be mutually exclusive
- Can be finite, countably infinite, or uncountable

1.2 Events:
----------
Event: Subset E ⊆ Ω of the sample space

Types of events:
- Elementary event: Single outcome {ω}
- Composite event: Union of elementary events
- Certain event: Ω (always occurs)
- Impossible event: ∅ (never occurs)

Event operations:
- Union: A ∪ B (A or B occurs)
- Intersection: A ∩ B (both A and B occur)
- Complement: Aᶜ = Ω \ A (A does not occur)
- Difference: A \ B = A ∩ Bᶜ

1.3 σ-Algebra (σ-Field):
-----------------------
Collection ℱ of subsets of Ω that satisfies:
1. Ω ∈ ℱ
2. If A ∈ ℱ, then Aᶜ ∈ ℱ
3. If A₁, A₂, ... ∈ ℱ, then ⋃ᵢ Aᵢ ∈ ℱ

Examples:
- Trivial σ-algebra: {∅, Ω}
- Power set: 2^Ω (all subsets)
- Borel σ-algebra: Generated by open sets in ℝ

Purpose: Ensures probability is well-defined on event space

1.4 Measurable Space:
--------------------
Pair (Ω, ℱ) where Ω is sample space and ℱ is σ-algebra

Requirements for probability theory:
- Events must be measurable (in ℱ)
- Enables rigorous treatment of continuous spaces
- Foundation for measure-theoretic probability

=======================================================

2. PROBABILITY AXIOMS AND PROPERTIES
====================================

2.1 Kolmogorov Axioms:
---------------------
Probability measure P: ℱ → [0,1] satisfying:

Axiom 1 (Non-negativity): P(A) ≥ 0 for all A ∈ ℱ
Axiom 2 (Normalization): P(Ω) = 1
Axiom 3 (Countable additivity): For disjoint events A₁, A₂, ...:
P(⋃ᵢ Aᵢ) = Σᵢ P(Aᵢ)

Probability space: (Ω, ℱ, P)

2.2 Basic Properties:
--------------------
P(∅) = 0 (probability of impossible event)
P(Aᶜ) = 1 - P(A) (complement rule)
P(A ∪ B) = P(A) + P(B) - P(A ∩ B) (inclusion-exclusion)

Monotonicity: If A ⊆ B, then P(A) ≤ P(B)
Boole's inequality: P(⋃ᵢ Aᵢ) ≤ Σᵢ P(Aᵢ)

2.3 Conditional Probability:
---------------------------
P(A|B) = P(A ∩ B)/P(B) for P(B) > 0

Interpretation: Probability of A given that B has occurred

Properties:
- P(·|B) is a probability measure
- P(A|B) + P(Aᶜ|B) = 1
- P(A|A) = 1 for P(A) > 0

Chain rule: P(A₁ ∩ A₂ ∩ ... ∩ Aₙ) = P(A₁)P(A₂|A₁)P(A₃|A₁ ∩ A₂)...

2.4 Law of Total Probability:
----------------------------
If B₁, B₂, ..., Bₙ partition Ω (disjoint and exhaustive):
P(A) = Σᵢ P(A|Bᵢ)P(Bᵢ)

Continuous version: P(A) = ∫ P(A|X = x)fₓ(x)dx

2.5 Bayes' Theorem:
------------------
P(Bᵢ|A) = P(A|Bᵢ)P(Bᵢ) / Σⱼ P(A|Bⱼ)P(Bⱼ)

Components:
- P(Bᵢ): Prior probability
- P(A|Bᵢ): Likelihood
- P(Bᵢ|A): Posterior probability
- Σⱼ P(A|Bⱼ)P(Bⱼ): Evidence/marginal likelihood

=======================================================

3. RANDOM VARIABLES AND DISTRIBUTIONS
=====================================

3.1 Random Variables:
--------------------
Random variable X: Function X: Ω → ℝ that is measurable
(Pre-image of any Borel set is in ℱ)

Types:
- Discrete: Takes countable values
- Continuous: Takes uncountable values
- Mixed: Combination of discrete and continuous

3.2 Cumulative Distribution Function (CDF):
------------------------------------------
F_X(x) = P(X ≤ x) for x ∈ ℝ

Properties:
- Non-decreasing: x ≤ y ⟹ F_X(x) ≤ F_X(y)
- Right-continuous: lim_{h→0⁺} F_X(x+h) = F_X(x)
- Limits: lim_{x→-∞} F_X(x) = 0, lim_{x→∞} F_X(x) = 1
- P(a < X ≤ b) = F_X(b) - F_X(a)

3.3 Probability Mass Function (PMF):
-----------------------------------
For discrete random variable:
p_X(x) = P(X = x)

Properties:
- p_X(x) ≥ 0 for all x
- Σₓ p_X(x) = 1
- F_X(x) = Σ_{y≤x} p_X(y)

3.4 Probability Density Function (PDF):
--------------------------------------
For continuous random variable:
f_X(x) = dF_X(x)/dx (when derivative exists)

Properties:
- f_X(x) ≥ 0 for all x
- ∫_{-∞}^∞ f_X(x)dx = 1
- P(a ≤ X ≤ b) = ∫_a^b f_X(x)dx
- F_X(x) = ∫_{-∞}^x f_X(t)dt

3.5 Common Discrete Distributions:
----------------------------------
Bernoulli(p): X ∈ {0,1}
P(X = 1) = p, P(X = 0) = 1-p

Binomial(n,p): X ∈ {0,1,...,n}
P(X = k) = (n choose k)p^k(1-p)^{n-k}

Poisson(λ): X ∈ {0,1,2,...}
P(X = k) = e^{-λ}λ^k/k!

Geometric(p): X ∈ {1,2,3,...}
P(X = k) = (1-p)^{k-1}p

3.6 Common Continuous Distributions:
-----------------------------------
Uniform(a,b): f_X(x) = 1/(b-a) for x ∈ [a,b]

Normal(μ,σ²): f_X(x) = (1/√(2πσ²))exp(-(x-μ)²/(2σ²))

Exponential(λ): f_X(x) = λe^{-λx} for x ≥ 0

Gamma(α,β): f_X(x) = (β^α/Γ(α))x^{α-1}e^{-βx} for x ≥ 0

Beta(α,β): f_X(x) = (Γ(α+β)/(Γ(α)Γ(β)))x^{α-1}(1-x)^{β-1} for x ∈ [0,1]

=======================================================

4. EXPECTATION AND MOMENTS
==========================

4.1 Expected Value:
------------------
For discrete X: E[X] = Σₓ x·p_X(x)
For continuous X: E[X] = ∫_{-∞}^∞ x·f_X(x)dx

Properties:
- Linearity: E[aX + bY] = aE[X] + bE[Y]
- Monotonicity: X ≤ Y ⟹ E[X] ≤ E[Y]
- E[c] = c for constant c

4.2 Law of Total Expectation:
----------------------------
E[X] = E[E[X|Y]]

Discrete case: E[X] = Σᵧ E[X|Y = y]P(Y = y)
Continuous case: E[X] = ∫ E[X|Y = y]f_Y(y)dy

4.3 Variance and Standard Deviation:
-----------------------------------
Var(X) = E[(X - E[X])²] = E[X²] - (E[X])²
SD(X) = √Var(X)

Properties:
- Var(aX + b) = a²Var(X)
- If X, Y independent: Var(X + Y) = Var(X) + Var(Y)

4.4 Higher Moments:
------------------
k-th moment: μₖ = E[Xᵏ]
k-th central moment: μₖ' = E[(X - μ)ᵏ]

Skewness: γ₁ = μ₃'/(μ₂')^{3/2}
Kurtosis: γ₂ = μ₄'/(μ₂')² - 3

4.5 Moment Generating Function (MGF):
------------------------------------
M_X(t) = E[e^{tX}] for t in neighborhood of 0

Properties:
- μₖ = M_X^{(k)}(0) (k-th derivative at 0)
- Uniquely determines distribution
- M_{aX+b}(t) = e^{bt}M_X(at)
- If X, Y independent: M_{X+Y}(t) = M_X(t)M_Y(t)

4.6 Characteristic Function:
---------------------------
φ_X(t) = E[e^{itX}] for all t ∈ ℝ

Properties:
- Always exists (unlike MGF)
- |φ_X(t)| ≤ 1
- φ_X(0) = 1
- Uniquely determines distribution
- Fourier transform of PDF

=======================================================

5. JOINT AND CONDITIONAL PROBABILITY
====================================

5.1 Joint Distributions:
-----------------------
For random variables X, Y:

Joint CDF: F_{X,Y}(x,y) = P(X ≤ x, Y ≤ y)

Joint PMF (discrete): p_{X,Y}(x,y) = P(X = x, Y = y)

Joint PDF (continuous): f_{X,Y}(x,y) = ∂²F_{X,Y}(x,y)/(∂x∂y)

5.2 Marginal Distributions:
--------------------------
Discrete: p_X(x) = Σᵧ p_{X,Y}(x,y)
Continuous: f_X(x) = ∫ f_{X,Y}(x,y)dy

Marginal CDF: F_X(x) = F_{X,Y}(x,∞)

5.3 Conditional Distributions:
-----------------------------
Discrete: p_{X|Y}(x|y) = p_{X,Y}(x,y)/p_Y(y) for p_Y(y) > 0
Continuous: f_{X|Y}(x|y) = f_{X,Y}(x,y)/f_Y(y) for f_Y(y) > 0

Properties:
- p_{X|Y}(·|y) and f_{X|Y}(·|y) are valid probability distributions
- Total probability: p_X(x) = Σᵧ p_{X|Y}(x|y)p_Y(y)

5.4 Covariance and Correlation:
------------------------------
Covariance: Cov(X,Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]

Correlation: ρ(X,Y) = Cov(X,Y)/(SD(X)·SD(Y))

Properties:
- -1 ≤ ρ(X,Y) ≤ 1
- |ρ(X,Y)| = 1 iff Y = aX + b for some a ≠ 0
- Cov(aX + b, cY + d) = ac·Cov(X,Y)

5.5 Multivariate Distributions:
------------------------------
For random vector X = (X₁, ..., Xₙ):

Joint PDF: f_X(x₁, ..., xₙ)
Mean vector: μ = E[X] = (E[X₁], ..., E[Xₙ])ᵀ
Covariance matrix: Σ = E[(X - μ)(X - μ)ᵀ]

Σᵢⱼ = Cov(Xᵢ, Xⱼ)

=======================================================

6. INDEPENDENCE AND CONDITIONAL INDEPENDENCE
============================================

6.1 Independence:
----------------
Events A and B are independent if:
P(A ∩ B) = P(A)P(B)

Equivalent conditions:
- P(A|B) = P(A) (when P(B) > 0)
- P(B|A) = P(B) (when P(A) > 0)

6.2 Independence of Random Variables:
------------------------------------
X and Y are independent if:
F_{X,Y}(x,y) = F_X(x)F_Y(y) for all x,y

Equivalent conditions:
- p_{X,Y}(x,y) = p_X(x)p_Y(y) (discrete case)
- f_{X,Y}(x,y) = f_X(x)f_Y(y) (continuous case)
- E[g(X)h(Y)] = E[g(X)]E[h(Y)] for any functions g,h

6.3 Mutual Independence:
-----------------------
X₁, ..., Xₙ are mutually independent if:
F_{X₁,...,Xₙ}(x₁, ..., xₙ) = ∏ᵢ F_{Xᵢ}(xᵢ)

Pairwise independence ≠ mutual independence

6.4 Conditional Independence:
----------------------------
X and Y are conditionally independent given Z if:
P(X = x, Y = y | Z = z) = P(X = x | Z = z)P(Y = y | Z = z)

Notation: X ⊥ Y | Z

Properties:
- X ⊥ Y | Z ⟹ P(X | Y, Z) = P(X | Z)
- Symmetry: X ⊥ Y | Z ⟺ Y ⊥ X | Z
- May not hold marginally: X ⊥ Y | Z ⟹ X ⊥ Y

6.5 Graphical Models:
--------------------
Independence relationships can be represented graphically:

Bayesian Networks: Directed acyclic graphs
- Nodes represent random variables
- Edges represent direct dependencies
- d-separation determines conditional independence

Markov Random Fields: Undirected graphs
- Markov property: X ⊥ Y | Z where Z separates X and Y

=======================================================

7. LIMIT THEOREMS
================

7.1 Modes of Convergence:
------------------------
Let X₁, X₂, ... be sequence of random variables

Convergence in probability: Xₙ →ᵖ X if
lim_{n→∞} P(|Xₙ - X| > ε) = 0 for all ε > 0

Almost sure convergence: Xₙ →ᵃ·ˢ· X if
P(lim_{n→∞} Xₙ = X) = 1

Convergence in distribution: Xₙ →ᵈ X if
lim_{n→∞} F_{Xₙ}(x) = F_X(x) at all continuity points of F_X

Convergence in Lᵖ: Xₙ →ᴸᵖ X if
lim_{n→∞} E[|Xₙ - X|ᵖ] = 0

Relationships: a.s. ⟹ prob. ⟹ dist., Lᵖ ⟹ prob.

7.2 Law of Large Numbers:
------------------------
Weak Law: If X₁, X₂, ... are i.i.d. with finite mean μ:
X̄ₙ = (1/n)Σᵢ₌₁ⁿ Xᵢ →ᵖ μ

Strong Law: Under same conditions:
X̄ₙ →ᵃ·ˢ· μ

Interpretation: Sample mean converges to population mean

7.3 Central Limit Theorem:
--------------------------
If X₁, X₂, ... are i.i.d. with mean μ and variance σ²:
√n(X̄ₙ - μ)/σ →ᵈ N(0,1)

Equivalently: X̄ₙ →ᵈ N(μ, σ²/n)

Extensions:
- Lindeberg-Lévy: For i.i.d. case
- Lyapunov: For independent, non-identical
- Multivariate CLT: For random vectors

7.4 Delta Method:
----------------
If √n(X̄ₙ - μ) →ᵈ N(0, σ²) and g is differentiable with g'(μ) ≠ 0:
√n(g(X̄ₙ) - g(μ)) →ᵈ N(0, (g'(μ))²σ²)

Multivariate delta method: For vector-valued functions

7.5 Continuous Mapping Theorem:
------------------------------
If Xₙ →ᵈ X and g is continuous at all points in support of X:
g(Xₙ) →ᵈ g(X)

Applications: Preserves convergence under continuous transformations

=======================================================

8. APPLICATIONS IN MACHINE LEARNING
===================================

8.1 Probabilistic Models:
------------------------
Machine learning as probabilistic inference:
- Data: D = {(x₁, y₁), ..., (xₙ, yₙ)}
- Model: p(y|x, θ) parameterized by θ
- Prior: p(θ)
- Posterior: p(θ|D) ∝ p(D|θ)p(θ)

8.2 Maximum Likelihood Estimation:
---------------------------------
θ̂_{MLE} = argmax_θ p(D|θ) = argmax_θ Σᵢ log p(yᵢ|xᵢ, θ)

Properties:
- Consistent: θ̂_{MLE} →ᵖ θ* as n → ∞
- Asymptotically normal: √n(θ̂_{MLE} - θ*) →ᵈ N(0, I(θ*)⁻¹)
- Asymptotically efficient: Achieves Cramér-Rao lower bound

8.3 Bayesian Inference:
----------------------
Posterior: p(θ|D) = p(D|θ)p(θ)/p(D)
Predictive: p(y*|x*, D) = ∫ p(y*|x*, θ)p(θ|D)dθ

Computational methods:
- Conjugate priors: Analytical solutions
- MCMC: Markov Chain Monte Carlo
- Variational inference: Approximate posterior

8.4 Classification and Regression:
---------------------------------
Linear regression: y = Xβ + ε where ε ~ N(0, σ²I)
Logistic regression: P(Y = 1|x) = σ(xᵀβ)

Naive Bayes: P(Y|X) ∝ P(Y)∏ᵢ P(Xᵢ|Y)
Assumes conditional independence given class

8.5 Generative vs Discriminative Models:
---------------------------------------
Generative: Model joint distribution p(x, y)
- Examples: Naive Bayes, HMM, LDA
- Can generate new samples
- Natural uncertainty quantification

Discriminative: Model conditional distribution p(y|x)
- Examples: Logistic regression, SVM, neural networks
- Often better predictive performance
- Directly optimizes decision boundary

8.6 Regularization as Prior:
---------------------------
Ridge regression: L2 penalty corresponds to Gaussian prior
Lasso regression: L1 penalty corresponds to Laplace prior
Elastic net: Combination of L1 and L2

Bayesian interpretation provides principled approach to regularization

8.7 Cross-Validation and Model Selection:
----------------------------------------
Leave-one-out CV: E[L(ŷ_{-i}, yᵢ)]
K-fold CV: Average loss across K folds

Information criteria:
- AIC: -2 log L + 2p (Akaike Information Criterion)
- BIC: -2 log L + p log n (Bayesian Information Criterion)

Trade-off between fit and complexity

8.8 Concentration Inequalities:
------------------------------
Hoeffding's inequality: For bounded random variables
P(|X̄ₙ - μ| ≥ t) ≤ 2exp(-2nt²/(b-a)²)

McDiarmid's inequality: For functions with bounded differences
Useful for generalization bounds in learning theory

8.9 PAC Learning Framework:
--------------------------
Probably Approximately Correct learning:
P(L(ĥ) - L(h*) ≤ ε) ≥ 1 - δ

Sample complexity bounds using concentration inequalities
VC dimension determines complexity of hypothesis class

Key Insights for ML:
- Probability provides mathematical foundation for uncertainty
- Central limit theorem justifies normal approximations
- Independence assumptions enable tractable models
- Bayesian framework unifies learning and inference
- Concentration inequalities provide generalization guarantees
- Understanding distributions crucial for model choice

=======================================================
END OF DOCUMENT 