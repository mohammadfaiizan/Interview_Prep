MULTIVARIATE STATISTICS AND DISTRIBUTIONS - Advanced Statistical Methods
=====================================================================

TABLE OF CONTENTS:
1. Multivariate Distributions and Density Functions
2. Multivariate Normal Distribution
3. Covariance Structures and Dependence
4. Principal Components and Factor Analysis
5. Clustering and Classification Methods
6. Hypothesis Testing for Multivariate Data
7. Time Series and Stochastic Processes
8. Applications in High-Dimensional Machine Learning

=======================================================

1. MULTIVARIATE DISTRIBUTIONS AND DENSITY FUNCTIONS
===================================================

1.1 Joint Distributions:
-----------------------
For random vector X = (X₁, ..., Xₚ)ᵀ ∈ ℝᵖ:

Joint CDF: F(x₁, ..., xₚ) = P(X₁ ≤ x₁, ..., Xₚ ≤ xₚ)
Joint PDF: f(x₁, ..., xₚ) = ∂ᵖF(x₁, ..., xₚ)/(∂x₁...∂xₚ)

Properties:
- ∫...∫ f(x₁, ..., xₚ)dx₁...dxₚ = 1
- f(x₁, ..., xₚ) ≥ 0
- P(X ∈ A) = ∫_A f(x)dx

1.2 Marginal and Conditional Distributions:
------------------------------------------
Marginal PDF of Xᵢ:
fᵢ(xᵢ) = ∫...∫ f(x₁, ..., xₚ)dx₁...dxᵢ₋₁dxᵢ₊₁...dxₚ

Conditional PDF:
f(xᵢ|x₋ᵢ) = f(x₁, ..., xₚ)/f₋ᵢ(x₋ᵢ)

where x₋ᵢ denotes all components except xᵢ

1.3 Transformation of Random Vectors:
------------------------------------
Linear transformation: Y = AX + b
fᵧ(y) = fₓ(A⁻¹(y - b))/|det(A)|

General transformation: Y = g(X)
fᵧ(y) = fₓ(g⁻¹(y))|J| where J is Jacobian determinant

Jacobian matrix: J = ∂(x₁, ..., xₚ)/∂(y₁, ..., yₚ)

1.4 Moment Generating Functions:
-------------------------------
Multivariate MGF: M_X(t) = E[exp(tᵀX)] where t = (t₁, ..., tₚ)ᵀ

Properties:
- Moments: E[X₁^k₁...Xₚ^kₚ] = ∂^(k₁+...+kₚ)M_X(t)/∂t₁^k₁...∂tₚ^kₚ|_{t=0}
- Independence: M_{X,Y}(s,t) = M_X(s)M_Y(t) iff X ⊥ Y
- Linear combinations: M_{aᵀX}(t) = M_X(ta)

1.5 Characteristic Functions:
----------------------------
Multivariate CF: φ_X(t) = E[exp(itᵀX)]

Properties:
- Always exists
- Uniquely determines distribution
- φ_X(0) = 1, |φ_X(t)| ≤ 1
- Fourier transform relationship

=======================================================

2. MULTIVARIATE NORMAL DISTRIBUTION
===================================

2.1 Definition and Density:
---------------------------
X ~ N_p(μ, Σ) if:
f(x) = (2π)^(-p/2)|Σ|^(-1/2)exp(-½(x-μ)ᵀΣ⁻¹(x-μ))

Parameters:
- μ ∈ ℝᵖ: mean vector
- Σ ∈ ℝᵖˣᵖ: covariance matrix (positive definite)

Special cases:
- p = 1: Univariate normal
- Σ = σ²I: Spherical normal
- Σ diagonal: Independent normals

2.2 Properties:
--------------
Linear combinations: If X ~ N_p(μ, Σ), then
aᵀX ~ N(aᵀμ, aᵀΣa) for any a ∈ ℝᵖ

Affine transformations: If Y = AX + b, then
Y ~ N_q(Aμ + b, AΣAᵀ)

Marginal distributions: Xᵢ ~ N(μᵢ, Σᵢᵢ)
For subset S: X_S ~ N(μ_S, Σ_{SS})

2.3 Conditional Distributions:
-----------------------------
Partition X = (X₁ᵀ, X₂ᵀ)ᵀ with corresponding partition:
μ = (μ₁ᵀ, μ₂ᵀ)ᵀ, Σ = [Σ₁₁ Σ₁₂; Σ₂₁ Σ₂₂]

Conditional distribution:
X₁|X₂ = x₂ ~ N(μ₁|₂, Σ₁|₂)

where:
μ₁|₂ = μ₁ + Σ₁₂Σ₂₂⁻¹(x₂ - μ₂)
Σ₁|₂ = Σ₁₁ - Σ₁₂Σ₂₂⁻¹Σ₂₁

2.4 Independence and Correlation:
--------------------------------
For multivariate normal:
Xᵢ ⊥ Xⱼ ⟺ Cov(Xᵢ, Xⱼ) = 0 ⟺ Σᵢⱼ = 0

General principle: Zero correlation implies independence

Partial correlation:
ρᵢⱼ|S = Corr(Xᵢ, Xⱼ|X_S) = -Σ⁻¹ᵢⱼ/√(Σ⁻¹ᵢᵢΣ⁻¹ⱼⱼ)

2.5 Quadratic Forms:
-------------------
If X ~ N_p(μ, Σ), then:
(X - μ)ᵀΣ⁻¹(X - μ) ~ χ²_p

Hotelling's T² statistic:
T² = n(X̄ - μ₀)ᵀS⁻¹(X̄ - μ₀)

where S is sample covariance matrix

Distribution: (n-p)T²/((n-1)p) ~ F_{p,n-p}

2.6 Maximum Likelihood Estimation:
---------------------------------
For sample X₁, ..., Xₙ ~ N_p(μ, Σ):

μ̂ = X̄ = (1/n)Σᵢ₌₁ⁿ Xᵢ
Σ̂ = (1/n)Σᵢ₌₁ⁿ (Xᵢ - X̄)(Xᵢ - X̄)ᵀ

Properties:
- μ̂ ~ N_p(μ, Σ/n)
- (n-1)S ~ Wishart_p(n-1, Σ)
- X̄ and S are independent

=======================================================

3. COVARIANCE STRUCTURES AND DEPENDENCE
=======================================

3.1 Covariance Matrix:
---------------------
Σ = E[(X - μ)(X - μ)ᵀ] = [Cov(Xᵢ, Xⱼ)]

Properties:
- Symmetric: Σ = Σᵀ
- Positive semidefinite: aᵀΣa ≥ 0 for all a
- Diagonal elements: Σᵢᵢ = Var(Xᵢ)

Correlation matrix:
R = [ρᵢⱼ] where ρᵢⱼ = Σᵢⱼ/√(ΣᵢᵢΣⱼⱼ)

Relationship: Σ = DRD where D = diag(√Σ₁₁, ..., √Σₚₚ)

3.2 Special Covariance Structures:
---------------------------------
Compound symmetry: Σᵢⱼ = σ² if i = j, ρσ² if i ≠ j

Autoregressive AR(1): Σᵢⱼ = σ²ρ^|i-j|

Toeplitz: Σᵢⱼ depends only on |i - j|

Block diagonal: Variables grouped into independent blocks

Factor structure: Σ = ΛΛᵀ + Ψ where Λ is loadings, Ψ diagonal

3.3 Wishart Distribution:
------------------------
If X₁, ..., Xₙ ~ N_p(0, Σ), then
W = Σᵢ₌₁ⁿ XᵢXᵢᵀ ~ Wishart_p(n, Σ)

Density:
f(W) = |W|^((n-p-1)/2)exp(-½tr(Σ⁻¹W))/(2^(np/2)|Σ|^(n/2)Γ_p(n/2))

Properties:
- E[W] = nΣ
- Mode: (n - p - 1)Σ (for n > p + 1)
- Additive: W₁ + W₂ ~ Wishart_p(n₁ + n₂, Σ)

3.4 Inverse Wishart Distribution:
-------------------------------
W⁻¹ ~ IW_p(ν, Ψ) if W ~ Wishart_p(ν, Ψ⁻¹)

Properties:
- E[W⁻¹] = Ψ/(ν - p - 1) for ν > p + 1
- Conjugate prior for covariance matrix in Bayesian analysis

3.5 Copulas:
-----------
Copula C: Function linking marginal distributions to joint distribution
F(x₁, ..., xₚ) = C(F₁(x₁), ..., Fₚ(xₚ))

Sklar's theorem: Every multivariate distribution can be written this way

Common copulas:
- Gaussian copula: Normal dependence structure
- t-copula: Heavy-tailed dependence
- Archimedean copulas: Clayton, Gumbel, Frank

Applications:
- Separate marginal behavior from dependence
- Model non-linear dependence
- Risk management in finance

=======================================================

4. PRINCIPAL COMPONENTS AND FACTOR ANALYSIS
===========================================

4.1 Principal Component Analysis (PCA):
--------------------------------------
Find orthogonal directions of maximum variance

Population version:
PC₁ = argmax_{||a||=1} Var(aᵀX) = argmax_{||a||=1} aᵀΣa

Solution: a₁ = first eigenvector of Σ
Variance: λ₁ = first eigenvalue of Σ

Subsequent components:
PCₖ = argmax_{||a||=1, a⊥PC₁,...,PCₖ₋₁} aᵀΣa

4.2 Sample PCA:
--------------
Sample covariance: S = (1/(n-1))Σᵢ₌₁ⁿ (xᵢ - x̄)(xᵢ - x̄)ᵀ
Eigendecomposition: S = PDPᵀ where D = diag(λ₁, ..., λₚ)

Principal component scores:
Yᵢₖ = pₖᵀ(xᵢ - x̄) for k-th component

Properties:
- Var(Yₖ) = λₖ
- Total variance: Σₖ λₖ = tr(S)
- Proportion explained by first k PCs: (Σⱼ₌₁ᵏ λⱼ)/(Σⱼ₌₁ᵖ λⱼ)

4.3 Factor Analysis:
-------------------
Model: X = μ + Λf + ε

where:
- f: k × 1 vector of common factors
- Λ: p × k matrix of factor loadings
- ε: p × 1 vector of specific factors

Assumptions:
- E[f] = 0, Var(f) = I
- E[ε] = 0, Var(ε) = Ψ (diagonal)
- f ⊥ ε

Covariance structure: Σ = ΛΛᵀ + Ψ

4.4 Factor Analysis Estimation:
------------------------------
Maximum likelihood:
Maximize: -½[log|Σ| + tr(S Σ⁻¹)]

where Σ = ΛΛᵀ + Ψ

Iterative algorithms:
- EM algorithm
- Scoring algorithm

Principal factor method:
Use principal components as initial estimates

4.5 Rotation Methods:
--------------------
Orthogonal rotations:
- Varimax: Maximize variance of squared loadings
- Quartimax: Simplify rows (variables)
- Equamax: Compromise between varimax and quartimax

Oblique rotations:
- Promax: Oblique rotation based on varimax
- Direct oblimin: Minimize cross-products

Goal: Achieve simple structure for interpretation

4.6 Model Selection:
-------------------
Number of factors:
- Scree plot: Look for elbow in eigenvalue plot
- Kaiser criterion: Retain factors with λ > 1
- Parallel analysis: Compare with random data
- Information criteria: AIC, BIC

Goodness of fit:
χ² = (n - 1 - (2p + 5)/6 - 2k/3)(log|Σ| - log|S| + tr(SΣ⁻¹) - p)

=======================================================

5. CLUSTERING AND CLASSIFICATION METHODS
========================================

5.1 Distance Measures:
---------------------
Euclidean distance: d(x, y) = ||x - y||₂ = √Σᵢ(xᵢ - yᵢ)²
Manhattan distance: d(x, y) = ||x - y||₁ = Σᵢ|xᵢ - yᵢ|
Mahalanobis distance: d(x, y) = √(x - y)ᵀΣ⁻¹(x - y)

Properties of Mahalanobis distance:
- Accounts for covariance structure
- Scale invariant
- Reduces to Euclidean when Σ = I

5.2 K-means Clustering:
----------------------
Objective: Minimize within-cluster sum of squares
W = Σₖ₌₁ᴷ Σᵢ∈Cₖ ||xᵢ - μₖ||²

Algorithm:
1. Initialize K cluster centers
2. Assign points to nearest center
3. Update centers: μₖ = (1/|Cₖ|)Σᵢ∈Cₖ xᵢ
4. Repeat until convergence

Extensions:
- K-means++: Smart initialization
- Mini-batch K-means: Stochastic updates
- Spherical K-means: For unit sphere data

5.3 Hierarchical Clustering:
---------------------------
Agglomerative (bottom-up):
1. Start with each point as cluster
2. Merge closest clusters
3. Repeat until single cluster

Linkage methods:
- Single: min{d(x, y) : x ∈ Cᵢ, y ∈ Cⱼ}
- Complete: max{d(x, y) : x ∈ Cᵢ, y ∈ Cⱼ}
- Average: (1/(|Cᵢ||Cⱼ|))Σₓ∈CᵢΣᵧ∈Cⱼ d(x, y)
- Ward: Minimize increase in within-cluster variance

Divisive (top-down): Start with all points, recursively split

5.4 Gaussian Mixture Models:
---------------------------
Model: f(x) = Σₖ₌₁ᴷ πₖ φ(x; μₖ, Σₖ)

where πₖ are mixing weights and φ is multivariate normal density

EM Algorithm:
E-step: γᵢₖ = P(component k | xᵢ) = πₖφ(xᵢ; μₖ, Σₖ)/f(xᵢ)
M-step: 
  πₖ = (1/n)Σᵢγᵢₖ
  μₖ = Σᵢγᵢₖxᵢ/Σᵢγᵢₖ
  Σₖ = Σᵢγᵢₖ(xᵢ - μₖ)(xᵢ - μₖ)ᵀ/Σᵢγᵢₖ

5.5 Model-Based Clustering:
--------------------------
Assume data comes from mixture of distributions
Use information criteria for model selection:
- Number of components
- Covariance structure (spherical, diagonal, full)

Covariance parameterizations:
- EII: Spherical, equal volume
- VII: Spherical, varying volume  
- EEI: Diagonal, equal volume and shape
- VVI: Diagonal, varying volume and shape
- EEE: Ellipsoidal, equal volume, shape, orientation
- VVV: Ellipsoidal, varying volume, shape, orientation

5.6 Cluster Validation:
----------------------
Internal measures:
- Silhouette width: s(i) = (b(i) - a(i))/max{a(i), b(i)}
- Calinski-Harabasz index: (B/(K-1))/(W/(n-K))
- Davies-Bouldin index: (1/K)Σₖmax_{j≠k}((σₖ + σⱼ)/d(μₖ, μⱼ))

External measures (when true clustering known):
- Adjusted Rand Index
- Normalized Mutual Information
- Fowlkes-Mallows Index

=======================================================

6. HYPOTHESIS TESTING FOR MULTIVARIATE DATA
===========================================

6.1 One-Sample Tests:
--------------------
Hotelling's T² test for H₀: μ = μ₀
T² = n(X̄ - μ₀)ᵀS⁻¹(X̄ - μ₀)

Test statistic: F = ((n-p)/(p(n-1)))T² ~ F_{p,n-p}

Assumptions:
- Multivariate normality
- Independent observations

6.2 Two-Sample Tests:
--------------------
Test H₀: μ₁ = μ₂

Pooled covariance: S_p = ((n₁-1)S₁ + (n₂-1)S₂)/(n₁ + n₂ - 2)
T² = (n₁n₂/(n₁ + n₂))(X̄₁ - X̄₂)ᵀS_p⁻¹(X̄₁ - X̄₂)

Test statistic: F = ((n₁ + n₂ - p - 1)/(p(n₁ + n₂ - 2)))T² ~ F_{p,n₁+n₂-p-1}

Unequal covariances (Behrens-Fisher problem):
More complex, no exact solution

6.3 MANOVA (Multivariate Analysis of Variance):
----------------------------------------------
Model: Y_{ij} = μ + αᵢ + ε_{ij}

where Y_{ij} is p-dimensional response vector

Test H₀: α₁ = α₂ = ... = αₖ = 0

Test statistics:
- Wilks' Λ = |E|/|H + E|
- Pillai's trace = tr(H(H + E)⁻¹)
- Hotelling-Lawley trace = tr(HE⁻¹)
- Roy's largest root = max eigenvalue of HE⁻¹

where H = between-groups matrix, E = within-groups matrix

6.4 Tests for Covariance Structure:
----------------------------------
Sphericity test: H₀: Σ = σ²I
Box's M test: Equal covariance matrices across groups
Bartlett's test: H₀: Σ is diagonal (independence)

Likelihood ratio tests generally used
Asymptotic χ² distributions under H₀

6.5 Profile Analysis:
--------------------
For repeated measures or multivariate groups:

Tests:
1. Parallelism: Equal profiles across groups
2. Levels: Equal overall levels
3. Flatness: Profiles are flat (all means equal)

Implemented using contrasts and MANOVA

6.6 Canonical Correlation Analysis:
----------------------------------
Study relationships between two sets of variables
X₁ ∈ ℝᵖ, X₂ ∈ ℝᵍ

Find linear combinations:
U = aᵀX₁, V = bᵀX₂

that maximize Corr(U, V)

Solution: Eigenvectors of matrices involving cross-covariances
Multiple canonical correlations (min(p,q) pairs)

=======================================================

7. TIME SERIES AND STOCHASTIC PROCESSES
=======================================

7.1 Stationary Processes:
-------------------------
Strictly stationary: Joint distribution invariant under time shifts
Weakly stationary: Constant mean and autocovariance depends only on lag

Autocovariance function: γ(h) = Cov(Xₜ, Xₜ₊ₕ)
Autocorrelation function: ρ(h) = γ(h)/γ(0)

Properties:
- γ(0) = Var(Xₜ)
- γ(h) = γ(-h) (symmetric)
- |ρ(h)| ≤ 1

7.2 ARMA Models:
---------------
Autoregressive AR(p): Xₜ = φ₁Xₜ₋₁ + ... + φₚXₜ₋ₚ + εₜ
Moving average MA(q): Xₜ = εₜ + θ₁εₜ₋₁ + ... + θᵍεₜ₋ᵍ
ARMA(p,q): Combination of AR and MA

Stationarity conditions:
- AR: Roots of φ(z) = 1 - φ₁z - ... - φₚzᵖ outside unit circle
- MA: Always stationary
- ARMA: AR stationarity condition

Invertibility: MA and ARMA models with MA roots outside unit circle

7.3 Vector Autoregression (VAR):
-------------------------------
Multivariate extension: Xₜ = Φ₁Xₜ₋₁ + ... + ΦₚXₜ₋ₚ + εₜ

where Xₜ ∈ ℝᵏ, Φᵢ ∈ ℝᵏˣᵏ, εₜ ~ N(0, Σ)

Estimation: Equation-by-equation OLS (same regressors)
Granger causality: Does X₁ help predict X₂?

Impulse response functions: Effect of shock on all variables
Forecast error variance decomposition

7.4 Cointegration:
-----------------
Non-stationary series with stationary linear combination

Error correction model:
ΔXₜ = α(βᵀXₜ₋₁) + Σᵢ₌₁ᵖ⁻¹ ΓᵢΔXₜ₋ᵢ + εₜ

where βᵀXₜ is cointegrating relationship

Johansen test: Maximum likelihood estimation of cointegrating vectors
Engle-Granger test: Two-step procedure

7.5 Spectral Analysis:
---------------------
Spectral density: f(ω) = (1/2π)Σₕ₌₋∞^∞ γ(h)e^{-iωh}

Fourier transform of autocovariance function
Decomposition of variance across frequencies

Periodogram: I(ω) = (1/2πn)|Σₜ₌₁ⁿ Xₜe^{-iωt}|²
Smoothed spectral estimators to reduce variance

Cross-spectral analysis for multivariate series:
- Coherence: Frequency-domain correlation
- Phase spectrum: Lead-lag relationships

7.6 State Space Models:
----------------------
State equation: αₜ₊₁ = Tₜαₜ + Rₜηₜ
Observation equation: yₜ = Zₜαₜ + εₜ

where αₜ is state vector, ηₜ and εₜ are errors

Kalman filter: Optimal recursive estimation
Applications: Missing data, irregular spacing, structural breaks

Dynamic linear models: Bayesian approach to state space
Sequential Monte Carlo: Particle filters for non-linear/non-Gaussian

=======================================================

8. APPLICATIONS IN HIGH-DIMENSIONAL MACHINE LEARNING
====================================================

8.1 High-Dimensional Statistics:
-------------------------------
When p ≈ n or p >> n:
- Classical asymptotics break down
- Curse of dimensionality
- Sparsity assumptions often needed

Random matrix theory: Behavior of eigenvalues/eigenvectors
Marchenko-Pastur law: Limiting spectral distribution

Concentration inequalities:
- Sub-Gaussian random variables
- Matrix concentration bounds
- Johnson-Lindenstrauss lemma

8.2 Regularized Estimation:
--------------------------
Ridge regression: β̂ = (XᵀX + λI)⁻¹Xᵀy
Lasso: β̂ = argmin{||y - Xβ||² + λ||β||₁}
Elastic net: Combination of ridge and lasso penalties

Multivariate responses:
Reduced rank regression: Rank constraint on coefficient matrix
Multi-task learning: Shared structure across tasks

Group lasso: Sparsity at group level
Fused lasso: Encourage smoothness in coefficients

8.3 Sparse Covariance Estimation:
---------------------------------
Graphical lasso: Sparse precision matrix estimation
Objective: log det(Θ) - tr(SΘ) - λ||Θ||₁

where Θ = Σ⁻¹ is precision matrix

Graph interpretation: Θᵢⱼ = 0 ⟺ Xᵢ ⊥ Xⱼ | X₋ᵢ,ⱼ

Banding/tapering: Assume covariance decreases with distance
Factor models: Low-rank plus diagonal structure

8.4 Dimensionality Reduction:
----------------------------
Sparse PCA: Principal components with sparse loadings
Random projections: Johnson-Lindenstrauss embedding
Manifold learning: Nonlinear dimensionality reduction

t-SNE: Preserve local neighborhood structure
UMAP: Uniform manifold approximation and projection
Autoencoders: Neural network-based dimensionality reduction

Dictionary learning: X ≈ DZ where D is dictionary, Z is sparse

8.5 Multiple Testing:
--------------------
Family-wise error rate (FWER) control:
- Bonferroni: α/m for m tests
- Holm: Step-down procedure
- Hochberg: Step-up procedure

False discovery rate (FDR) control:
- Benjamini-Hochberg: Linear step-up
- Benjamini-Krieger-Yekutieli: Two-stage procedure
- Storey's q-value: Estimate of FDR

Simultaneous confidence intervals:
- Scheffé's method: All linear combinations
- Tukey's method: Pairwise comparisons

8.6 Feature Selection:
---------------------
Filter methods: Score features independently
- Correlation with response
- Mutual information
- F-test, t-test

Wrapper methods: Use learning algorithm
- Forward/backward selection
- Recursive feature elimination

Embedded methods: Selection during model fitting
- Lasso path
- Random forests variable importance
- Stability selection

8.7 Network Analysis:
--------------------
Gaussian graphical models: Θᵢⱼ ≠ 0 ⟺ edge between i and j
Hub nodes: High degree vertices
Community detection: Graph partitioning

Scale-free networks: Power-law degree distribution
Small-world networks: High clustering, short path lengths

Dynamic networks: Time-varying graph structure
Multilayer networks: Multiple types of relationships

8.8 Modern Applications:
-----------------------
Genomics: Gene expression analysis, GWAS
- Multiple testing for thousands of genes/SNPs
- Pathway analysis using graph structure
- Batch effect correction

Neuroimaging: Brain connectivity analysis
- Functional connectivity networks
- Dimensionality reduction for voxel data
- Classification of brain states

Finance: Portfolio optimization, risk management
- High-dimensional covariance matrix estimation
- Factor models for returns
- Stress testing and scenario analysis

Text mining: Document-term matrices
- Latent semantic analysis (SVD)
- Topic models (probabilistic factor analysis)
- Sentiment analysis classification

Key Insights for ML:
- Multivariate methods essential for understanding complex data
- Dimensionality reduction crucial when p is large
- Sparsity assumptions enable statistical learning in high dimensions
- Graphical models reveal conditional independence structure
- Bootstrap and cross-validation adapt to multivariate setting
- Time series methods handle temporal dependence
- Multiple testing correction prevents false discoveries

=======================================================
END OF DOCUMENT 