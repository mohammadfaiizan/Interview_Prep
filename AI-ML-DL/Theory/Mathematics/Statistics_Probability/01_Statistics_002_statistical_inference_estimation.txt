STATISTICAL INFERENCE AND ESTIMATION - Theory and Methods for ML
==================================================================

TABLE OF CONTENTS:
1. Fundamentals of Statistical Inference
2. Point Estimation Theory
3. Maximum Likelihood Estimation
4. Bayesian Estimation
5. Confidence Intervals and Hypothesis Testing
6. Non-parametric Methods
7. Bootstrap and Resampling Methods
8. Applications in Machine Learning

=======================================================

1. FUNDAMENTALS OF STATISTICAL INFERENCE
========================================

1.1 Statistical Model:
---------------------
Statistical model: Family of probability distributions
M = {P_Œ∏ : Œ∏ ‚àà Œò}

where:
- Œ∏: parameter (unknown)
- Œò: parameter space
- P_Œ∏: probability distribution indexed by Œ∏

Examples:
- Normal: Œ∏ = (Œº, œÉ¬≤), Œò = ‚Ñù √ó ‚Ñù‚Çä
- Bernoulli: Œ∏ = p, Œò = [0,1]
- Regression: Œ∏ = Œ≤, Œò = ‚Ñù·µñ

1.2 Sample and Sampling Distribution:
------------------------------------
Sample: X‚ÇÅ, ..., X‚Çô ~ P_Œ∏ (i.i.d.)
Sample space: ùí≥‚Åø
Sampling distribution: Joint distribution of (X‚ÇÅ, ..., X‚Çô)

Sufficient statistic T(X‚ÇÅ, ..., X‚Çô):
Contains all information about Œ∏ in the sample

Factorization theorem:
T is sufficient iff p(x|Œ∏) = g(T(x), Œ∏)h(x)

1.3 Likelihood Function:
-----------------------
Likelihood: L(Œ∏) = L(Œ∏; x‚ÇÅ, ..., x‚Çô) = ‚àè·µ¢‚Çå‚ÇÅ‚Åø f(x·µ¢; Œ∏)
Log-likelihood: ‚Ñì(Œ∏) = log L(Œ∏) = Œ£·µ¢‚Çå‚ÇÅ‚Åø log f(x·µ¢; Œ∏)

Properties:
- Function of Œ∏ for fixed data
- Not a probability density in Œ∏
- Contains information about plausible parameter values

1.4 Fisher Information:
----------------------
Fisher information: I(Œ∏) = E[(-‚àÇ¬≤‚Ñì(Œ∏)/‚àÇŒ∏¬≤)]

Alternative: I(Œ∏) = E[(‚àÇ‚Ñì(Œ∏)/‚àÇŒ∏)¬≤]

Fisher information matrix (multivariate):
I(Œ∏)·µ¢‚±º = E[-‚àÇ¬≤‚Ñì(Œ∏)/(‚àÇŒ∏·µ¢‚àÇŒ∏‚±º)]

Properties:
- Measures information about Œ∏ in single observation
- Related to curvature of log-likelihood
- Higher information ‚üπ more precise estimation

1.5 Exponential Family:
----------------------
f(x; Œ∏) = h(x)exp(Œ∑(Œ∏)·µÄT(x) - A(Œ∏))

where:
- Œ∑(Œ∏): natural parameter
- T(x): sufficient statistic
- A(Œ∏): log-partition function

Properties:
- Many common distributions belong to exponential family
- Nice theoretical properties for inference
- Sufficient statistics have simple form

=======================================================

2. POINT ESTIMATION THEORY
==========================

2.1 Estimator Properties:
-------------------------
Estimator: Function Œ∏ÃÇ‚Çô = Œ∏ÃÇ‚Çô(X‚ÇÅ, ..., X‚Çô)
Estimate: Realized value Œ∏ÃÇ‚Çô(x‚ÇÅ, ..., x‚Çô)

Unbiasedness: E[Œ∏ÃÇ‚Çô] = Œ∏ for all Œ∏ ‚àà Œò
Bias: bias(Œ∏ÃÇ‚Çô) = E[Œ∏ÃÇ‚Çô] - Œ∏

Consistency: Œ∏ÃÇ‚Çô ‚Üí·µñ Œ∏ as n ‚Üí ‚àû
Strong consistency: Œ∏ÃÇ‚Çô ‚Üí·µÉ¬∑À¢¬∑ Œ∏ as n ‚Üí ‚àû

2.2 Mean Squared Error:
----------------------
MSE(Œ∏ÃÇ‚Çô) = E[(Œ∏ÃÇ‚Çô - Œ∏)¬≤] = Var(Œ∏ÃÇ‚Çô) + (bias(Œ∏ÃÇ‚Çô))¬≤

Bias-variance decomposition:
- Unbiased estimators: MSE = Variance
- Biased estimators: Trade-off between bias and variance

Efficiency: Estimator with smallest MSE (or variance if unbiased)

2.3 Cram√©r-Rao Lower Bound:
---------------------------
For unbiased estimator Œ∏ÃÇ:
Var(Œ∏ÃÇ) ‚â• 1/I(Œ∏) (univariate case)
Var(Œ∏ÃÇ) ‚â• I(Œ∏)‚Åª¬π (multivariate case)

Efficient estimator: Achieves Cram√©r-Rao bound
- Rare in finite samples
- Often achievable asymptotically

2.4 Sufficiency and Completeness:
---------------------------------
Rao-Blackwell theorem: If T is sufficient for Œ∏ and Œ∏ÃÇ is unbiased:
Œ∏ÃÉ = E[Œ∏ÃÇ|T] is unbiased and Var(Œ∏ÃÉ) ‚â§ Var(Œ∏ÃÇ)

Complete statistic T: E[g(T)] = 0 for all Œ∏ ‚üπ g(T) = 0 a.s.

Lehmann-Scheff√© theorem: If T is complete sufficient and Œ∏ÃÇ(T) is unbiased,
then Œ∏ÃÇ(T) is UMVUE (uniformly minimum variance unbiased estimator)

2.5 Method of Moments:
---------------------
Equate sample moments to population moments:
(1/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø X·µ¢·µè = E[X·µè] for k = 1, ..., p

Solve for parameters Œ∏‚ÇÅ, ..., Œ∏‚Çö

Properties:
- Simple to compute
- Consistent under regularity conditions
- May not be efficient
- Can yield estimates outside parameter space

=======================================================

3. MAXIMUM LIKELIHOOD ESTIMATION
================================

3.1 Maximum Likelihood Estimator (MLE):
---------------------------------------
Œ∏ÃÇ_{MLE} = argmax_Œ∏ L(Œ∏) = argmax_Œ∏ ‚Ñì(Œ∏)

First-order condition (score equation):
‚àÇ‚Ñì(Œ∏)/‚àÇŒ∏|_{Œ∏=Œ∏ÃÇ} = 0

Properties under regularity conditions:
- Consistent: Œ∏ÃÇ_{MLE} ‚Üí·µñ Œ∏‚ÇÄ
- Asymptotically normal: ‚àön(Œ∏ÃÇ_{MLE} - Œ∏‚ÇÄ) ‚Üí·µà N(0, I(Œ∏‚ÇÄ)‚Åª¬π)
- Asymptotically efficient: Achieves Cram√©r-Rao bound
- Invariant: If œÑ = g(Œ∏), then œÑÃÇ_{MLE} = g(Œ∏ÃÇ_{MLE})

3.2 Regularity Conditions:
--------------------------
1. Œò is open subset of ‚Ñù·µñ
2. Support of f(x; Œ∏) doesn't depend on Œ∏
3. f(x; Œ∏) is differentiable in Œ∏
4. Integration and differentiation can be interchanged
5. Fisher information is finite and positive definite

3.3 Computational Methods:
-------------------------
Newton-Raphson: Œ∏_{k+1} = Œ∏‚Çñ - [‚àá¬≤‚Ñì(Œ∏‚Çñ)]‚Åª¬π‚àá‚Ñì(Œ∏‚Çñ)

Fisher scoring: Œ∏_{k+1} = Œ∏‚Çñ + I(Œ∏‚Çñ)‚Åª¬π‚àá‚Ñì(Œ∏‚Çñ)

EM Algorithm (for missing data/latent variables):
E-step: Q(Œ∏|Œ∏‚ÅΩ·µè‚Åæ) = E[log p(X,Z|Œ∏)|X,Œ∏‚ÅΩ·µè‚Åæ]
M-step: Œ∏‚ÅΩ·µè‚Å∫¬π‚Åæ = argmax_Œ∏ Q(Œ∏|Œ∏‚ÅΩ·µè‚Åæ)

3.4 Examples:
------------
Normal distribution (Œº, œÉ¬≤ unknown):
ŒºÃÇ = XÃÑ‚Çô = (1/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø X·µ¢
œÉÃÇ¬≤ = (1/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø (X·µ¢ - XÃÑ‚Çô)¬≤

Exponential distribution (Œª unknown):
ŒªÃÇ = 1/XÃÑ‚Çô

Logistic regression:
‚Ñì(Œ≤) = Œ£·µ¢‚Çå‚ÇÅ‚Åø [y·µ¢x·µ¢·µÄŒ≤ - log(1 + exp(x·µ¢·µÄŒ≤))]

3.5 Profile Likelihood:
----------------------
For Œ∏ = (œà, Œª) where œà is parameter of interest:
Profile likelihood: L_p(œà) = max_Œª L(œà, Œª)

Used when:
- Only interested in subset of parameters
- Nuisance parameters present
- Dimension reduction needed

=======================================================

4. BAYESIAN ESTIMATION
======================

4.1 Bayesian Framework:
----------------------
Prior: œÄ(Œ∏)
Likelihood: L(Œ∏|x) = f(x|Œ∏)
Posterior: œÄ(Œ∏|x) ‚àù L(Œ∏|x)œÄ(Œ∏)

Bayes' theorem:
œÄ(Œ∏|x) = L(Œ∏|x)œÄ(Œ∏) / ‚à´ L(Œ∏|x)œÄ(Œ∏)dŒ∏

Predictive distribution:
p(x_{new}|x) = ‚à´ f(x_{new}|Œ∏)œÄ(Œ∏|x)dŒ∏

4.2 Point Estimates:
-------------------
Posterior mean: Œ∏ÃÇ_{PM} = E[Œ∏|x]
Posterior median: Œ∏ÃÇ_{Med} = median(Œ∏|x)
Maximum a posteriori (MAP): Œ∏ÃÇ_{MAP} = argmax_Œ∏ œÄ(Œ∏|x)

Loss functions:
- Squared loss: L(Œ∏, a) = (Œ∏ - a)¬≤ ‚üπ optimal action = posterior mean
- Absolute loss: L(Œ∏, a) = |Œ∏ - a| ‚üπ optimal action = posterior median
- 0-1 loss: L(Œ∏, a) = I(Œ∏ ‚â† a) ‚üπ optimal action = posterior mode

4.3 Conjugate Priors:
---------------------
Conjugate family: Prior and posterior have same functional form

Examples:
- Normal likelihood, normal prior ‚üπ normal posterior
- Binomial likelihood, beta prior ‚üπ beta posterior
- Poisson likelihood, gamma prior ‚üπ gamma posterior

Beta-Binomial: X|p ~ Binomial(n, p), p ~ Beta(Œ±, Œ≤)
‚üπ p|x ~ Beta(Œ± + x, Œ≤ + n - x)

Normal-Normal: X|Œº ~ N(Œº, œÉ¬≤), Œº ~ N(Œº‚ÇÄ, œÑ¬≤)
‚üπ Œº|x ~ N((œÑ¬≤xÃÑ + œÉ¬≤Œº‚ÇÄ)/(œÑ¬≤ + œÉ¬≤/n), (œÑ¬≤œÉ¬≤/n)/(œÑ¬≤ + œÉ¬≤/n))

4.4 Non-informative Priors:
---------------------------
Jeffreys prior: œÄ(Œ∏) ‚àù ‚àödet(I(Œ∏))
- Invariant under reparameterization
- Often improper but leads to proper posterior

Reference priors: Maximize expected information gain
Uniform priors: œÄ(Œ∏) ‚àù 1 (on appropriate scale)

4.5 Empirical Bayes:
-------------------
Estimate hyperparameters from data:
Œ∏·µ¢|Œº ~ œÄ(Œ∏·µ¢|Œº), Œº unknown
Estimate ŒºÃÇ from marginal distribution
Use œÄ(Œ∏·µ¢|ŒºÃÇ) as "prior"

Two-stage procedure:
1. Estimate hyperparameters
2. Compute posterior using estimated hyperparameters

4.6 Computational Methods:
-------------------------
Markov Chain Monte Carlo (MCMC):
- Metropolis-Hastings algorithm
- Gibbs sampling
- Hamiltonian Monte Carlo

Variational inference:
- Approximate posterior with simpler distribution
- Minimize KL divergence
- Faster than MCMC but less accurate

=======================================================

5. CONFIDENCE INTERVALS AND HYPOTHESIS TESTING
==============================================

5.1 Confidence Intervals:
------------------------
100(1-Œ±)% confidence interval: Random interval [L(X), U(X)] such that
P(Œ∏ ‚àà [L(X), U(X)]) = 1 - Œ± for all Œ∏

Interpretation: Long-run frequency of coverage

Pivotal quantity method:
Find Q(X, Œ∏) with known distribution independent of Œ∏
Use to construct interval

5.2 Large Sample Confidence Intervals:
-------------------------------------
Based on asymptotic normality of MLE:
Œ∏ÃÇ ¬± z_{Œ±/2}‚àö(√é(Œ∏ÃÇ)‚Åª¬π/n)

where √é(Œ∏ÃÇ) is observed Fisher information

Delta method for functions:
g(Œ∏ÃÇ) ¬± z_{Œ±/2}|g'(Œ∏ÃÇ)|‚àö(√é(Œ∏ÃÇ)‚Åª¬π/n)

Profile likelihood intervals:
{Œ∏ : 2[‚Ñì(Œ∏ÃÇ) - ‚Ñì(Œ∏)] ‚â§ œá¬≤_{1,Œ±}}

5.3 Hypothesis Testing:
----------------------
Null hypothesis: H‚ÇÄ: Œ∏ ‚àà Œò‚ÇÄ
Alternative hypothesis: H‚ÇÅ: Œ∏ ‚àà Œò‚ÇÅ (often Œò‚ÇÅ = Œò‚ÇÄ·∂ú)

Test statistic: T(X)
Rejection region: R = {x : T(x) ‚àà C}
p-value: P(T(X) ‚â• T(x_obs)|H‚ÇÄ)

Type I error: Œ± = P(reject H‚ÇÄ|H‚ÇÄ true)
Type II error: Œ≤ = P(accept H‚ÇÄ|H‚ÇÅ true)
Power: 1 - Œ≤ = P(reject H‚ÇÄ|H‚ÇÅ true)

5.4 Likelihood Ratio Test:
-------------------------
Test statistic: Œª(x) = L(Œ∏ÃÇ‚ÇÄ)/L(Œ∏ÃÇ)
where Œ∏ÃÇ‚ÇÄ = argmax_{Œ∏‚ààŒò‚ÇÄ} L(Œ∏), Œ∏ÃÇ = argmax_{Œ∏‚ààŒò} L(Œ∏)

Asymptotic distribution under H‚ÇÄ:
-2 log Œª(X) ‚Üí·µà œá¬≤_r where r = dim(Œò) - dim(Œò‚ÇÄ)

Score test: Based on score function at restricted estimate
Wald test: Based on unrestricted estimate and its standard error

5.5 Multiple Testing:
--------------------
Familywise error rate (FWER): P(at least one Type I error)
False discovery rate (FDR): E[V/R] where V = false discoveries, R = total discoveries

Bonferroni correction: Use Œ±/m for each of m tests
Controls FWER at level Œ±

Benjamini-Hochberg procedure: Controls FDR at level Œ±

5.6 Goodness of Fit Tests:
-------------------------
Kolmogorov-Smirnov test: Based on empirical CDF
D_n = sup_x |F_n(x) - F‚ÇÄ(x)|

Anderson-Darling test: Weighted version of KS test
More sensitive to tail differences

Chi-square test: For discrete distributions
X¬≤ = Œ£·µ¢ (O·µ¢ - E·µ¢)¬≤/E·µ¢ ~ œá¬≤_{k-1-p}

=======================================================

6. NON-PARAMETRIC METHODS
=========================

6.1 Empirical Distribution Function:
-----------------------------------
F_n(x) = (1/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø I(X·µ¢ ‚â§ x)

Properties:
- Unbiased: E[F_n(x)] = F(x)
- Consistent: F_n(x) ‚Üí·µÉ¬∑À¢¬∑ F(x)
- Glivenko-Cantelli: sup_x |F_n(x) - F(x)| ‚Üí·µÉ¬∑À¢¬∑ 0

Dvoretzky-Kiefer-Wolfowitz inequality:
P(sup_x |F_n(x) - F(x)| > Œµ) ‚â§ 2exp(-2nŒµ¬≤)

6.2 Kernel Density Estimation:
------------------------------
fÃÇ_h(x) = (1/nh)Œ£·µ¢‚Çå‚ÇÅ‚Åø K((x - X·µ¢)/h)

where K is kernel function and h is bandwidth

Common kernels:
- Gaussian: K(u) = (1/‚àö2œÄ)exp(-u¬≤/2)
- Epanechnikov: K(u) = (3/4)(1 - u¬≤)I(|u| ‚â§ 1)
- Uniform: K(u) = (1/2)I(|u| ‚â§ 1)

Bias-variance trade-off:
- Small h: Low bias, high variance
- Large h: High bias, low variance

Optimal bandwidth: h* ‚àù n^(-1/5) (for second-order kernels)

6.3 Non-parametric Regression:
-----------------------------
Nadaraya-Watson estimator:
mÃÇ(x) = Œ£·µ¢‚Çå‚ÇÅ‚Åø w_i(x)Y·µ¢

where w_i(x) = K((x - X·µ¢)/h) / Œ£‚±º‚Çå‚ÇÅ‚Åø K((x - X‚±º)/h)

Local polynomial regression:
Fit polynomial locally around each point

Smoothing splines:
Minimize: Œ£·µ¢‚Çå‚ÇÅ‚Åø (y·µ¢ - g(x·µ¢))¬≤ + Œª‚à´ (g''(t))¬≤dt

6.4 Rank-Based Methods:
----------------------
Wilcoxon signed-rank test: For paired data
W‚Å∫ = Œ£·µ¢:d·µ¢>0 R_i where R_i is rank of |d·µ¢|

Mann-Whitney U test: For two independent samples
U = Œ£·µ¢‚Çå‚ÇÅ‚Åø¬π Œ£‚±º‚Çå‚ÇÅ‚Åø¬≤ I(X_i > Y_j)

Spearman's rank correlation:
œÅ_s = 1 - (6Œ£·µ¢‚Çå‚ÇÅ‚Åø d_i¬≤)/(n(n¬≤-1))
where d_i = rank(X_i) - rank(Y_i)

6.5 Order Statistics:
--------------------
X‚Çç‚ÇÅ‚Çé ‚â§ X‚Çç‚ÇÇ‚Çé ‚â§ ... ‚â§ X‚Çç‚Çô‚Çé

Sample quantiles:
QÃÇ_p = X‚Çç‚åànp‚åâ‚Çé (simple definition)

Properties:
- Sample median: QÃÇ‚ÇÄ.‚ÇÖ
- Robust to outliers
- Asymptotically normal

=======================================================

7. BOOTSTRAP AND RESAMPLING METHODS
===================================

7.1 Bootstrap Principle:
-----------------------
Empirical distribution FÃÇ_n as proxy for unknown F
Bootstrap sample: X‚ÇÅ*, ..., X_n* sampled i.i.d. from FÃÇ_n
Bootstrap statistic: Œ∏ÃÇ* = T(X‚ÇÅ*, ..., X_n*)

Plug-in principle: Estimate F by FÃÇ_n, then compute quantities of interest

7.2 Non-parametric Bootstrap:
----------------------------
Algorithm:
1. Draw bootstrap sample (x‚ÇÅ*, ..., x_n*) from {x‚ÇÅ, ..., x_n} with replacement
2. Compute Œ∏ÃÇ* = T(x‚ÇÅ*, ..., x_n*)
3. Repeat B times to get Œ∏ÃÇ*‚ÇÅ, ..., Œ∏ÃÇ*_B
4. Use empirical distribution of Œ∏ÃÇ*·µ¢ to approximate distribution of Œ∏ÃÇ

Bootstrap estimate of bias: bias* = Œ∏ÃÇ*ÃÑ - Œ∏ÃÇ
Bootstrap estimate of variance: var* = (1/(B-1))Œ£·µ¢‚Çå‚ÇÅ·µá (Œ∏ÃÇ*·µ¢ - Œ∏ÃÇ*ÃÑ)¬≤

7.3 Bootstrap Confidence Intervals:
----------------------------------
Normal approximation: Œ∏ÃÇ ¬± z_{Œ±/2}‚àövar*

Percentile method: [Œ∏ÃÇ*_{(Œ±/2)}, Œ∏ÃÇ*_{(1-Œ±/2)}]

Bias-corrected and accelerated (BCa):
Adjusts for bias and skewness

Bootstrap-t: Uses studentized statistic

7.4 Parametric Bootstrap:
------------------------
1. Estimate parameters: Œ∏ÃÇ = Œ∏ÃÇ(x‚ÇÅ, ..., x_n)
2. Generate bootstrap sample from F_{Œ∏ÃÇ}
3. Compute bootstrap statistic
4. Repeat and analyze

Useful when parametric model is reasonable

7.5 Jackknife:
-------------
Delete-one jackknife:
Œ∏ÃÇ‚Çç·µ¢‚Çé = T(x‚ÇÅ, ..., x_{i-1}, x_{i+1}, ..., x_n)

Jackknife estimate of bias: bias_jack = (n-1)(Œ∏ÃÇ‚Çç.‚Çé - Œ∏ÃÇ)
where Œ∏ÃÇ‚Çç.‚Çé = (1/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø Œ∏ÃÇ‚Çç·µ¢‚Çé

Jackknife estimate of variance: var_jack = ((n-1)/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø (Œ∏ÃÇ‚Çç·µ¢‚Çé - Œ∏ÃÇ‚Çç.‚Çé)¬≤

7.6 Cross-Validation:
--------------------
Leave-one-out CV: Use n-1 observations to predict left-out observation
K-fold CV: Divide data into K folds, use K-1 folds for training

CV estimate of prediction error:
CV = (1/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø L(y·µ¢, ≈∑‚Çç‚Çã·µ¢‚Çé)

Bootstrap .632 estimator:
Err_{.632} = 0.368 √ó err + 0.632 √ó Err_boot

=======================================================

8. APPLICATIONS IN MACHINE LEARNING
===================================

8.1 Bias-Variance Decomposition:
-------------------------------
For prediction error at point x‚ÇÄ:
Err(x‚ÇÄ) = Bias¬≤(fÃÇ(x‚ÇÄ)) + Var(fÃÇ(x‚ÇÄ)) + œÉ¬≤

Irreducible error: œÉ¬≤ (noise)
Reducible error: Bias¬≤ + Variance

Trade-off:
- Complex models: Low bias, high variance
- Simple models: High bias, low variance

8.2 Model Selection and Regularization:
--------------------------------------
AIC: -2‚Ñì(Œ∏ÃÇ) + 2p (Akaike Information Criterion)
BIC: -2‚Ñì(Œ∏ÃÇ) + p log n (Bayesian Information Criterion)

Regularized likelihood:
‚Ñì_reg(Œ∏) = ‚Ñì(Œ∏) - Œª||Œ∏||_q

Lasso (q=1): Promotes sparsity
Ridge (q=2): Shrinks coefficients
Elastic net: Combination of L1 and L2

8.3 Ensemble Methods:
--------------------
Bagging: Bootstrap aggregating
- Train models on bootstrap samples
- Average predictions (regression) or vote (classification)
- Reduces variance

Boosting: Sequential learning
- Weight misclassified examples more heavily
- Combines weak learners into strong learner
- Can reduce bias and variance

Random forests: Bagging + random feature selection

8.4 Uncertainty Quantification:
------------------------------
Prediction intervals: Account for parameter uncertainty + noise
Credible intervals: Bayesian posterior intervals

Conformal prediction: Distribution-free prediction intervals
- Split data into training and calibration sets
- Use calibration set to determine interval size

Monte Carlo dropout: Approximate Bayesian inference in neural networks

8.5 Hypothesis Testing in ML:
----------------------------
Statistical significance of model differences:
- McNemar's test (paired comparisons)
- Wilcoxon signed-rank test
- Bootstrap tests

Multiple comparisons: Testing many models/hyperparameters
- Control family-wise error rate
- Use held-out test set sparingly

8.6 Causal Inference:
--------------------
Randomized experiments: Gold standard for causal inference
Observational studies: Require additional assumptions

Methods:
- Instrumental variables
- Regression discontinuity
- Difference-in-differences
- Propensity score matching

Confounding: Association ‚â† causation

8.7 A/B Testing:
---------------
Randomized controlled experiments for online systems

Statistical power: Probability of detecting effect if it exists
Sample size calculation: n ‚àù 1/Œî¬≤ where Œî is effect size

Multiple testing issues:
- Testing multiple metrics
- Peeking at results
- Post-hoc analyses

8.8 Bayesian Methods in ML:
---------------------------
Bayesian neural networks: Uncertainty over weights
Gaussian processes: Non-parametric Bayesian regression
Bayesian optimization: For hyperparameter tuning

Variational inference: Scalable approximate Bayesian methods
Markov Chain Monte Carlo: Exact sampling from posterior

8.9 Robustness and Diagnostics:
------------------------------
Outlier detection: Identify anomalous observations
Influence analysis: How much each observation affects results

Robust estimators:
- M-estimators: Generalize MLE with different loss functions
- Trimmed means: Remove extreme observations
- Huber estimator: Compromise between mean and median

Model diagnostics:
- Residual analysis
- Q-Q plots
- Goodness-of-fit tests

Key Insights for ML:
- Statistical inference provides rigorous framework for learning from data
- Bias-variance trade-off fundamental to model selection
- Bootstrap enables uncertainty quantification without distributional assumptions
- Hypothesis testing requires careful consideration of multiple comparisons
- Bayesian methods naturally incorporate prior knowledge and uncertainty
- Robustness important when model assumptions violated

=======================================================
END OF DOCUMENT 