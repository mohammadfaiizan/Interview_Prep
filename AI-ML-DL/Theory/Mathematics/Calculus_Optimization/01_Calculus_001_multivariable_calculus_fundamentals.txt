MULTIVARIABLE CALCULUS FUNDAMENTALS - For AI/ML Applications
============================================================

TABLE OF CONTENTS:
1. Multivariable Functions
2. Partial Derivatives
3. Gradients and Directional Derivatives
4. Jacobian Matrix
5. Hessian Matrix
6. Chain Rule in Multiple Variables
7. Critical Points and Optimization
8. Applications in Machine Learning

=======================================================

1. MULTIVARIABLE FUNCTIONS
==========================

Definition:
A multivariable function f: ℝⁿ → ℝᵐ maps vectors from n-dimensional 
input space to m-dimensional output space.

Notation:
- Scalar-valued: f(x₁, x₂, ..., xₙ) → ℝ
- Vector-valued: f(x) = [f₁(x), f₂(x), ..., fₘ(x)]ᵀ
- Compact form: f: ℝⁿ → ℝᵐ

Common Examples in ML:
- Loss function: L(θ) where θ ∈ ℝᵖ (parameters)
- Neural network: f(x; θ) where x ∈ ℝⁿ (input), θ ∈ ℝᵖ (weights)
- Probability density: p(x) where x ∈ ℝᵈ
- Distance function: d(x, y) where x, y ∈ ℝⁿ

Continuity:
f is continuous at point a if:
lim[x→a] f(x) = f(a)

In terms of ε-δ definition:
∀ε > 0, ∃δ > 0 such that ||x - a|| < δ ⟹ |f(x) - f(a)| < ε

Level Sets:
For scalar function f: ℝⁿ → ℝ:
- Level curve (n=2): {(x,y) : f(x,y) = c}
- Level surface (n=3): {(x,y,z) : f(x,y,z) = c}
- Level set (general): {x ∈ ℝⁿ : f(x) = c}

=======================================================

2. PARTIAL DERIVATIVES
======================

Definition:
The partial derivative of f with respect to xᵢ at point a is:
∂f/∂xᵢ(a) = lim[h→0] [f(a₁,...,aᵢ+h,...,aₙ) - f(a₁,...,aᵢ,...,aₙ)]/h

Notation:
- ∂f/∂xᵢ, fₓᵢ, Dᵢf, ∂ᵢf

Geometric Interpretation:
- Rate of change in direction of coordinate axis
- Slope of tangent line to curve obtained by fixing other variables

Computing Partial Derivatives:
1. Treat all other variables as constants
2. Apply standard differentiation rules
3. Chain rule applies for composite functions

Examples:
f(x,y) = x²y + 3xy² - 5y
∂f/∂x = 2xy + 3y²
∂f/∂y = x² + 6xy - 5

Higher-Order Partial Derivatives:
- Second partials: ∂²f/∂xᵢ∂xⱼ = fₓᵢₓⱼ
- Mixed partials: ∂²f/∂x∂y, ∂²f/∂y∂x
- Clairaut's theorem: If continuous, fₓᵧ = fᵧₓ

Notation for Higher Order:
∂²f/∂x² = fₓₓ
∂²f/∂x∂y = fₓᵧ = fᵧₓ
∂³f/∂x²∂y = fₓₓᵧ

=======================================================

3. GRADIENTS AND DIRECTIONAL DERIVATIVES
========================================

3.1 Gradient Vector:
-------------------
For scalar function f: ℝⁿ → ℝ, the gradient is:
∇f(x) = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ]ᵀ

Properties:
- Points in direction of steepest increase
- Magnitude gives rate of steepest increase
- Perpendicular to level sets
- Vector field: assigns vector to each point

Alternative Notation:
- grad f, Df, ∇f

3.2 Directional Derivative:
--------------------------
Rate of change of f at point a in direction of unit vector u:
Dᵤf(a) = lim[t→0] [f(a + tu) - f(a)]/t

Relationship to Gradient:
Dᵤf(a) = ∇f(a) · u = ||∇f(a)|| ||u|| cos θ

where θ is angle between ∇f(a) and u.

Key Results:
- Maximum directional derivative: ||∇f(a)|| (in direction of ∇f)
- Minimum directional derivative: -||∇f(a)|| (opposite to ∇f)
- Zero directional derivative: perpendicular to ∇f

3.3 Gradient Properties:
-----------------------
Linearity: ∇(αf + βg) = α∇f + β∇g
Product rule: ∇(fg) = f∇g + g∇f
Chain rule: ∇(f(g(x))) = f'(g(x))∇g(x)

=======================================================

4. JACOBIAN MATRIX
==================

Definition:
For vector-valued function f: ℝⁿ → ℝᵐ:
f(x) = [f₁(x), f₂(x), ..., fₘ(x)]ᵀ

The Jacobian matrix J is m×n matrix:
J(x) = [∂fᵢ/∂xⱼ] = [
  ∂f₁/∂x₁  ∂f₁/∂x₂  ...  ∂f₁/∂xₙ
  ∂f₂/∂x₁  ∂f₂/∂x₂  ...  ∂f₂/∂xₙ
  ⋮        ⋮        ⋱    ⋮
  ∂fₘ/∂x₁  ∂fₘ/∂x₂  ...  ∂fₘ/∂xₙ
]

Alternative Notation:
- Jf, Df, f'(x)

Properties:
- Generalizes derivative to vector functions
- Linear approximation: f(x+h) ≈ f(x) + J(x)h
- Represents local linear transformation

Special Cases:
- If m = 1: J = ∇f (gradient as row vector)
- If n = 1: J = f'(x) (ordinary derivative)
- If n = m: square matrix, determinant gives local scaling

Jacobian Determinant:
For f: ℝⁿ → ℝⁿ, det(J) represents:
- Local area/volume scaling factor
- Zero determinant indicates singular transformation
- Used in change of variables for integration

=======================================================

5. HESSIAN MATRIX
=================

Definition:
For scalar function f: ℝⁿ → ℝ, the Hessian is n×n matrix of second partial derivatives:
H(x) = [∂²f/∂xᵢ∂xⱼ] = [
  ∂²f/∂x₁²    ∂²f/∂x₁∂x₂  ...  ∂²f/∂x₁∂xₙ
  ∂²f/∂x₂∂x₁  ∂²f/∂x₂²    ...  ∂²f/∂x₂∂xₙ
  ⋮           ⋮           ⋱    ⋮
  ∂²f/∂xₙ∂x₁  ∂²f/∂xₙ∂x₂  ...  ∂²f/∂xₙ²
]

Alternative Notation:
- Hf, D²f, ∇²f

Properties:
- Symmetric matrix (if continuous second derivatives)
- Captures local curvature information
- Used in second-order optimization methods

Relationship to Taylor Series:
f(x + h) ≈ f(x) + ∇f(x)ᵀh + ½hᵀH(x)h

Classification by Eigenvalues:
- Positive definite (all λᵢ > 0): Local minimum
- Negative definite (all λᵢ < 0): Local maximum  
- Indefinite (mixed signs): Saddle point
- Positive/negative semidefinite: Degenerate cases

Conditions for Definiteness:
- Positive definite: All leading principal minors > 0
- Negative definite: Alternating signs starting with < 0
- Use eigenvalue decomposition for general case

=======================================================

6. CHAIN RULE IN MULTIPLE VARIABLES
===================================

6.1 Scalar Chain Rule:
---------------------
For composite function z = f(g(t)) where g: ℝ → ℝⁿ, f: ℝⁿ → ℝ:
dz/dt = ∇f(g(t)) · g'(t) = Σᵢ (∂f/∂xᵢ)(dxᵢ/dt)

6.2 Vector Chain Rule:
---------------------
For h(t) = f(g(t)) where g: ℝ → ℝⁿ, f: ℝⁿ → ℝᵐ:
h'(t) = J_f(g(t)) · g'(t)

where J_f is Jacobian of f.

6.3 General Chain Rule:
----------------------
For h(t) = f(g(t)) where g: ℝᵖ → ℝⁿ, f: ℝⁿ → ℝᵐ:
J_h(t) = J_f(g(t)) · J_g(t)

Matrix form: [∂hᵢ/∂tⱼ] = [∂fᵢ/∂xₖ] · [∂xₖ/∂tⱼ]

6.4 Computational Graph Perspective:
-----------------------------------
- Forward mode: Compute derivatives in direction of computation
- Reverse mode: Backpropagate derivatives (used in backpropagation)
- Automatic differentiation utilizes chain rule systematically

=======================================================

7. CRITICAL POINTS AND OPTIMIZATION
===================================

7.1 Critical Points:
-------------------
Point x* is critical if ∇f(x*) = 0

Types of Critical Points:
- Local minimum: f(x) ≥ f(x*) in neighborhood
- Local maximum: f(x) ≤ f(x*) in neighborhood  
- Saddle point: Neither minimum nor maximum
- Global minimum/maximum: Over entire domain

7.2 Second Derivative Test:
--------------------------
At critical point x* with Hessian H:
- If H positive definite: Local minimum
- If H negative definite: Local maximum
- If H indefinite: Saddle point
- If H semidefinite: Test inconclusive

7.3 Constrained Optimization:
----------------------------
Method of Lagrange Multipliers:
For optimization problem: minimize f(x) subject to g(x) = 0

Form Lagrangian: L(x,λ) = f(x) - λg(x)
Critical point conditions:
∇ₓL = ∇f(x) - λ∇g(x) = 0
∇_λL = -g(x) = 0

Interpretation: At optimum, ∇f and ∇g are parallel

7.4 KKT Conditions:
------------------
For inequality constraints: minimize f(x) subject to gᵢ(x) ≤ 0

Karush-Kuhn-Tucker conditions:
1. ∇f(x*) - Σᵢ λᵢ∇gᵢ(x*) = 0
2. λᵢ ≥ 0 for all i
3. λᵢgᵢ(x*) = 0 for all i (complementary slackness)
4. gᵢ(x*) ≤ 0 for all i

=======================================================

8. APPLICATIONS IN MACHINE LEARNING
===================================

8.1 Gradient-Based Optimization:
--------------------------------
Gradient Descent: θ_{k+1} = θ_k - α∇L(θ_k)
- Uses gradient ∇L to find minimum of loss function
- Learning rate α controls step size
- Convergence depends on Hessian properties

Stochastic Gradient Descent (SGD):
- Use approximate gradient from minibatch
- Noise helps escape local minima
- Lower computational cost per iteration

8.2 Neural Network Backpropagation:
----------------------------------
Forward Pass: Compute activations layer by layer
Backward Pass: Use chain rule to compute gradients

For network f(x; θ) with loss L(y, f(x; θ)):
∂L/∂θ computed via repeated application of chain rule

Computational efficiency through reverse-mode differentiation

8.3 Newton's Method and Quasi-Newton:
------------------------------------
Newton's Method: θ_{k+1} = θ_k - H⁻¹∇L(θ_k)
- Uses Hessian for second-order information
- Quadratic convergence near minimum
- Expensive Hessian computation and inversion

Quasi-Newton (BFGS, L-BFGS):
- Approximate Hessian using gradient history
- Good convergence with lower computational cost

8.4 Regularization:
------------------
L2 Regularization: L_reg(θ) = L(θ) + λ||θ||²
∇L_reg = ∇L + 2λθ

L1 Regularization: L_reg(θ) = L(θ) + λ||θ||₁
Subgradient required due to non-differentiability at 0

8.5 Principal Component Analysis (PCA):
--------------------------------------
Find directions of maximum variance
Eigenvalue problem: C v = λ v
where C is covariance matrix (related to Hessian of variance)

8.6 Support Vector Machines (SVM):
---------------------------------
Constrained optimization with KKT conditions
Dual formulation introduces Lagrange multipliers
Support vectors correspond to active constraints

8.7 Maximum Likelihood Estimation:
----------------------------------
Maximize log-likelihood: θ* = argmax_θ ℓ(θ)
Use gradient: ∇ℓ(θ) = 0
Fisher information matrix relates to Hessian

Key Insights for ML:
- Gradients provide direction for parameter updates
- Hessians give curvature information for optimization
- Chain rule enables efficient gradient computation
- Critical points correspond to optimal parameters
- Constrained optimization handles regularization

=======================================================
END OF DOCUMENT 