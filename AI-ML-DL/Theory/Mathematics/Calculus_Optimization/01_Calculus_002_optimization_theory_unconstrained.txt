OPTIMIZATION THEORY - UNCONSTRAINED OPTIMIZATION
=================================================

TABLE OF CONTENTS:
1. Fundamentals of Optimization Theory
2. Necessary and Sufficient Conditions
3. Gradient Descent Methods
4. Newton's Method and Quasi-Newton Methods
5. Line Search Techniques
6. Convergence Analysis
7. Stochastic Optimization
8. Applications in Machine Learning

=======================================================

1. FUNDAMENTALS OF OPTIMIZATION THEORY
======================================

1.1 Problem Formulation:
-----------------------
Unconstrained optimization problem:
minimize f(x) where x ∈ ℝⁿ

Components:
- Objective function: f: ℝⁿ → ℝ
- Decision variables: x = [x₁, x₂, ..., xₙ]ᵀ
- Optimal solution: x* = argmin f(x)
- Optimal value: f* = f(x*)

1.2 Types of Optima:
-------------------
Global minimum: f(x*) ≤ f(x) for all x ∈ ℝⁿ
Local minimum: f(x*) ≤ f(x) for all x in neighborhood of x*
Strict local minimum: f(x*) < f(x) for all x ≠ x* in neighborhood

Isolated minimum: x* has neighborhood containing no other local minima
Weak minimum: f(x*) = f(x) for some x ≠ x* in neighborhood

1.3 Convexity:
-------------
Convex function: f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y) for all λ ∈ [0,1]

Properties:
- Any local minimum is global minimum
- Set of global minima is convex
- Gradient condition: f(y) ≥ f(x) + ∇f(x)ᵀ(y - x)

Strictly convex: Inequality is strict for x ≠ y and λ ∈ (0,1)
- Implies unique global minimum

Strong convexity: f(y) ≥ f(x) + ∇f(x)ᵀ(y - x) + (μ/2)||y - x||²
- Parameter μ > 0 is strong convexity constant
- Implies faster convergence rates

1.4 Smoothness:
--------------
Lipschitz continuous gradient: ||∇f(x) - ∇f(y)|| ≤ L||x - y||
- L is Lipschitz constant
- Enables convergence analysis with fixed step sizes

Twice differentiable: Second derivatives exist and are continuous
- Enables Newton-type methods
- Hessian matrix: H(x) = ∇²f(x)

=======================================================

2. NECESSARY AND SUFFICIENT CONDITIONS
======================================

2.1 First-Order Necessary Conditions:
-------------------------------------
Theorem: If x* is local minimum and f differentiable at x*, then:
∇f(x*) = 0

Proof outline:
- If ∇f(x*) ≠ 0, then d = -∇f(x*) is descent direction
- For small α > 0: f(x* + αd) < f(x*)
- Contradicts local minimality

Critical point: Point where ∇f(x) = 0
- All local minima are critical points
- Not all critical points are local minima

2.2 Second-Order Necessary Conditions:
-------------------------------------
Theorem: If x* is local minimum and f twice differentiable at x*, then:
1. ∇f(x*) = 0 (first-order condition)
2. ∇²f(x*) ⪰ 0 (Hessian positive semidefinite)

Implications:
- All eigenvalues of Hessian ≥ 0
- May have zero eigenvalues (degenerate case)

2.3 Second-Order Sufficient Conditions:
--------------------------------------
Theorem: If f twice continuously differentiable and:
1. ∇f(x*) = 0
2. ∇²f(x*) ≻ 0 (Hessian positive definite)

Then x* is strict local minimum.

Implications:
- All eigenvalues of Hessian > 0
- Guarantees isolated local minimum
- Enables convergence analysis for Newton's method

2.4 Saddle Points:
-----------------
Critical point that is neither local minimum nor maximum
- Hessian has both positive and negative eigenvalues
- Common in high-dimensional optimization
- Challenge for gradient-based methods

Escape from saddle points:
- Gradient descent with noise
- Trust region methods
- Second-order information

=======================================================

3. GRADIENT DESCENT METHODS
===========================

3.1 Basic Gradient Descent:
---------------------------
Algorithm:
xₖ₊₁ = xₖ - αₖ∇f(xₖ)

where αₖ > 0 is step size (learning rate)

Intuition:
- Move in direction of steepest descent
- ∇f(xₖ) points in direction of steepest increase
- -∇f(xₖ) points in direction of steepest decrease

3.2 Step Size Selection:
-----------------------
Fixed step size: αₖ = α for all k
- Simple but may not converge or converge slowly
- Requires careful tuning

Diminishing step size: αₖ → 0 as k → ∞
- Conditions: Σₖ αₖ = ∞, Σₖ αₖ² < ∞
- Example: αₖ = 1/k, αₖ = 1/√k

Exact line search: αₖ = argmin_α f(xₖ - α∇f(xₖ))
- Optimal step size in gradient direction
- Often computationally expensive

Backtracking line search: Start with α₀, reduce until sufficient decrease
- Armijo condition: f(xₖ₊₁) ≤ f(xₖ) - c₁αₖ||∇f(xₖ)||²
- Balances computational cost and convergence

3.3 Convergence Analysis:
------------------------
For convex f with Lipschitz continuous gradient:
f(xₖ) - f* ≤ O(1/k)

For strongly convex f:
||xₖ - x*||² ≤ (1 - μ/L)ᵏ||x₀ - x*||²

where μ is strong convexity parameter, L is Lipschitz constant

Condition number: κ = L/μ
- κ ≈ 1: Fast convergence
- κ >> 1: Slow convergence ("ill-conditioned")

3.4 Momentum Methods:
--------------------
Heavy ball method:
xₖ₊₁ = xₖ - α∇f(xₖ) + β(xₖ - xₖ₋₁)

Intuition: Add momentum to accelerate convergence
- β ∈ [0,1) is momentum parameter
- Helps escape poor local geometry

Nesterov accelerated gradient:
yₖ = xₖ + β(xₖ - xₖ₋₁)
xₖ₊₁ = yₖ - α∇f(yₖ)

Achieves optimal O(1/k²) convergence rate for convex functions

=======================================================

4. NEWTON'S METHOD AND QUASI-NEWTON METHODS
===========================================

4.1 Newton's Method:
-------------------
Algorithm:
xₖ₊₁ = xₖ - [∇²f(xₖ)]⁻¹∇f(xₖ)

Derivation from Taylor expansion:
f(x) ≈ f(xₖ) + ∇f(xₖ)ᵀ(x - xₖ) + ½(x - xₖ)ᵀ∇²f(xₖ)(x - xₖ)

Minimize quadratic approximation:
∇f(xₖ) + ∇²f(xₖ)(x - xₖ) = 0

Newton direction: dₖ = -[∇²f(xₖ)]⁻¹∇f(xₖ)

4.2 Properties of Newton's Method:
---------------------------------
Advantages:
- Quadratic convergence near solution
- Scale invariant (invariant to linear transformations)
- Uses second-order information

Disadvantages:
- Requires Hessian computation and inversion: O(n³) per iteration
- Hessian may not be positive definite
- May not converge globally

Convergence rate:
||xₖ₊₁ - x*|| ≤ M||xₖ - x*||²

for some constant M when close to solution

4.3 Modified Newton Methods:
---------------------------
Damped Newton: Add line search
xₖ₊₁ = xₖ - αₖ[∇²f(xₖ)]⁻¹∇f(xₖ)

Regularized Newton: Ensure positive definiteness
xₖ₊₁ = xₖ - [∇²f(xₖ) + λₖI]⁻¹∇f(xₖ)

where λₖ ≥ 0 is regularization parameter

Trust region: Constrain step size
minimize mₖ(p) = ∇f(xₖ)ᵀp + ½pᵀ∇²f(xₖ)p
subject to ||p|| ≤ Δₖ

4.4 Quasi-Newton Methods:
------------------------
Idea: Approximate Hessian using gradient information only

Secant condition: Bₖ₊₁sₖ = yₖ
where sₖ = xₖ₊₁ - xₖ, yₖ = ∇f(xₖ₊₁) - ∇f(xₖ)

BFGS update:
Bₖ₊₁ = Bₖ - (BₖsₖsₖᵀBₖ)/(sₖᵀBₖsₖ) + (yₖyₖᵀ)/(yₖᵀsₖ)

L-BFGS: Limited memory version
- Store only last m vector pairs (sᵢ, yᵢ)
- O(mn) storage instead of O(n²)
- Efficient for large-scale problems

DFP update: Dual to BFGS
- Updates inverse Hessian approximation directly
- Generally less robust than BFGS

=======================================================

5. LINE SEARCH TECHNIQUES
=========================

5.1 Exact Line Search:
---------------------
Problem: αₖ = argmin_α f(xₖ + αdₖ)

where dₖ is search direction

Optimality condition: ∇f(xₖ + αₖdₖ)ᵀdₖ = 0
- Gradient orthogonal to search direction
- Often expensive to compute exactly

5.2 Wolfe Conditions:
--------------------
Sufficient decrease (Armijo condition):
f(xₖ + αdₖ) ≤ f(xₖ) + c₁α∇f(xₖ)ᵀdₖ

Curvature condition:
∇f(xₖ + αdₖ)ᵀdₖ ≥ c₂∇f(xₖ)ᵀdₖ

where 0 < c₁ < c₂ < 1 (typically c₁ = 10⁻⁴, c₂ = 0.9)

Strong Wolfe conditions: Replace ≥ with |∇f(xₖ + αdₖ)ᵀdₖ| ≤ c₂|∇f(xₖ)ᵀdₖ|

5.3 Backtracking Line Search:
----------------------------
Algorithm:
1. Start with α = α₀
2. While f(xₖ + αdₖ) > f(xₖ) + c₁α∇f(xₖ)ᵀdₖ:
   α = ρα (typically ρ = 0.5)
3. Return α

Simple and efficient for many applications
Guarantees sufficient decrease

5.4 Interpolation Methods:
-------------------------
Quadratic interpolation:
- Use function values at 3 points
- Fit quadratic polynomial
- Minimize to find step size

Cubic interpolation:
- Use function and gradient values
- More accurate but more expensive

Bisection method:
- For unimodal functions
- Guaranteed convergence
- Slower than interpolation methods

=======================================================

6. CONVERGENCE ANALYSIS
=======================

6.1 Convergence Rates:
---------------------
Linear convergence: ||xₖ₊₁ - x*|| ≤ r||xₖ - x*||
- Constant r ∈ (0,1)
- Error decreases exponentially: O(rᵏ)

Superlinear convergence: ||xₖ₊₁ - x*||/||xₖ - x*|| → 0
- Faster than any linear rate
- BFGS often achieves superlinear convergence

Quadratic convergence: ||xₖ₊₁ - x*|| ≤ M||xₖ - x*||²
- Newton's method near solution
- Very fast convergence when applicable

6.2 Global vs Local Convergence:
-------------------------------
Global convergence: Algorithm converges from any starting point
- Usually to stationary point, not necessarily global minimum
- Requires line search or trust region

Local convergence: Algorithm converges when started near solution
- Often faster convergence rates
- Newton's method, BFGS with exact line search

6.3 Complexity Analysis:
-----------------------
Oracle complexity: Number of function/gradient evaluations
Iteration complexity: Number of iterations
Arithmetic complexity: Total computational cost

For convex smooth functions:
- Gradient descent: O(1/ε) iterations for ε-optimal solution
- Accelerated methods: O(1/√ε) iterations
- Newton's method: O(log log(1/ε)) iterations (if feasible)

6.4 Stopping Criteria:
---------------------
Gradient norm: ||∇f(xₖ)|| ≤ ε
Relative gradient: ||∇f(xₖ)||/||∇f(x₀)|| ≤ ε
Function change: |f(xₖ₊₁) - f(xₖ)| ≤ ε
Relative function change: |f(xₖ₊₁) - f(xₖ)|/|f(xₖ)| ≤ ε
Parameter change: ||xₖ₊₁ - xₖ|| ≤ ε

Practical considerations:
- Combine multiple criteria
- Account for numerical precision
- Problem-specific tolerances

=======================================================

7. STOCHASTIC OPTIMIZATION
==========================

7.1 Stochastic Gradient Descent (SGD):
--------------------------------------
Problem: minimize f(x) = 𝔼[F(x, ξ)]

where ξ is random variable

Algorithm:
xₖ₊₁ = xₖ - αₖgₖ

where gₖ is stochastic gradient: 𝔼[gₖ] = ∇f(xₖ)

Examples:
- Machine learning: F(x, ξ) = loss on single data point
- Simulation optimization: F(x, ξ) = simulation output

7.2 Variance and Bias:
---------------------
Stochastic gradient gₖ = ∇F(xₖ, ξₖ)
- Unbiased: 𝔼[gₖ] = ∇f(xₖ)
- Variance: Var[gₖ] = 𝔼[||gₖ - ∇f(xₖ)||²]

High variance causes:
- Slow convergence
- Oscillations around optimum
- Requires diminishing step sizes

Variance reduction techniques:
- Mini-batching
- Control variates
- Importance sampling

7.3 Mini-Batch Methods:
----------------------
Batch size b: Use b samples per iteration
gₖ = (1/b)Σᵢ₌₁ᵇ ∇F(xₖ, ξₖᵢ)

Trade-offs:
- Larger b: Lower variance, higher computational cost
- Smaller b: Higher variance, lower computational cost
- b = 1: Standard SGD
- b = n: Full gradient

Optimal batch size depends on:
- Problem structure
- Available parallelism
- Memory constraints

7.4 Adaptive Methods:
--------------------
AdaGrad: Adapt step size based on gradient history
xₖ₊₁ = xₖ - (α/√(Gₖ + ε))∇f(xₖ)

where Gₖ = Σᵢ₌₁ᵏ ∇f(xᵢ)∇f(xᵢ)ᵀ

RMSprop: Exponential moving average
Gₖ = γGₖ₋₁ + (1-γ)∇f(xₖ)∇f(xₖ)ᵀ

Adam: Combines momentum and adaptive step sizes
mₖ = β₁mₖ₋₁ + (1-β₁)∇f(xₖ)
vₖ = β₂vₖ₋₁ + (1-β₂)[∇f(xₖ)]²
xₖ₊₁ = xₖ - α(m̂ₖ/√v̂ₖ + ε)

where m̂ₖ, v̂ₖ are bias-corrected estimates

=======================================================

8. APPLICATIONS IN MACHINE LEARNING
===================================

8.1 Empirical Risk Minimization:
-------------------------------
Problem: minimize (1/n)Σᵢ₌₁ⁿ ℓ(f(xᵢ; θ), yᵢ) + λR(θ)

Components:
- Loss function: ℓ(ŷ, y)
- Model: f(x; θ) parameterized by θ
- Regularization: R(θ) (e.g., ||θ||², ||θ||₁)
- Regularization parameter: λ ≥ 0

Gradient: ∇θ = (1/n)Σᵢ₌₁ⁿ ∇ℓ(f(xᵢ; θ), yᵢ) + λ∇R(θ)

8.2 Neural Network Training:
---------------------------
Forward propagation: Compute activations layer by layer
Backward propagation: Compute gradients via chain rule

Challenges:
- High dimensionality (millions/billions of parameters)
- Non-convex optimization landscape
- Saddle points and local minima
- Vanishing/exploding gradients

Solutions:
- Stochastic optimization (SGD, Adam)
- Batch normalization
- Residual connections
- Careful initialization

8.3 Logistic Regression:
-----------------------
Objective: minimize Σᵢ₌₁ⁿ log(1 + exp(-yᵢxᵢᵀθ)) + λ||θ||²

Properties:
- Convex objective function
- Smooth gradients
- Global optimum exists

Gradient: ∇θ = Σᵢ₌₁ⁿ (-yᵢxᵢ)/(1 + exp(yᵢxᵢᵀθ)) + 2λθ

Methods:
- Gradient descent
- Newton-CG
- L-BFGS
- Stochastic methods for large datasets

8.4 Support Vector Machines:
---------------------------
Primal problem: minimize ½||w||² + C Σᵢ₌₁ⁿ max(0, 1 - yᵢ(wᵀxᵢ + b))

Challenges:
- Non-smooth objective (hinge loss)
- Requires subgradient methods
- Often solved via dual formulation

Smooth approximations:
- Huber loss
- Logistic loss
- Enable standard gradient methods

8.5 Matrix Factorization:
------------------------
Problem: minimize ||R - UV^T||² + λ(||U||² + ||V||²)

where R ∈ ℝᵐˣⁿ is observed matrix, U ∈ ℝᵐˣᵏ, V ∈ ℝⁿˣᵏ

Alternating optimization:
- Fix V, optimize U (convex subproblem)
- Fix U, optimize V (convex subproblem)
- Repeat until convergence

Stochastic optimization:
- Sample observed entries randomly
- Update factors based on single entries
- Efficient for sparse matrices

8.6 Hyperparameter Optimization:
-------------------------------
Problem: minimize validation error as function of hyperparameters

Challenges:
- Expensive function evaluations (train full model)
- Non-convex, non-smooth objective
- Mixed continuous/discrete variables

Methods:
- Grid search
- Random search
- Bayesian optimization
- Gradient-based optimization (when possible)

8.7 Regularization Paths:
------------------------
Problem: Solve optimization for sequence of regularization parameters λ

Examples:
- LASSO path: min ||y - Xβ||² + λ||β||₁
- Ridge path: min ||y - Xβ||² + λ||β||²

Path-following algorithms:
- Start with large λ (heavily regularized)
- Gradually decrease λ
- Use previous solution as warm start

Benefits:
- Computational efficiency
- Model selection across regularization levels
- Understanding of solution structure

Key Insights for ML:
- Most ML problems are optimization problems
- Understanding convergence properties guides algorithm choice
- Stochastic methods essential for large-scale problems
- Second-order information valuable but expensive
- Convexity simplifies analysis but many ML problems non-convex
- Trade-offs between convergence speed and computational cost

=======================================================
END OF DOCUMENT 