ADVANCED TOPICS: SVD AND PCA - Linear Algebra Applications
============================================================

TABLE OF CONTENTS:
1. Singular Value Decomposition (SVD) Theory
2. Computing and Properties of SVD
3. Principal Component Analysis (PCA) Theory
4. PCA Implementation and Variants
5. Connections Between SVD and PCA
6. Low-Rank Approximations
7. Advanced Applications
8. Computational Considerations and ML Applications

=======================================================

1. SINGULAR VALUE DECOMPOSITION (SVD) THEORY
============================================

1.1 Definition and Existence:
----------------------------
For any matrix A ∈ ℝᵐˣⁿ, the SVD is:
A = UΣVᵀ

where:
- U ∈ ℝᵐˣᵐ: orthogonal matrix (left singular vectors)
- Σ ∈ ℝᵐˣⁿ: diagonal matrix (singular values)
- V ∈ ℝⁿˣⁿ: orthogonal matrix (right singular vectors)

Singular values: σ₁ ≥ σ₂ ≥ ... ≥ σₘᵢₙ₍ₘ,ₙ₎ ≥ 0

Existence: SVD exists for any real or complex matrix
Uniqueness: Singular values are unique; vectors unique if σᵢ distinct

1.2 Geometric Interpretation:
----------------------------
Any linear transformation can be decomposed as:
1. Rotation/reflection (V)
2. Scaling along orthogonal directions (Σ)  
3. Rotation/reflection (U)

Visual: A maps unit sphere to ellipsoid
- Principal axes given by columns of U
- Axis lengths given by singular values σᵢ

1.3 Relationship to Eigendecomposition:
--------------------------------------
AᵀA = VΣᵀΣVᵀ = V(diag(σ₁²,...,σₙ²))Vᵀ
- Right singular vectors = eigenvectors of AᵀA
- Squared singular values = eigenvalues of AᵀA

AAᵀ = UΣΣᵀUᵀ = U(diag(σ₁²,...,σₘ²))Uᵀ
- Left singular vectors = eigenvectors of AAᵀ
- Same squared singular values

1.4 Fundamental Subspaces:
-------------------------
Four fundamental subspaces revealed by SVD:

If rank(A) = r:
- Column space: Col(A) = span{u₁,...,uᵣ}
- Row space: Row(A) = span{v₁,...,vᵣ}
- Null space: Null(A) = span{vᵣ₊₁,...,vₙ}
- Left null space: Null(Aᵀ) = span{uᵣ₊₁,...,uₘ}

Orthogonal relationships:
- Row(A) ⊥ Null(A)
- Col(A) ⊥ Null(Aᵀ)

1.5 Reduced SVD:
---------------
For A ∈ ℝᵐˣⁿ with rank r:
A = UᵣΣᵣVᵣᵀ

where:
- Uᵣ ∈ ℝᵐˣʳ: first r columns of U
- Σᵣ ∈ ℝʳˣʳ: diagonal with positive singular values
- Vᵣ ∈ ℝⁿˣʳ: first r columns of V

Advantages:
- Removes zero singular values
- More efficient storage and computation
- Reveals essential rank-r structure

=======================================================

2. COMPUTING AND PROPERTIES OF SVD
==================================

2.1 Computing SVD:
-----------------
Direct approach (not recommended):
1. Compute AᵀA and AAᵀ
2. Find eigendecompositions
3. Construct U, Σ, V

Problems: Numerical instability, condition number squared

Standard algorithms:
1. Golub-Reinsch algorithm (bidiagonalization + iteration)
2. Divide-and-conquer methods
3. Jacobi SVD methods
4. Randomized SVD (for large matrices)

2.2 Key Properties:
------------------
Rank: rank(A) = number of non-zero singular values

Norms:
- ||A||₂ = σ₁ (spectral norm, largest singular value)
- ||A||F = √(σ₁² + σ₂² + ... + σᵣ²) (Frobenius norm)

Condition number: κ(A) = σₘₐₓ/σₘᵢₙ
- κ(A) = 1: perfectly conditioned
- κ(A) >> 1: ill-conditioned

Pseudo-inverse: A⁺ = VΣ⁺Uᵀ where Σ⁺ᵢᵢ = 1/σᵢ if σᵢ > 0, else 0

2.3 Perturbation Theory:
-----------------------
How do singular values change under perturbation A → A + E?

Weyl's theorem: |σᵢ(A + E) - σᵢ(A)| ≤ ||E||₂

Sensitivity depends on:
- Gap between singular values
- Magnitude of perturbation
- Structure of perturbation

2.4 Matrix Approximation:
------------------------
Best rank-k approximation in Frobenius and spectral norms:
Aₖ = UₖΣₖVₖᵀ = Σᵢ₌₁ᵏ σᵢuᵢvᵢᵀ

Optimality (Eckart-Young theorem):
- Aₖ minimizes ||A - B||₂ over all rank-k matrices B
- Aₖ minimizes ||A - B||F over all rank-k matrices B

Approximation error:
- ||A - Aₖ||₂ = σₖ₊₁
- ||A - Aₖ||F = √(σₖ₊₁² + ... + σᵣ²)

=======================================================

3. PRINCIPAL COMPONENT ANALYSIS (PCA) THEORY
============================================

3.1 Problem Formulation:
-----------------------
Given data matrix X ∈ ℝⁿˣᵖ (n samples, p features)
Goal: Find k-dimensional subspace that best approximates data

Optimization problem:
minimize Σᵢ ||xᵢ - projₛ(xᵢ)||²
subject to S is k-dimensional subspace

Solution: S spanned by top k principal components

3.2 Mathematical Derivation:
---------------------------
Assumptions:
- Data is centered: Σᵢ xᵢ = 0
- Want to find directions of maximum variance

First principal component w₁:
maximize w₁ᵀSw₁ subject to ||w₁||₂ = 1
where S = (1/n)XᵀX is sample covariance matrix

Lagrangian: L = w₁ᵀSw₁ - λ(w₁ᵀw₁ - 1)
Solution: Sw₁ = λw₁ (eigenvalue problem)

Choose λ = largest eigenvalue, w₁ = corresponding eigenvector

3.3 Sequential Extraction:
-------------------------
k-th principal component:
maximize wₖᵀSwₖ
subject to ||wₖ||₂ = 1 and wₖ ⊥ w₁,...,wₖ₋₁

Solution: wₖ = k-th largest eigenvector of S
Variance explained by wₖ: λₖ

Total variance explained: (Σᵢ₌₁ᵏ λᵢ) / (Σᵢ₌₁ᵖ λᵢ)

3.4 Dimensionality Reduction:
----------------------------
Principal component scores:
yᵢ = Wᵀxᵢ where W = [w₁ w₂ ... wₖ]

Reconstruction:
x̂ᵢ = Wyᵢ = WWᵀxᵢ

Reconstruction error:
||xᵢ - x̂ᵢ||² = Σⱼ₌ₖ₊₁ᵖ (wⱼᵀxᵢ)²

3.5 Geometric Interpretation:
----------------------------
PCA finds:
1. Center of data cloud (mean)
2. Principal axes (eigenvectors)
3. Spread along each axis (eigenvalues)

First PC: direction of maximum variance
Second PC: direction of maximum remaining variance, orthogonal to first
And so on...

=======================================================

4. PCA IMPLEMENTATION AND VARIANTS
==================================

4.1 Standard PCA Algorithm:
--------------------------
1. Center the data: X̃ = X - 1μᵀ where μ = (1/n)X1
2. Compute covariance matrix: S = (1/(n-1))X̃ᵀX̃
3. Eigendecomposition: S = WΛWᵀ
4. Sort eigenvalues/eigenvectors in descending order
5. Select top k components: Wₖ = [w₁ ... wₖ]
6. Transform data: Y = X̃Wₖ

Alternative via SVD:
1. Center data: X̃ = X - 1μᵀ
2. SVD: X̃ = UΣVᵀ
3. Principal components: W = V
4. Explained variance: λᵢ = σᵢ²/(n-1)

4.2 Choosing Number of Components:
---------------------------------
Scree plot: Plot eigenvalues λᵢ vs component index
- Look for "elbow" where slope changes dramatically

Cumulative explained variance:
- Choose k such that Σᵢ₌₁ᵏ λᵢ / Σᵢ₌₁ᵖ λᵢ ≥ threshold (e.g., 0.9)

Cross-validation:
- Split data, perform PCA on training set
- Evaluate reconstruction error on validation set

Kaiser criterion: Keep components with λᵢ > mean(λ)

4.3 Robust PCA:
--------------
Standard PCA sensitive to outliers

L1-PCA: Use L1 norm instead of L2
- More robust to outliers
- Requires iterative algorithms

Robust covariance estimation:
- Minimum Covariance Determinant (MCD)
- Minimum Volume Ellipsoid (MVE)

4.4 Sparse PCA:
--------------
Goal: Find sparse principal components (few non-zero loadings)

Formulation:
maximize wᵀSwsubject to ||w||₂ = 1, ||w||₁ ≤ c

Methods:
- LASSO-type penalties
- Iterative thresholding
- Semidefinite programming relaxations

Benefits:
- Interpretability (fewer variables involved)
- Feature selection
- Noise reduction

4.5 Incremental PCA:
-------------------
For streaming data or memory constraints

Idea: Update principal components as new data arrives
- Update sample mean and covariance
- Update eigendecomposition incrementally

Challenges:
- Computational complexity
- Numerical stability
- Concept drift

4.6 Kernel PCA:
--------------
Nonlinear extension using kernel trick

Steps:
1. Map data to feature space: x → φ(x)
2. Perform PCA in feature space
3. Never explicitly compute φ(x)

Kernel matrix: K where Kᵢⱼ = k(xᵢ, xⱼ) = ⟨φ(xᵢ), φ(xᵢ)⟩

Principal components in feature space:
vₖ = Σᵢ αₖᵢ φ(xᵢ)

Solve: Kα = nλα (eigenvalue problem for K)

=======================================================

5. CONNECTIONS BETWEEN SVD AND PCA
==================================

5.1 Direct Relationship:
-----------------------
For centered data matrix X ∈ ℝⁿˣᵖ:
X = UΣVᵀ (SVD)

Covariance matrix:
S = (1/(n-1))XᵀX = (1/(n-1))VΣ²Vᵀ

Principal components = columns of V
Explained variance = σᵢ²/(n-1)

5.2 Computational Advantages:
----------------------------
SVD approach:
- Avoids forming XᵀX (better numerical stability)
- Single decomposition vs two (if computing both U and V)
- Natural for n << p or n >> p cases

When n >> p: Form XᵀX, then eigendecomposition
When n << p: Form XXᵀ, then relate to XᵀX via SVD

5.3 Interpretation Differences:
------------------------------
SVD perspective:
- Factorizes data matrix directly
- Left vectors (U): sample space structure
- Right vectors (V): feature space structure
- Singular values: importance of each factor

PCA perspective:
- Analyzes covariance structure
- Focuses on variance explanation
- Principal components have direct variance interpretation

5.4 Matrix Completion Connection:
-------------------------------
Missing data problem: Observe subset of entries in matrix

Low-rank assumption: True matrix has rank r << min(m,n)

SVD provides optimal low-rank approximation:
- Complete matrix ≈ UᵣΣᵣVᵣᵀ
- Minimize Frobenius norm of observed entries

Algorithms:
- Iterative hard thresholding
- Nuclear norm minimization
- Alternating minimization

=======================================================

6. LOW-RANK APPROXIMATIONS
==========================

6.1 Best Rank-k Approximation:
------------------------------
Given A ∈ ℝᵐˣⁿ, find rank-k matrix B minimizing ||A - B||

Solution (Eckart-Young): B = Aₖ = Σᵢ₌₁ᵏ σᵢuᵢvᵢᵀ

Error bounds:
- ||A - Aₖ||₂ = σₖ₊₁
- ||A - Aₖ||F = √(σₖ₊₁² + ... + σᵣ²)

Optimality: No rank-k matrix achieves smaller error

6.2 Approximation Quality:
-------------------------
Relative error in Frobenius norm:
||A - Aₖ||F / ||A||F = √(Σᵢ₌ₖ₊₁ʳ σᵢ²) / √(Σᵢ₌₁ʳ σᵢ²)

Decay of singular values determines approximation quality:
- Fast decay: Good low-rank approximation
- Slow decay: Poor low-rank approximation

6.3 Randomized Low-Rank Approximation:
-------------------------------------
For large matrices, exact SVD computationally expensive

Randomized algorithm:
1. Random sampling: Y = AΩ where Ω ∈ ℝⁿˣˡ random
2. QR decomposition: Y = QR
3. Project: B = QᵀA
4. SVD of B: B = ŨΣ̃Ṽᵀ
5. Recover: U = QŨ, Σ = Σ̃, V = Ṽ

Advantages:
- O(mnl) complexity vs O(mn²) for exact SVD
- Parallelizable
- Good for matrices with fast singular value decay

6.4 Applications of Low-Rank Approximation:
------------------------------------------
Image compression:
- Image as matrix A
- Store Uₖ, Σₖ, Vₖ instead of full A
- Compression ratio: (m+n)k vs mn

Collaborative filtering:
- User-item rating matrix
- Low-rank structure captures latent factors
- Handle missing ratings naturally

Principal component regression:
- Regression on principal components
- Regularization through dimensionality reduction
- Handles multicollinearity

=======================================================

7. ADVANCED APPLICATIONS
========================

7.1 Latent Semantic Analysis (LSA):
----------------------------------
Text analysis using SVD

Document-term matrix A:
- Rows: documents
- Columns: terms
- Entries: term frequencies (often weighted)

SVD: A = UΣVᵀ
- U: document-concept associations
- V: term-concept associations  
- Σ: strength of each concept

Dimensionality reduction:
- Keep top k concepts
- Reduced representation captures semantic relationships

7.2 Eigenfaces for Face Recognition:
-----------------------------------
Face images as vectors in high-dimensional space
Covariance matrix captures facial variations

Principal components = "eigenfaces"
- Each eigenface is weighted combination of face images
- Capture main modes of variation

Face recognition:
- Project new face onto eigenface space
- Compare projections using distance metrics

7.3 Genomics and Bioinformatics:
-------------------------------
Gene expression data analysis:
- Rows: genes, Columns: conditions/samples
- PCA reveals major sources of variation

Population structure:
- Rows: individuals, Columns: genetic markers
- Principal components correlate with geographic ancestry

Batch effect correction:
- Remove unwanted technical variation
- Preserve biological signal

7.4 Signal Processing:
---------------------
Karhunen-Loève Transform (KLT):
- Optimal decorrelating transform
- Same as PCA for discrete signals

Image denoising:
- Assume clean image has low-rank structure
- Corrupted by noise: A = A₀ + N
- Recover A₀ via low-rank approximation

Spectral analysis:
- Time series data as matrix (delay embedding)
- SVD reveals temporal patterns and modes

7.5 Graph Analysis:
------------------
Adjacency matrix A of graph
SVD reveals community structure:
- Large singular values ↔ strong communities
- Corresponding vectors identify community membership

Graph clustering:
- Use top singular vectors as features
- Apply k-means to low-dimensional representation

=======================================================

8. COMPUTATIONAL CONSIDERATIONS AND ML APPLICATIONS
===================================================

8.1 Computational Complexity:
-----------------------------
Standard SVD: O(min(mn², m²n)) for A ∈ ℝᵐˣⁿ
PCA via covariance: O(p³ + np²) for X ∈ ℝⁿˣᵖ
PCA via SVD: O(np²) when n >> p, O(n²p) when p >> n

Memory requirements:
- Full SVD: O(mn) for U, V storage
- Truncated SVD: O((m+n)k) for rank-k approximation

Iterative methods:
- Power iteration, Lanczos, Arnoldi
- Suitable for sparse matrices or matrix-free operations

8.2 Numerical Considerations:
----------------------------
Centering data crucial for PCA:
- Uncentered PCA captures mean instead of variance structure

Scaling/normalization:
- PCA sensitive to variable scales
- Standardize variables if different units

Numerical stability:
- SVD more stable than eigendecomposition of XᵀX
- Avoid forming normal equations when possible

8.3 Deep Learning Applications:
------------------------------
Weight initialization:
- SVD of random matrices for initialization
- Preserves gradient flow in deep networks

Network compression:
- Low-rank factorization of weight matrices
- Reduce parameters while preserving performance

Feature learning:
- Autoencoder objective related to PCA
- Deep autoencoders learn nonlinear principal components

8.4 Distributed and Parallel Computing:
--------------------------------------
Map-Reduce implementations:
- Distribute computation across multiple machines
- Aggregate local covariance matrices

Streaming algorithms:
- Update SVD/PCA as new data arrives
- Memory-efficient for large datasets

GPU acceleration:
- Matrix operations highly parallelizable
- cuSOLVER, cuBLAS libraries for GPU SVD

8.5 Modern Extensions:
---------------------
Probabilistic PCA:
- Generative model with latent variables
- Handles missing data naturally
- Uncertainty quantification

Variational autoencoders:
- Nonlinear generalization of probabilistic PCA
- Deep generative models with latent variables

Sparse coding:
- Overcomplete dictionaries vs orthogonal bases
- Non-negative matrix factorization
- ICA and blind source separation

8.6 Software and Tools:
----------------------
NumPy/SciPy: Basic SVD and PCA implementations
Scikit-learn: Comprehensive PCA variants
TensorFlow/PyTorch: GPU-accelerated implementations
MATLAB: Robust numerical linear algebra

Specialized libraries:
- ARPACK: Large-scale eigenvalue problems
- PROPACK: SVD for sparse matrices
- Redsvd: Fast randomized SVD

Key Insights for ML:
- SVD and PCA are fundamental dimensionality reduction tools
- Low-rank structure appears throughout machine learning
- Computational efficiency crucial for large-scale applications
- Understanding geometric interpretation aids in method selection
- Numerical stability considerations important for reliable results
- Modern extensions handle nonlinearity and probabilistic modeling

=======================================================
END OF DOCUMENT 