MATRICES AND MATRIX OPERATIONS - Linear Algebra Fundamentals
============================================================

TABLE OF CONTENTS:
1. Matrix Definitions and Notation
2. Basic Matrix Operations
3. Matrix Multiplication
4. Special Types of Matrices
5. Matrix Inverse and Determinant
6. Rank and Linear Systems
7. Eigenvalues and Eigenvectors
8. Matrix Factorizations
9. Applications in Machine Learning

=======================================================

1. MATRIX DEFINITIONS AND NOTATION
==================================

1.1 Basic Definition:
--------------------
A matrix is a rectangular array of numbers arranged in rows and columns.

General form: A = [aᵢⱼ] where i = 1,2,...,m and j = 1,2,...,n

A ∈ ℝᵐˣⁿ = [
  a₁₁  a₁₂  ...  a₁ₙ
  a₂₁  a₂₂  ...  a₂ₙ
  ⋮    ⋮    ⋱   ⋮
  aₘ₁  aₘ₂  ...  aₘₙ
]

Dimensions: m × n (m rows, n columns)
Elements: aᵢⱼ is element in i-th row, j-th column

1.2 Special Cases:
-----------------
Row vector: 1 × n matrix
v = [v₁, v₂, ..., vₙ]

Column vector: m × 1 matrix
u = [u₁; u₂; ...; uₘ]

Square matrix: m = n (same number of rows and columns)
Scalar: 1 × 1 matrix

1.3 Matrix Indexing:
-------------------
Python/NumPy style: A[i,j] (0-indexed)
Mathematical style: aᵢⱼ (1-indexed)

Row extraction: A[i,:] or aᵢ₊
Column extraction: A[:,j] or a₊ⱼ

Submatrix: A[i₁:i₂, j₁:j₂]

1.4 Matrix Equality:
-------------------
Two matrices A and B are equal if:
1. Same dimensions: A, B ∈ ℝᵐˣⁿ
2. Corresponding elements equal: aᵢⱼ = bᵢⱼ for all i,j

=======================================================

2. BASIC MATRIX OPERATIONS
==========================

2.1 Matrix Addition:
-------------------
Definition: For A, B ∈ ℝᵐˣⁿ
(A + B)ᵢⱼ = aᵢⱼ + bᵢⱼ

Properties:
- Commutativity: A + B = B + A
- Associativity: (A + B) + C = A + (B + C)
- Zero matrix: A + 0 = A
- Additive inverse: A + (-A) = 0

Example:
[1  2] + [5  6] = [6   8]
[3  4]   [7  8]   [10 12]

2.2 Scalar Multiplication:
-------------------------
Definition: For scalar c ∈ ℝ and matrix A ∈ ℝᵐˣⁿ
(cA)ᵢⱼ = c · aᵢⱼ

Properties:
- Distributivity: c(A + B) = cA + cB
- Distributivity: (c + d)A = cA + dA
- Associativity: c(dA) = (cd)A
- Identity: 1A = A

2.3 Matrix Subtraction:
----------------------
A - B = A + (-1)B = [aᵢⱼ - bᵢⱼ]

2.4 Element-wise Operations:
---------------------------
Hadamard product (element-wise multiplication):
(A ∘ B)ᵢⱼ = aᵢⱼ · bᵢⱼ

Element-wise division: (A ⊘ B)ᵢⱼ = aᵢⱼ / bᵢⱼ

Note: Different from matrix multiplication!

=======================================================

3. MATRIX MULTIPLICATION
========================

3.1 Standard Matrix Multiplication:
----------------------------------
Definition: For A ∈ ℝᵐˣᵖ and B ∈ ℝᵖˣⁿ
C = AB where cᵢⱼ = Σₖ₌₁ᵖ aᵢₖbₖⱼ

Requirement: Number of columns in A = Number of rows in B
Result: C ∈ ℝᵐˣⁿ

Interpretation:
- cᵢⱼ = dot product of i-th row of A with j-th column of B
- Each element is sum of products

3.2 Properties of Matrix Multiplication:
---------------------------------------
Associativity: (AB)C = A(BC)
Distributivity: A(B + C) = AB + AC, (A + B)C = AC + BC
Non-commutativity: AB ≠ BA (in general)
Identity: AI = IA = A (where I is identity matrix)

3.3 Computational Complexity:
----------------------------
Naive algorithm: O(mnp) for A ∈ ℝᵐˣᵖ, B ∈ ℝᵖˣⁿ
Strassen's algorithm: O(n^log₂7) ≈ O(n^2.807)
Theoretical optimal: O(n^2.373) (Coppersmith-Winograd)

3.4 Block Matrix Multiplication:
-------------------------------
Partition matrices into blocks:
A = [A₁₁  A₁₂]  B = [B₁₁  B₁₂]
    [A₂₁  A₂₂]      [B₂₁  B₂₂]

AB = [A₁₁B₁₁ + A₁₂B₂₁  A₁₁B₁₂ + A₁₂B₂₂]
     [A₂₁B₁₁ + A₂₂B₂₁  A₂₁B₁₂ + A₂₂B₂₂]

Useful for:
- Large matrix computations
- Parallel computing
- Memory optimization

=======================================================

4. SPECIAL TYPES OF MATRICES
============================

4.1 Identity Matrix:
-------------------
I ∈ ℝⁿˣⁿ where Iᵢⱼ = δᵢⱼ = {1 if i=j, 0 if i≠j}

Properties:
- AI = IA = A for compatible dimensions
- Multiplicative identity

4.2 Zero Matrix:
---------------
0 ∈ ℝᵐˣⁿ where all elements are zero
Properties:
- A + 0 = A (additive identity)
- A · 0 = 0 · A = 0

4.3 Diagonal Matrix:
-------------------
D = diag(d₁, d₂, ..., dₙ) where Dᵢⱼ = 0 if i ≠ j

Properties:
- Fast multiplication: DA has rows scaled by diagonal elements
- Easy inversion: D⁻¹ = diag(1/d₁, 1/d₂, ..., 1/dₙ)
- Eigenvalues are diagonal elements

4.4 Triangular Matrices:
-----------------------
Upper triangular: Uᵢⱼ = 0 for i > j
Lower triangular: Lᵢⱼ = 0 for i < j

Properties:
- Fast forward/backward substitution
- Determinant = product of diagonal elements
- Eigenvalues = diagonal elements

4.5 Symmetric Matrices:
----------------------
A = Aᵀ (A equals its transpose)

Properties:
- Real eigenvalues
- Orthogonal eigenvectors
- Positive definiteness concepts apply

4.6 Orthogonal Matrices:
-----------------------
QᵀQ = QQᵀ = I

Properties:
- Preserves lengths and angles
- Determinant = ±1
- Columns/rows form orthonormal basis
- Q⁻¹ = Qᵀ

4.7 Positive Definite Matrices:
------------------------------
xᵀAx > 0 for all x ≠ 0

Equivalent conditions:
- All eigenvalues positive
- All leading principal minors positive
- Cholesky decomposition exists

=======================================================

5. MATRIX INVERSE AND DETERMINANT
=================================

5.1 Matrix Inverse:
------------------
For square matrix A ∈ ℝⁿˣⁿ, inverse A⁻¹ satisfies:
AA⁻¹ = A⁻¹A = I

Existence condition: A must be non-singular (det(A) ≠ 0)

Properties:
- (A⁻¹)⁻¹ = A
- (AB)⁻¹ = B⁻¹A⁻¹
- (Aᵀ)⁻¹ = (A⁻¹)ᵀ
- (cA)⁻¹ = (1/c)A⁻¹

5.2 Computing Matrix Inverse:
----------------------------
Gaussian elimination with augmented matrix [A|I]
Row reduce to [I|A⁻¹]

For 2×2 matrix:
A = [a  b]  ⟹  A⁻¹ = 1/(ad-bc) [ d  -b]
    [c  d]                        [-c   a]

5.3 Determinant:
---------------
Scalar value det(A) or |A| for square matrix A

2×2 case: det([a b; c d]) = ad - bc

Recursive definition (cofactor expansion):
det(A) = Σⱼ (-1)ⁱ⁺ʲ aᵢⱼ Mᵢⱼ

where Mᵢⱼ is (i,j)-minor (determinant of submatrix)

5.4 Properties of Determinant:
-----------------------------
- det(AB) = det(A)det(B)
- det(Aᵀ) = det(A)
- det(cA) = cⁿdet(A) for n×n matrix
- det(A⁻¹) = 1/det(A)
- Row operations affect determinant predictably

5.5 Geometric Interpretation:
----------------------------
|det(A)| = volume of parallelepiped formed by column vectors
- det(A) > 0: preserves orientation
- det(A) < 0: reverses orientation
- det(A) = 0: collapses to lower dimension

=======================================================

6. RANK AND LINEAR SYSTEMS
==========================

6.1 Matrix Rank:
---------------
rank(A) = dimension of column space = number of linearly independent columns

Equivalent definitions:
- Number of linearly independent rows
- Number of non-zero rows in row echelon form
- Number of non-zero singular values

Properties:
- rank(A) ≤ min(m,n) for A ∈ ℝᵐˣⁿ
- rank(AB) ≤ min(rank(A), rank(B))
- rank(A) = rank(Aᵀ)

6.2 Full Rank:
-------------
A ∈ ℝᵐˣⁿ has full rank if rank(A) = min(m,n)

Full column rank: rank(A) = n (columns linearly independent)
Full row rank: rank(A) = m (rows linearly independent)

6.3 Linear Systems:
------------------
Ax = b where A ∈ ℝᵐˣⁿ, x ∈ ℝⁿ, b ∈ ℝᵐ

Solution existence and uniqueness:
- Unique solution: A square and invertible (x = A⁻¹b)
- No solution: b ∉ Col(A)
- Infinite solutions: b ∈ Col(A) but A not full column rank

6.4 Fundamental Theorem of Linear Algebra:
------------------------------------------
For A ∈ ℝᵐˣⁿ with rank r:

Four fundamental subspaces:
1. Column space: Col(A) ⊆ ℝᵐ, dim = r
2. Row space: Row(A) = Col(Aᵀ) ⊆ ℝⁿ, dim = r  
3. Null space: Null(A) ⊆ ℝⁿ, dim = n - r
4. Left null space: Null(Aᵀ) ⊆ ℝᵐ, dim = m - r

Orthogonal relationships:
- Row(A) ⊥ Null(A)
- Col(A) ⊥ Null(Aᵀ)

=======================================================

7. EIGENVALUES AND EIGENVECTORS
===============================

7.1 Definitions:
---------------
For square matrix A ∈ ℝⁿˣⁿ:

Eigenvalue λ and eigenvector v ≠ 0 satisfy:
Av = λv

Equivalently: (A - λI)v = 0

Geometric interpretation: v is direction unchanged by A (up to scaling by λ)

7.2 Characteristic Polynomial:
-----------------------------
det(A - λI) = 0

This gives characteristic polynomial of degree n
Roots are eigenvalues (counting multiplicities)

For 2×2 matrix A = [a b; c d]:
λ² - (a+d)λ + (ad-bc) = 0
λ = (a+d ± √((a+d)² - 4(ad-bc)))/2

7.3 Properties:
--------------
- Sum of eigenvalues = trace(A) = Σᵢ aᵢᵢ
- Product of eigenvalues = det(A)
- Eigenvalues of triangular matrix = diagonal elements
- If A invertible, eigenvalues of A⁻¹ are 1/λᵢ

7.4 Diagonalization:
-------------------
A is diagonalizable if A = PDP⁻¹ where:
- D = diag(λ₁, λ₂, ..., λₙ) (eigenvalues)
- P = [v₁ v₂ ... vₙ] (eigenvectors)

Conditions:
- A has n linearly independent eigenvectors
- Equivalently: geometric multiplicity = algebraic multiplicity for each eigenvalue

7.5 Special Cases:
-----------------
Symmetric matrices:
- Always diagonalizable
- Real eigenvalues
- Orthogonal eigenvectors
- Spectral theorem: A = QΛQᵀ where Q orthogonal

Positive definite matrices:
- All eigenvalues positive
- Important for optimization

=======================================================

8. MATRIX FACTORIZATIONS
========================

8.1 LU Decomposition:
--------------------
A = LU where L lower triangular, U upper triangular

Use: Efficient solution of linear systems
- Solve Ly = b (forward substitution)
- Solve Ux = y (backward substitution)

Computational complexity: O(n³) for decomposition, O(n²) per solve

8.2 QR Decomposition:
--------------------
A = QR where Q orthogonal, R upper triangular

Applications:
- Solving least squares problems
- Computing eigenvalues (QR algorithm)
- Gram-Schmidt orthogonalization

8.3 Singular Value Decomposition (SVD):
--------------------------------------
A = UΣVᵀ where:
- U ∈ ℝᵐˣᵐ orthogonal (left singular vectors)
- Σ ∈ ℝᵐˣⁿ diagonal (singular values σ₁ ≥ σ₂ ≥ ... ≥ 0)
- V ∈ ℝⁿˣⁿ orthogonal (right singular vectors)

Properties:
- Exists for any matrix (not just square)
- rank(A) = number of non-zero singular values
- ||A||₂ = σ₁ (largest singular value)

8.4 Cholesky Decomposition:
--------------------------
For positive definite A: A = LLᵀ where L lower triangular

Advantages:
- Half the storage of LU
- Numerically stable
- O(n³/3) complexity (faster than LU)

8.5 Eigendecomposition:
----------------------
For diagonalizable A: A = PDP⁻¹

Special case of SVD when A symmetric:
A = QΛQᵀ where Λ = diag(eigenvalues)

=======================================================

9. APPLICATIONS IN MACHINE LEARNING
===================================

9.1 Linear Models:
-----------------
Linear regression: y = Xβ + ε
- X ∈ ℝⁿˣᵖ design matrix
- Normal equations: (XᵀX)β = Xᵀy
- Solution: β = (XᵀX)⁻¹Xᵀy (if XᵀX invertible)

Ridge regression: β = (XᵀX + λI)⁻¹Xᵀy
- λI regularization term ensures invertibility

9.2 Principal Component Analysis (PCA):
--------------------------------------
Find directions of maximum variance:
1. Center data: X̃ = X - μ1ᵀ
2. Compute covariance: C = (1/n)X̃ᵀX̃
3. Eigendecomposition: C = QΛQᵀ
4. Principal components = columns of Q

Dimensionality reduction: Project onto top k eigenvectors

9.3 Neural Networks:
-------------------
Forward propagation: z⁽ˡ⁾ = W⁽ˡ⁾a⁽ˡ⁻¹⁾ + b⁽ˡ⁾
- Weight matrices W⁽ˡ⁾ encode learned transformations
- Batch processing uses matrix operations

Backpropagation: Gradients computed via matrix chain rule

9.4 Support Vector Machines:
---------------------------
Kernel matrix: K ∈ ℝⁿˣⁿ where Kᵢⱼ = k(xᵢ, xⱼ)
- Must be positive semidefinite (Mercer's theorem)
- Eigendecomposition ensures valid kernel

9.5 Recommender Systems:
-----------------------
Matrix factorization: R ≈ UVᵀ
- R ∈ ℝᵐˣⁿ user-item rating matrix
- U ∈ ℝᵐˣᵏ user factors
- V ∈ ℝⁿˣᵏ item factors

SVD and non-negative matrix factorization commonly used

9.6 Graph Analysis:
------------------
Adjacency matrix A: Aᵢⱼ = 1 if edge between nodes i,j
Laplacian matrix: L = D - A where D = diag(degree)

Spectral clustering uses eigendecomposition of L

9.7 Optimization:
----------------
Gradient descent: θₜ₊₁ = θₜ - α∇f(θₜ)
- Hessian matrix H = ∇²f captures curvature
- Newton's method: θₜ₊₁ = θₜ - H⁻¹∇f(θₜ)

Condition number κ(A) = σₘₐₓ/σₘᵢₙ affects convergence:
- κ(A) ≈ 1: fast convergence
- κ(A) >> 1: slow convergence

9.8 Computational Considerations:
-------------------------------
Memory efficiency:
- Sparse matrices for large, mostly-zero matrices
- Block operations for memory hierarchy
- In-place operations when possible

Numerical stability:
- Avoid explicit matrix inversion when possible
- Use QR instead of normal equations for least squares
- Regularization for ill-conditioned problems

GPU acceleration:
- Matrix operations are highly parallelizable
- BLAS libraries optimized for different architectures
- Frameworks like cuBLAS for GPU computing

Key Insights for ML:
- Matrices encode linear transformations fundamental to ML
- Matrix decompositions provide computational tools
- Understanding rank, eigenvalues crucial for analysis
- Numerical considerations affect practical implementation
- Most ML algorithms reduce to matrix operations

=======================================================
END OF DOCUMENT 