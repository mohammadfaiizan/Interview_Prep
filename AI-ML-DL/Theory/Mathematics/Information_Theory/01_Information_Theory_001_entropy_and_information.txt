ENTROPY AND INFORMATION - Information Theory Fundamentals
=========================================================

TABLE OF CONTENTS:
1. Information Measures and Entropy
2. Joint and Conditional Entropy
3. Mutual Information and Information Theory
4. Divergences and Information Distances  
5. Entropy Rate and Asymptotic Equipartition
6. Data Compression and Source Coding
7. Maximum Entropy Principle
8. Applications in Machine Learning

=======================================================

1. INFORMATION MEASURES AND ENTROPY
===================================

1.1 Self-Information:
--------------------
For event A with probability P(A):
Self-information: I(A) = -log P(A) = log(1/P(A))

Properties:
- I(A) ‚â• 0 (information is non-negative)
- I(A) = 0 iff P(A) = 1 (certain events carry no information)
- I(A) increases as P(A) decreases (rare events are informative)
- I(A ‚à© B) = I(A) + I(B) if A and B independent (additivity)

Units depend on logarithm base:
- Base 2: bits
- Base e: nats  
- Base 10: dits (decimals)

1.2 Shannon Entropy:
-------------------
For discrete random variable X with PMF p(x):
H(X) = -‚àë‚Çì p(x) log p(x) = ùîº[I(X)]

Alternative notation: H(p) when emphasizing distribution

Properties:
- H(X) ‚â• 0 (non-negativity)
- H(X) = 0 iff X is deterministic
- H(X) ‚â§ log |ùí≥| with equality iff X uniform (maximum entropy)
- Concave function of probability distribution

Intuition: Average amount of information (surprise) in X

1.3 Binary Entropy Function:
----------------------------
For Bernoulli(p) random variable:
H(p) = -p log p - (1-p) log(1-p)

Properties:
- H(0) = H(1) = 0
- H(1/2) = 1 bit (maximum)
- Symmetric around p = 1/2
- Strictly concave on (0,1)

Applications:
- Fundamental in coding theory
- Analysis of binary classification
- Bounds in learning theory

1.4 Differential Entropy:
------------------------
For continuous random variable X with PDF f(x):
h(X) = -‚à´ f(x) log f(x) dx

Properties:
- Can be negative (unlike discrete entropy)
- Not invariant under coordinate transformation
- h(X + a) = h(X) (translation invariant)
- h(aX) = h(X) + log |a| (scaling)

Examples:
- Uniform on [a,b]: h(X) = log(b - a)
- Gaussian N(Œº, œÉ¬≤): h(X) = ¬Ω log(2œÄeœÉ¬≤)
- Exponential with rate Œª: h(X) = 1 - log Œª

1.5 Relative Entropy (KL Divergence):
------------------------------------
For distributions P and Q:
D(P||Q) = ‚àë‚Çì p(x) log(p(x)/q(x))

Continuous case:
D(P||Q) = ‚à´ f(x) log(f(x)/g(x)) dx

Properties:
- D(P||Q) ‚â• 0 (non-negativity)
- D(P||Q) = 0 iff P = Q (almost everywhere)
- Not symmetric: D(P||Q) ‚â† D(Q||P) in general
- Not a metric (triangle inequality fails)

Information divergence: Measures "distance" between distributions

1.6 Cross-Entropy:
-----------------
H(P,Q) = -‚àë‚Çì p(x) log q(x) = H(P) + D(P||Q)

Interpretation: Expected code length when using code optimized for Q to encode data from P

Applications:
- Loss function in classification
- Model evaluation metric
- Connection to maximum likelihood

=======================================================

2. JOINT AND CONDITIONAL ENTROPY
================================

2.1 Joint Entropy:
-----------------
For joint distribution p(x,y):
H(X,Y) = -‚àë‚Çì,·µß p(x,y) log p(x,y)

Interpretation: Uncertainty in joint outcome

Chain rule for entropy:
H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)

Subadditivity:
H(X,Y) ‚â§ H(X) + H(Y)

Equality iff X and Y independent

2.2 Conditional Entropy:
-----------------------
H(Y|X) = -‚àë‚Çì,·µß p(x,y) log p(y|x)
       = ‚àë‚Çì p(x)H(Y|X = x)
       = ùîº‚Çì[H(Y|X = x)]

Properties:
- H(Y|X) ‚â• 0
- H(Y|X) ‚â§ H(Y) (conditioning reduces entropy)
- H(Y|X) = H(Y) iff X and Y independent
- H(Y|Y) = 0 (no uncertainty given perfect information)

2.3 Chain Rule Generalization:
-----------------------------
For random variables X‚ÇÅ, X‚ÇÇ, ..., X‚Çô:
H(X‚ÇÅ, X‚ÇÇ, ..., X‚Çô) = ‚àë·µ¢‚Çå‚ÇÅ‚Åø H(X·µ¢|X‚ÇÅ, ..., X·µ¢‚Çã‚ÇÅ)

Consequences:
- Each conditioning term non-negative
- Provides decomposition of joint uncertainty
- Useful for analyzing information flow

2.4 Data Processing Inequality:
------------------------------
If X ‚Üí Y ‚Üí Z forms Markov chain, then:
H(X|Z) ‚â• H(X|Y)

Equivalently: I(X;Z) ‚â§ I(X;Y)

Interpretation: Processing cannot increase information
Z contains less information about X than Y does

Applications:
- Bounds in communication systems
- Feature selection principles  
- Deep learning layer analysis

2.5 Fano's Inequality:
---------------------
For estimating X from Y with error probability P‚Çë:
H(X|Y) ‚â§ H(P‚Çë) + P‚Çë log(|ùí≥| - 1)

Fundamental limit relating entropy to error probability

Weak form: H(X|Y) ‚â§ 1 + P‚Çë log |ùí≥|

Applications:
- Lower bounds on error probability
- Converse theorems in information theory
- Fundamental limits in learning theory

=======================================================

3. MUTUAL INFORMATION AND INFORMATION THEORY
============================================

3.1 Mutual Information:
----------------------
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
       = H(X) + H(Y) - H(X,Y)
       = D(p(x,y)||p(x)p(y))

Interpretation: Amount of information shared between X and Y

Properties:
- I(X;Y) ‚â• 0 (non-negativity)
- I(X;Y) = 0 iff X and Y independent
- I(X;Y) = I(Y;X) (symmetry)
- I(X;X) = H(X)

3.2 Conditional Mutual Information:
----------------------------------
I(X;Y|Z) = H(X|Z) - H(X|Y,Z)
         = H(X,Z) + H(Y,Z) - H(Z) - H(X,Y,Z)

Chain rule for mutual information:
I(X;Y,Z) = I(X;Y) + I(X;Z|Y)

Properties:
- I(X;Y|Z) ‚â• 0
- I(X;Y|Z) can be larger or smaller than I(X;Y)
- If X ‚Üí Z ‚Üí Y, then I(X;Y|Z) = 0

3.3 Information Diagrams:
------------------------
Venn diagram analogy for information measures:
- H(X): Area of circle X
- I(X;Y): Intersection of circles X and Y
- H(X|Y): Area of X not overlapping Y

For three variables:
- Seven regions corresponding to different information components
- Useful for visualizing information relationships

3.4 Multivariate Mutual Information:
-----------------------------------
Interaction information:
I(X;Y;Z) = I(X;Y|Z) - I(X;Y)

Can be positive, negative, or zero
Measures interaction beyond pairwise relationships

Total correlation:
C(X‚ÇÅ, ..., X‚Çô) = ‚àë·µ¢ H(X·µ¢) - H(X‚ÇÅ, ..., X‚Çô)

Measures total dependence among variables

3.5 Information Inequalities:
----------------------------
Submodularity of entropy:
H(X ‚à™ Y) + H(X ‚à© Y) ‚â§ H(X) + H(Y)

Han's inequality:
(n-1)H(X‚ÇÅ, ..., X‚Çô) ‚â§ ‚àë·µ¢ H(X‚ÇÅ, ..., X·µ¢‚Çã‚ÇÅ, X·µ¢‚Çä‚ÇÅ, ..., X‚Çô)

Shearer's lemma: Generalization to arbitrary subsets

Applications in:
- Concentration inequalities
- Combinatorial optimization
- Network information theory

=======================================================

4. DIVERGENCES AND INFORMATION DISTANCES
========================================

4.1 f-Divergences:
-----------------
General family: D‚Çì(P||Q) = ‚à´ q(x)f(p(x)/q(x)) dx

Where f is convex function with f(1) = 0

Examples:
- KL divergence: f(t) = t log t
- Hellinger distance: f(t) = (‚àöt - 1)¬≤
- Total variation: f(t) = ¬Ω|t - 1|
- œá¬≤ divergence: f(t) = (t - 1)¬≤

Properties:
- Convex in both arguments
- f-divergence ‚â• 0
- Joint convexity: D‚Çì(ŒªP‚ÇÅ + (1-Œª)P‚ÇÇ||ŒªQ‚ÇÅ + (1-Œª)Q‚ÇÇ) ‚â§ ŒªD‚Çì(P‚ÇÅ||Q‚ÇÅ) + (1-Œª)D‚Çì(P‚ÇÇ||Q‚ÇÇ)

4.2 Jensen-Shannon Divergence:
-----------------------------
JS(P,Q) = ¬ΩD(P||M) + ¬ΩD(Q||M)

Where M = ¬Ω(P + Q) is mixture distribution

Properties:
- Symmetric: JS(P,Q) = JS(Q,P)
- Bounded: 0 ‚â§ JS(P,Q) ‚â§ 1 (in bits)
- ‚àöJS is metric

Applications:
- Phylogenetic analysis
- Document similarity
- GAN training (in some formulations)

4.3 Wasserstein Distance:
------------------------
For distributions P and Q on metric space:
W‚ÇÅ(P,Q) = inf_{Œ≥‚ààŒì(P,Q)} ‚à´ d(x,y) dŒ≥(x,y)

Where Œì(P,Q) is set of couplings with marginals P and Q

Properties:
- True metric (satisfies triangle inequality)
- Metrizes weak convergence
- Computationally tractable via optimal transport

Applications:
- Wasserstein GANs
- Domain adaptation
- Barycenter problems

4.4 Maximum Mean Discrepancy (MMD):
----------------------------------
For reproducing kernel Hilbert space ‚Ñã:
MMD(P,Q) = ||Œº‚Çö - Œº·µ©||‚Ñã

Where Œº‚Çö = ‚à´ k(x,¬∑) dP(x) is mean embedding

Empirical estimate:
MMD¬≤(P,Q) = (1/m¬≤)‚àë·µ¢,‚±º k(x·µ¢,x‚±º) + (1/n¬≤)‚àë·µ¢,‚±º k(y·µ¢,y‚±º) - (2/mn)‚àë·µ¢,‚±º k(x·µ¢,y‚±º)

Applications:
- Two-sample testing
- Domain adaptation
- Generative model evaluation

4.5 Information Geometry:
------------------------
Riemannian geometry on probability manifolds
Fisher information as Riemannian metric:
g·µ¢‚±º(Œ∏) = ùîº[‚àÇlog p(x;Œ∏)/‚àÇŒ∏·µ¢ √ó ‚àÇlog p(x;Œ∏)/‚àÇŒ∏‚±º]

Natural gradient: Use Fisher metric for optimization
‚àáÃÉ‚Ñì = F‚Åª¬π‚àá‚Ñì where F is Fisher information matrix

Œ±-connections: Family of affine connections
Œ± = 1: mixture connection (e-connection)
Œ± = -1: exponential connection (m-connection)

Applications:
- Natural parameter optimization
- Variational inference
- Statistical manifolds

=======================================================

5. ENTROPY RATE AND ASYMPTOTIC EQUIPARTITION
============================================

5.1 Entropy Rate:
----------------
For stochastic process {X·µ¢}:
H(ùí≥) = lim_{n‚Üí‚àû} (1/n)H(X‚ÇÅ, X‚ÇÇ, ..., X‚Çô)

When limit exists (entropy rate or entropy per symbol)

Alternative definition:
H'(ùí≥) = lim_{n‚Üí‚àû} H(X‚Çô|X‚ÇÅ, ..., X‚Çô‚Çã‚ÇÅ)

For stationary processes: H(ùí≥) = H'(ùí≥)

Examples:
- i.i.d. process: H(ùí≥) = H(X‚ÇÅ)
- Markov chain: H(ùí≥) = H(X‚ÇÇ|X‚ÇÅ) for stationary chain

5.2 Asymptotic Equipartition Property (AEP):
-------------------------------------------
For i.i.d. process with entropy H:
-(1/n)log p(X‚ÇÅ, ..., X‚Çô) ‚Üí H in probability

Typical set A_Œµ^(n):
A_Œµ^(n) = {(x‚ÇÅ, ..., x‚Çô) : |-(1/n)log p(x‚ÇÅ, ..., x‚Çô) - H| ‚â§ Œµ}

Properties:
- P(A_Œµ^(n)) ‚Üí 1 as n ‚Üí ‚àû
- |A_Œµ^(n)| ‚â§ 2^{n(H+Œµ)}
- |A_Œµ^(n)| ‚â• (1-Œµ)2^{n(H-Œµ)} for large n

5.3 Source Coding Theorem:
--------------------------
Minimum expected code length ‚â• H (source entropy)
Can achieve expected code length arbitrarily close to H

Huffman coding: Optimal for prefix-free codes
Arithmetic coding: Approaches entropy rate for long sequences

Block coding: Code sequences instead of symbols
Rate: R = (1/n)ùîº[‚Ñì(X‚ÇÅ, ..., X‚Çô)]
Achievable rate: R ‚â• H

5.4 Lossy Compression:
---------------------
Rate-distortion theory: Trade-off between compression and fidelity

Rate-distortion function:
R(D) = min_{p(xÃÇ|x): ùîº[d(X,XÃÇ)]‚â§D} I(X;XÃÇ)

Properties:
- R(D) decreasing and convex in D
- R(0) = H(X) (lossless)
- R(D‚Çò‚Çê‚Çì) = 0 (maximum distortion)

Examples:
- Gaussian: R(D) = ¬Ωlog(œÉ¬≤/D) for D ‚â§ œÉ¬≤
- Binary: Explicit formulas available

5.5 Universal Compression:
-------------------------
Compress without knowing source distribution

Lempel-Ziv algorithms: Achieve entropy rate asymptotically
Context tree weighting: Bayesian approach
Prediction by partial matching: Variable order Markov models

Redundancy: Excess over entropy
r_n = ùîº[‚Ñì_n] - nH

Minimax redundancy: (k/2)log n + O(1) for k-parameter family

=======================================================

6. DATA COMPRESSION AND SOURCE CODING
=====================================

6.1 Prefix-Free Codes:
---------------------
Code C: Mapping from alphabet to binary strings
Prefix-free: No codeword is prefix of another

Kraft inequality: ‚àë·µ¢ 2^{-l·µ¢} ‚â§ 1 for lengths l·µ¢
Equality achievable iff prefix-free code exists

Expected length: L = ‚àë·µ¢ p(x·µ¢)l·µ¢
Optimal: Minimize L subject to Kraft inequality

6.2 Huffman Coding:
------------------
Greedy algorithm for optimal prefix-free codes:
1. Merge two least probable symbols
2. Recursively code merged symbols
3. Assign bits based on tree structure

Properties:
- Optimal among prefix-free codes
- H(X) ‚â§ L < H(X) + 1
- Within 1 bit of entropy bound

Block Huffman: Code sequences for better compression
Asymptotically achieves entropy rate

6.3 Shannon-Fano-Elias Coding:
-----------------------------
Based on cumulative distribution function
Codeword length: ‚åàlog(1/p(x))‚åâ + 1

Algorithm:
1. Compute CDF: F(x) = ‚àë_{y<x} p(y)
2. Use FÃÑ(x) = F(x) + p(x)/2
3. Binary expansion of FÃÑ(x)

Achieves H(X) ‚â§ L < H(X) + 2

6.4 Arithmetic Coding:
---------------------
Represent entire sequence by single number in [0,1)
Recursively partition interval based on symbol probabilities

Advantages:
- Fractional bits per symbol
- Approaches entropy rate for long sequences
- Adaptive to changing probabilities

Implementation issues:
- Finite precision arithmetic
- Streaming requirements
- Complexity considerations

6.5 Universal Codes:
-------------------
For integers without known distribution:

Elias gamma code: Encode n in ‚åàlog n‚åâ + 2‚åälog n‚åã + 1 bits
Elias delta code: ‚åàlog n‚åâ + ‚åàlog(‚åàlog n‚åâ + 1)‚åâ + 2‚åälog(‚åàlog n‚åâ + 1)‚åã + 1 bits

Universal for sequences:
Lempel-Ziv-Welch (LZW): Dictionary-based
Burrows-Wheeler transform: Preprocessing for better compression

6.6 Kolmogorov Complexity:
-------------------------
K(x) = min{|p| : p is program that outputs x}

Properties:
- Uncomputable function
- Universal (up to constant)
- K(x) ‚â§ |x| + O(1)

Incompressible strings: K(x) ‚â• |x|
Most strings incompressible

Connection to randomness:
- Random strings have high Kolmogorov complexity
- Provides foundation for algorithmic information theory

=======================================================

7. MAXIMUM ENTROPY PRINCIPLE
============================

7.1 Maximum Entropy Principle:
------------------------------
Choose distribution with maximum entropy subject to constraints

Rationale:
- Least biased estimate
- Most conservative inference
- Information-theoretic justification

General form:
Maximize H(p) = -‚àë·µ¢ p(x·µ¢) log p(x·µ¢)
Subject to: ‚àë·µ¢ p(x·µ¢)f‚±º(x·µ¢) = Œ±‚±º for j = 1, ..., m
           ‚àë·µ¢ p(x·µ¢) = 1

7.2 Lagrangian Solution:
-----------------------
L = -‚àë·µ¢ p(x·µ¢) log p(x·µ¢) - ‚àë‚±º Œª‚±º(‚àë·µ¢ p(x·µ¢)f‚±º(x·µ¢) - Œ±‚±º) - Œº(‚àë·µ¢ p(x·µ¢) - 1)

Solution: p(x·µ¢) = exp(-‚àë‚±º Œª‚±ºf‚±º(x·µ¢))/Z
Where Z = ‚àë·µ¢ exp(-‚àë‚±º Œª‚±ºf‚±º(x·µ¢)) is partition function

Exponential family form: Natural parameterization

7.3 Examples:
------------
No constraints: Uniform distribution
Mean constraint: Exponential distribution
Mean and variance constraints: Gaussian distribution

Discrete uniform: Maximum entropy on finite set
Geometric: Maximum entropy on {1, 2, 3, ...} with given mean
Poisson: Maximum entropy on {0, 1, 2, ...} with given mean

7.4 Continuous Maximum Entropy:
------------------------------
Support constraint: Uniform on support
Mean constraint: Exponential (positive support)
Mean and variance: Gaussian (unlimited support)
Positive with given moments: Gamma distribution

Log-concave maximum entropy distributions:
Include many common distributions

7.5 Relative Entropy Maximization:
---------------------------------
Maximize H(p,q) = -‚àë·µ¢ p(x·µ¢) log q(x·µ¢)
Subject to constraints on p

Equivalent to minimizing D(p||r) where r is reference distribution

Applications:
- Bayesian updating
- Portfolio optimization
- Image reconstruction

7.6 Maximum Entropy Learning:
----------------------------
Feature-based models: p(y|x) ‚àù exp(‚àë‚±º Œª‚±ºf‚±º(x,y))

Logistic regression: Binary classification maximum entropy
Conditional random fields: Structured maximum entropy models

Training: Find Œª to match empirical feature expectations
Convex optimization problem

=======================================================

8. APPLICATIONS IN MACHINE LEARNING
===================================

8.1 Feature Selection:
---------------------
Mutual information between features and target:
I(X;Y) measures relevance of feature X for predicting Y

Challenges:
- High-dimensional estimation
- Bias in finite samples
- Continuous variables

Minimum redundancy maximum relevance (mRMR):
Maximize relevance I(X;Y)
Minimize redundancy I(X;S) where S is selected features

Information gain: IG(S,A) = H(S) - H(S|A)
Used in decision tree construction

8.2 Information-Theoretic Clustering:
------------------------------------
Information bottleneck: Compress X while preserving information about Y
min I(T;X) - Œ≤I(T;Y)

Deterministic annealing: Gradually increase Œ≤
Phase transitions in optimal solutions

Mutual information clustering:
Maximize I(C;X) where C are cluster assignments

8.3 Generative Models:
---------------------
Maximum likelihood ‚â° Minimize cross-entropy
Cross-entropy loss: H(p,q) = -‚àë·µ¢ p(x·µ¢) log q(x·µ¢)

Variational autoencoders: Information-theoretic interpretation
- Reconstruction: -log p(x|z)
- Regularization: D(q(z|x)||p(z))

GANs: Minimax game related to JS divergence
WGAN: Use Wasserstein distance

8.4 Deep Learning Theory:
------------------------
Information processing inequality: Deep networks as Markov chains
Each layer can only lose information about input

Information plane: Plot I(T;X) vs I(T;Y) for each layer T
Compression and generalization phases

Dropout as information bottleneck:
Random masking adds noise, forces compression

8.5 Reinforcement Learning:
--------------------------
Entropy regularization: Encourage exploration
Policy entropy: H(œÄ) = -‚àë‚Çê œÄ(a|s) log œÄ(a|s)

Maximum entropy RL: Add entropy bonus to reward
Soft actor-critic: Entropy-regularized policy optimization

Information gain for exploration:
Choose actions to maximize information about environment

8.6 Causal Discovery:
--------------------
Conditional independence testing using mutual information
I(X;Y|Z) = 0 iff X ‚ä• Y | Z

Causal direction: Use asymmetry in algorithms
Information-geometric causal inference

Transfer entropy: Directed information flow
Granger causality in information-theoretic terms

8.7 Natural Language Processing:
-------------------------------
Language model perplexity: 2^{H(X)}
Lower perplexity = better model

Pointwise mutual information: Association strength
PMI(x,y) = log(p(x,y)/(p(x)p(y)))

Topic models: Information-theoretic interpretation
Latent Dirichlet allocation as information bottleneck

8.8 Neuroscience Applications:
-----------------------------
Neural coding: How neurons encode information
Mutual information between stimulus and response

Information integration: Consciousness theories
Integrated information theory (IIT)

Transfer entropy: Information flow between brain regions
Effective connectivity analysis

8.9 Fairness and Interpretability:
---------------------------------
Equalized odds: Mutual information constraints
I(≈∂;S|Y) = 0 where S is sensitive attribute

Information-theoretic interpretability:
Relevance: I(X;Y)
Redundancy: I(X;Z) for features X,Z

Minimal sufficient statistic: T(X) with I(T(X);Y) = I(X;Y)
Removes irrelevant information

Key Insights for ML:
- Information theory provides fundamental limits
- Entropy quantifies uncertainty and information content
- Mutual information measures statistical dependence
- Maximum entropy principle guides model selection
- Cross-entropy loss has information-theoretic foundation
- Information bottleneck balances compression and prediction
- Concentration inequalities bound information-theoretic quantities

=======================================================
END OF DOCUMENT 