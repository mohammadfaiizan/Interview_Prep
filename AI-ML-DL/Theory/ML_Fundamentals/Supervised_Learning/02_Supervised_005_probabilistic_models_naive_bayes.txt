PROBABILISTIC MODELS AND NAIVE BAYES - Bayesian Approaches to Classification
============================================================================

TABLE OF CONTENTS:
1. Probabilistic Classification Fundamentals
2. Naive Bayes Classifier
3. Gaussian Discriminant Analysis
4. Advanced Probabilistic Models
5. Bayesian Parameter Estimation
6. Model Selection and Evaluation
7. Extensions and Variants
8. Applications and Practical Considerations

=======================================================

1. PROBABILISTIC CLASSIFICATION FUNDAMENTALS
============================================

1.1 Bayesian Decision Theory:
----------------------------
Probabilistic Framework:
- Feature vector: x ‚àà ‚Ñù·µà
- Class variable: y ‚àà {1, 2, ..., K}
- Joint distribution: p(x, y)

Bayes' Theorem:
p(y|x) = p(x|y)p(y) / p(x)

Where:
- p(y|x): Posterior probability
- p(x|y): Class-conditional likelihood
- p(y): Prior probability
- p(x): Evidence (marginal likelihood)

1.2 Optimal Decision Rule:
-------------------------
Bayes Classifier:
≈∑ = argmax_k p(y = k|x)

Using Bayes' theorem:
≈∑ = argmax_k p(x|y = k)p(y = k)

Bayes Risk:
R* = ‚à´ min_k p(y = k|x) dx

Optimal Error Rate:
No classifier can achieve lower error rate than Bayes classifier

1.3 Loss Functions:
------------------
0-1 Loss:
L(y, ≈∑) = ùüô[y ‚â† ≈∑]

Expected Loss:
R(ƒù) = ‚àë‚Çì ‚àë·µß L(y, ƒù(x)) p(x, y)

Minimum Risk Decision:
ƒù*(x) = argmin_ƒù ‚àë·µß L(y, ƒù) p(y|x)

1.4 Generative vs Discriminative Models:
---------------------------------------
Generative Models:
- Model joint distribution p(x, y)
- Learn p(x|y) and p(y)
- Can generate new samples
- Examples: Naive Bayes, QDA

Discriminative Models:
- Model conditional distribution p(y|x)
- Directly optimize classification boundary
- Examples: Logistic regression, SVM

Trade-offs:
- Generative: Better with small data, interpretable
- Discriminative: Better asymptotic performance

1.5 Maximum Likelihood Estimation:
---------------------------------
Likelihood Function:
L(Œ∏) = ‚àè·µ¢‚Çå‚ÇÅ‚Åø p(x·µ¢, y·µ¢|Œ∏)

Log-Likelihood:
‚Ñì(Œ∏) = ‚àë·µ¢‚Çå‚ÇÅ‚Åø log p(x·µ¢, y·µ¢|Œ∏)

MLE Estimator:
Œ∏ÃÇ = argmax_Œ∏ ‚Ñì(Œ∏)

Properties:
- Consistent: Œ∏ÃÇ ‚Üí Œ∏* as n ‚Üí ‚àû
- Asymptotically normal
- Efficient (minimum variance)

=======================================================

2. NAIVE BAYES CLASSIFIER
=========================

2.1 Conditional Independence Assumption:
---------------------------------------
Naive Bayes Assumption:
p(x‚ÇÅ, x‚ÇÇ, ..., x·µà|y) = ‚àè‚±º‚Çå‚ÇÅ·µà p(x‚±º|y)

Features are conditionally independent given class label

Classifier:
≈∑ = argmax_k p(y = k) ‚àè‚±º‚Çå‚ÇÅ·µà p(x‚±º|y = k)

Log-form:
≈∑ = argmax_k [log p(y = k) + ‚àë‚±º‚Çå‚ÇÅ·µà log p(x‚±º|y = k)]

2.2 Parameter Estimation:
------------------------
Prior Probabilities:
pÃÇ(y = k) = n‚Çñ/n

where n‚Çñ is number of samples in class k

Class-Conditional Probabilities:
pÃÇ(x‚±º|y = k) depends on feature type

For categorical features:
pÃÇ(x‚±º = v|y = k) = count(x‚±º = v, y = k)/n‚Çñ

2.3 Gaussian Naive Bayes:
------------------------
Assumption: Features follow Gaussian distribution
p(x‚±º|y = k) = N(Œº‚±º‚Çñ, œÉ¬≤‚±º‚Çñ)

Parameter Estimation:
ŒºÃÇ‚±º‚Çñ = (1/n‚Çñ) ‚àë·µ¢:y·µ¢=k x·µ¢‚±º

œÉÃÇ¬≤‚±º‚Çñ = (1/n‚Çñ) ‚àë·µ¢:y·µ¢=k (x·µ¢‚±º - ŒºÃÇ‚±º‚Çñ)¬≤

Decision Boundary:
Quadratic in x (if different variances per class)
Linear if œÉ¬≤‚±º‚Çñ = œÉ¬≤‚±º for all k

2.4 Multinomial Naive Bayes:
---------------------------
For count data (e.g., text):
p(x|y = k) = (‚àë‚±º x‚±º)! / ‚àè‚±º x‚±º! √ó ‚àè‚±º Œ∏‚±º‚ÇñÀ£ ≤

where ‚àë‚±º Œ∏‚±º‚Çñ = 1

Parameter Estimation:
Œ∏ÃÇ‚±º‚Çñ = (‚àë·µ¢:y·µ¢=k x·µ¢‚±º + Œ±) / (‚àë‚±º ‚àë·µ¢:y·µ¢=k x·µ¢‚±º + dŒ±)

Œ± > 0 is smoothing parameter (Laplace smoothing)

2.5 Bernoulli Naive Bayes:
-------------------------
For binary features:
p(x‚±º|y = k) = Œ∏‚±º‚ÇñÀ£ ≤ (1 - Œ∏‚±º‚Çñ)^(1-x‚±º)

Parameter Estimation:
Œ∏ÃÇ‚±º‚Çñ = (‚àë·µ¢:y·µ¢=k x·µ¢‚±º + Œ±) / (n‚Çñ + 2Œ±)

Good for document classification with binary word presence

2.6 Smoothing Techniques:
------------------------
Laplace Smoothing (Add-one):
Œ∏ÃÇ‚±º‚Çñ = (count + 1) / (total + vocabulary_size)

Add-Œ± Smoothing:
Œ∏ÃÇ‚±º‚Çñ = (count + Œ±) / (total + Œ± √ó vocabulary_size)

Good-Turing Smoothing:
Redistribute probability mass from seen to unseen events

Advantages:
- Prevents zero probabilities
- Better generalization
- Handles unseen feature values

2.7 Feature Selection for Naive Bayes:
-------------------------------------
Mutual Information:
I(X; Y) = ‚àë‚Çì,·µß p(x, y) log(p(x, y)/(p(x)p(y)))

Chi-Square Test:
œá¬≤ = ‚àë·µ¢ (O·µ¢ - E·µ¢)¬≤/E·µ¢

Information Gain:
IG(Y, X) = H(Y) - H(Y|X)

Select features with highest relevance to class

=======================================================

3. GAUSSIAN DISCRIMINANT ANALYSIS
=================================

3.1 Linear Discriminant Analysis (LDA):
--------------------------------------
Assumptions:
- Gaussian class-conditionals: p(x|y = k) = N(Œº‚Çñ, Œ£)
- Shared covariance matrix across classes
- Equal priors: p(y = k) = 1/K (optional)

Decision Boundary:
Linear hyperplanes

Discriminant Function:
Œ¥‚Çñ(x) = x·µÄŒ£‚Åª¬πŒº‚Çñ - ¬ΩŒº‚Çñ·µÄŒ£‚Åª¬πŒº‚Çñ + log p(y = k)

Classification Rule:
≈∑ = argmax_k Œ¥‚Çñ(x)

3.2 Parameter Estimation for LDA:
--------------------------------
Class Means:
ŒºÃÇ‚Çñ = (1/n‚Çñ) ‚àë·µ¢:y·µ¢=k x·µ¢

Pooled Covariance:
Œ£ÃÇ = (1/(n-K)) ‚àë‚Çñ ‚àë·µ¢:y·µ¢=k (x·µ¢ - ŒºÃÇ‚Çñ)(x·µ¢ - ŒºÃÇ‚Çñ)·µÄ

Prior Probabilities:
pÃÇ(y = k) = n‚Çñ/n

3.3 Quadratic Discriminant Analysis (QDA):
-----------------------------------------
Assumptions:
- Gaussian class-conditionals: p(x|y = k) = N(Œº‚Çñ, Œ£‚Çñ)
- Different covariance per class
- More flexible than LDA

Decision Boundary:
Quadratic surfaces

Discriminant Function:
Œ¥‚Çñ(x) = -¬Ωlog|Œ£‚Çñ| - ¬Ω(x-Œº‚Çñ)·µÄŒ£‚Çñ‚Åª¬π(x-Œº‚Çñ) + log p(y = k)

Parameter Estimation:
Œ£ÃÇ‚Çñ = (1/n‚Çñ) ‚àë·µ¢:y·µ¢=k (x·µ¢ - ŒºÃÇ‚Çñ)(x·µ¢ - ŒºÃÇ‚Çñ)·µÄ

3.4 Bias-Variance Trade-off:
---------------------------
LDA vs QDA:
- LDA: Higher bias, lower variance
- QDA: Lower bias, higher variance
- LDA better with small samples
- QDA better when classes have different covariances

Parameter Count:
- LDA: K√ód + d(d+1)/2 + K-1
- QDA: K√ód + K√ód(d+1)/2 + K-1

3.5 Regularized Discriminant Analysis:
-------------------------------------
Regularized Covariance:
Œ£‚Çñ(Œª) = (1-Œª)Œ£‚Çñ + ŒªŒ£

Shrinkage toward pooled covariance
Œª ‚àà [0,1] controls regularization

Ridge-like Regularization:
Œ£‚Çñ(Œ≥) = (1-Œ≥)Œ£‚Çñ + Œ≥œÉ¬≤I

Shrinkage toward diagonal matrix

3.6 Dimensionality Reduction:
----------------------------
Fisher's Linear Discriminant:
Project to subspace maximizing between-class vs within-class scatter

Between-class Scatter:
S·µ¶ = ‚àë‚Çñ n‚Çñ(Œº‚Çñ - Œº)(Œº‚Çñ - Œº)·µÄ

Within-class Scatter:
S·µ• = ‚àë‚Çñ ‚àë·µ¢:y·µ¢=k (x·µ¢ - Œº‚Çñ)(x·µ¢ - Œº‚Çñ)·µÄ

Objective:
max_w w·µÄS·µ¶w / w·µÄS·µ•w

Solution: Eigenvectors of S·µ•‚Åª¬πS·µ¶

=======================================================

4. ADVANCED PROBABILISTIC MODELS
================================

4.1 Mixture Models:
------------------
Gaussian Mixture Model:
p(x) = ‚àë‚Çñ œÄ‚Çñ N(x; Œº‚Çñ, Œ£‚Çñ)

where œÄ‚Çñ are mixing coefficients

For classification:
p(x|y = c) = ‚àë‚Çñ œÄ‚Çñc N(x; Œº‚Çñc, Œ£‚Çñc)

Allows multiple modes per class

4.2 EM Algorithm for GMM:
------------------------
E-step:
Œ≥·µ¢‚Çñ = œÄ‚Çñ N(x·µ¢; Œº‚Çñ, Œ£‚Çñ) / ‚àë‚±º œÄ‚±º N(x·µ¢; Œº‚±º, Œ£‚±º)

M-step:
œÄ‚Çñ = (1/n) ‚àë·µ¢ Œ≥·µ¢‚Çñ
Œº‚Çñ = ‚àë·µ¢ Œ≥·µ¢‚Çñ x·µ¢ / ‚àë·µ¢ Œ≥·µ¢‚Çñ
Œ£‚Çñ = ‚àë·µ¢ Œ≥·µ¢‚Çñ (x·µ¢ - Œº‚Çñ)(x·µ¢ - Œº‚Çñ)·µÄ / ‚àë·µ¢ Œ≥·µ¢‚Çñ

4.3 Bayesian Networks:
---------------------
Graphical Model:
- Nodes: Random variables
- Edges: Conditional dependencies
- Directed Acyclic Graph (DAG)

Factorization:
p(x‚ÇÅ, ..., x‚Çô) = ‚àè·µ¢ p(x·µ¢|pa(x·µ¢))

where pa(x·µ¢) are parents of node i

Naive Bayes as Bayesian Network:
- Class node as parent of all features
- No edges between features

4.4 Hidden Markov Models:
------------------------
For sequential data:
- Hidden states: z‚ÇÅ, z‚ÇÇ, ..., z‚Çú
- Observations: x‚ÇÅ, x‚ÇÇ, ..., x‚Çú
- Markov assumption: p(z‚Çú|z‚ÇÅ,...,z‚Çú‚Çã‚ÇÅ) = p(z‚Çú|z‚Çú‚Çã‚ÇÅ)

Joint Distribution:
p(x‚ÇÅ:‚Çú, z‚ÇÅ:‚Çú) = p(z‚ÇÅ)‚àè‚Çú p(z‚Çú|z‚Çú‚Çã‚ÇÅ)p(x‚Çú|z‚Çú)

Applications:
- Speech recognition
- Part-of-speech tagging
- Gene finding

4.5 Maximum Entropy Models:
---------------------------
Principle: Choose distribution with maximum entropy subject to constraints

Exponential Family Form:
p(y|x) = exp(‚àë·µ¢ Œª·µ¢f·µ¢(x,y) - A(Œª))

where f·µ¢(x,y) are feature functions

Equivalent to logistic regression for classification

4.6 Conditional Random Fields:
-----------------------------
Undirected graphical model for structured prediction

Linear-chain CRF:
p(y|x) = exp(‚àë‚Çú ‚àë·µ¢ Œª·µ¢f·µ¢(y‚Çú‚Çã‚ÇÅ,y‚Çú,x,t)) / Z(x)

Applications:
- Named entity recognition
- Part-of-speech tagging
- Image segmentation

=======================================================

5. BAYESIAN PARAMETER ESTIMATION
================================

5.1 Bayesian Approach:
---------------------
Prior Distribution:
œÄ(Œ∏) encodes beliefs before seeing data

Likelihood:
L(Œ∏|D) = ‚àè·µ¢ p(x·µ¢|Œ∏)

Posterior Distribution:
œÄ(Œ∏|D) ‚àù L(Œ∏|D)œÄ(Œ∏)

Predictive Distribution:
p(x*|D) = ‚à´ p(x*|Œ∏)œÄ(Œ∏|D)dŒ∏

5.2 Conjugate Priors:
--------------------
Conjugate Prior: Posterior has same form as prior

Beta-Binomial:
- Prior: Œ∏ ~ Beta(Œ±, Œ≤)
- Likelihood: Binomial
- Posterior: Œ∏|D ~ Beta(Œ± + s, Œ≤ + f)

Dirichlet-Multinomial:
- Prior: Œ∏ ~ Dir(Œ±‚ÇÅ, ..., Œ±‚Çñ)
- Likelihood: Multinomial
- Posterior: Œ∏|D ~ Dir(Œ±‚ÇÅ + n‚ÇÅ, ..., Œ±‚Çñ + n‚Çñ)

Normal-Normal:
- Prior: Œº ~ N(Œº‚ÇÄ, œÉ‚ÇÄ¬≤)
- Likelihood: x ~ N(Œº, œÉ¬≤)
- Posterior: Œº|D ~ N(Œº‚Çô, œÉ‚Çô¬≤)

5.3 Bayesian Naive Bayes:
------------------------
Prior on Parameters:
- Class priors: p(œÄ‚Çñ) = Dir(Œ±‚ÇÅ, ..., Œ±‚Çñ)
- Feature parameters: p(Œ∏‚±º‚Çñ) = Beta(Œ±‚±º‚Çñ, Œ≤‚±º‚Çñ)

Posterior Predictive:
p(y = k|x, D) ‚àù pÃÇ(y = k) ‚àè‚±º pÃÇ(x‚±º|y = k)

where pÃÇ includes posterior uncertainty

5.4 Model Selection:
-------------------
Marginal Likelihood:
p(D|M) = ‚à´ p(D|Œ∏, M)œÄ(Œ∏|M)dŒ∏

Bayes Factor:
BF‚ÇÅ‚ÇÇ = p(D|M‚ÇÅ)/p(D|M‚ÇÇ)

Model Posterior:
p(M|D) ‚àù p(D|M)p(M)

Automatic Ockham's razor effect

5.5 Approximation Methods:
-------------------------
Laplace Approximation:
Approximate posterior with Gaussian at mode

Variational Inference:
Approximate posterior with simpler distribution
Minimize KL divergence

MCMC Sampling:
- Metropolis-Hastings
- Gibbs sampling
- Hamiltonian Monte Carlo

Sample from posterior distribution

=======================================================

6. MODEL SELECTION AND EVALUATION
=================================

6.1 Performance Metrics:
-----------------------
Classification Accuracy:
Acc = (1/n) ‚àë·µ¢ ùüô[y·µ¢ = ≈∑·µ¢]

Log-Likelihood:
LL = ‚àë·µ¢ log p(y·µ¢|x·µ¢)

Measures probability assigned to correct class

Cross-Entropy:
CE = -(1/n) ‚àë·µ¢ log p(y·µ¢|x·µ¢)

Brier Score:
BS = (1/n) ‚àë·µ¢ ‚àë‚Çñ (ùüô[y·µ¢ = k] - p(y = k|x·µ¢))¬≤

6.2 Calibration:
---------------
Well-calibrated classifier:
P(Y = 1|PÃÇ(Y = 1|X) = p) = p

Calibration Plot:
Plot empirical frequency vs predicted probability

Calibration Methods:
- Platt scaling: Sigmoid function
- Isotonic regression: Monotonic mapping

6.3 Cross-Validation:
--------------------
Stratified k-Fold:
Maintain class proportions in each fold

Leave-One-Out:
Special case where k = n
Expensive but unbiased

Repeated CV:
Multiple random partitions
More robust estimates

6.4 Information Criteria:
------------------------
Akaike Information Criterion:
AIC = -2‚Ñì + 2k

Bayesian Information Criterion:
BIC = -2‚Ñì + k log n

Deviance Information Criterion:
DIC = DÃÑ + pD

where DÃÑ is posterior mean deviance, pD is effective parameters

6.5 Validation Challenges:
-------------------------
Small Sample Sizes:
- High variance in estimates
- Nested CV for hyperparameters
- Bootstrap confidence intervals

Imbalanced Classes:
- Stratified sampling
- Appropriate metrics (F1, AUC)
- Class-specific evaluation

Temporal Data:
- Time-series splits
- No future information
- Concept drift considerations

=======================================================

7. EXTENSIONS AND VARIANTS
==========================

7.1 Semi-Naive Bayes:
--------------------
Relax independence assumption partially

Tree-Augmented Naive Bayes (TAN):
- Allow one additional parent per feature
- Learn tree structure
- Balance accuracy and complexity

Bayesian Network Augmented Naive Bayes (BAN):
- More complex dependency structures
- Structure learning algorithms

7.2 Kernel Density Estimation:
-----------------------------
Non-parametric density estimation:
pÃÇ(x|y = k) = (1/n‚Çñh) ‚àë·µ¢:y·µ¢=k K((x - x·µ¢)/h)

Kernel Functions:
- Gaussian: K(u) = exp(-u¬≤/2)/‚àö(2œÄ)
- Epanechnikov: K(u) = (3/4)(1 - u¬≤)‚Çä
- Uniform: K(u) = (1/2)ùüô[|u| ‚â§ 1]

Bandwidth Selection:
- Cross-validation
- Plug-in methods
- Adaptive bandwidths

7.3 Locally Weighted Naive Bayes:
--------------------------------
Weight training examples by distance to query:
pÃÇ(y = k|x) ‚àù ‚àë·µ¢ w·µ¢(x) ùüô[y·µ¢ = k]

Weights:
w·µ¢(x) = K((x - x·µ¢)/h)

Adapts to local data distribution

7.4 Ensemble Methods:
--------------------
Bagging Naive Bayes:
- Bootstrap samples
- Average predictions
- Reduces variance

Random Subspace Naive Bayes:
- Random feature subsets
- Combine predictions
- Handles high dimensions

Bayesian Model Averaging:
p(y|x, D) = ‚àë‚Çò p(y|x, M)p(M|D)

Weight by model posterior

7.5 Online Learning:
-------------------
Incremental Updates:
Update parameters with new data points

Sliding Window:
Maintain recent examples only
Handles concept drift

Forgetting Factor:
Exponentially weight recent examples
Œª-weighted updates

7.6 Multi-label Naive Bayes:
---------------------------
Multiple labels per instance:
p(y‚ÇÅ, ..., y‚Çó|x) = ‚àè‚±º p(y‚±º|x)

Assume label independence given features

Binary Relevance:
Train separate binary classifier per label

Classifier Chains:
Include previous label predictions as features

=======================================================

8. APPLICATIONS AND PRACTICAL CONSIDERATIONS
============================================

8.1 Text Classification:
-----------------------
Document Representation:
- Bag-of-words: Binary or count features
- TF-IDF weighting
- N-gram features

Multinomial Naive Bayes:
- Good for count data
- Laplace smoothing essential
- Feature selection important

Applications:
- Spam filtering
- Sentiment analysis
- Topic classification
- Language detection

8.2 Medical Diagnosis:
---------------------
Feature Types:
- Binary symptoms
- Continuous measurements
- Ordinal severity scales

Advantages:
- Interpretable probabilities
- Handles missing values
- Incorporates prior knowledge

Challenges:
- Feature dependencies
- Rare diseases (prior specification)
- Ethical considerations

8.3 Recommendation Systems:
--------------------------
Collaborative Filtering:
- User-item interactions as features
- Multinomial model for ratings
- Handle sparsity with smoothing

Content-based Filtering:
- Item features
- User preference modeling
- Cold start problem

8.4 Real-time Applications:
--------------------------
Advantages:
- Fast prediction (linear in features)
- Incremental learning
- Low memory requirements

Stream Processing:
- Update statistics incrementally
- Sliding window for concept drift
- Approximate updates for efficiency

8.5 Feature Engineering:
-----------------------
Discretization:
- Convert continuous to categorical
- Equal-width or equal-frequency bins
- Optimal binning algorithms

Feature Selection:
- Mutual information
- Chi-square test
- Correlation-based filters

Text Features:
- Stop word removal
- Stemming/lemmatization
- N-gram extraction
- Feature hashing

8.6 Implementation Considerations:
---------------------------------
Numerical Stability:
- Log-space computations
- Avoid underflow in products
- Stable parameter updates

Memory Efficiency:
- Sparse feature representations
- Incremental statistics
- Feature hashing

Parallelization:
- Independent feature processing
- Map-reduce implementations
- GPU acceleration for large vocabularies

8.7 Common Pitfalls:
-------------------
Independence Assumption:
- Violated in many real applications
- Feature engineering to reduce dependence
- Consider alternative models

Zero Probabilities:
- Always use smoothing
- Handle unseen feature values
- Laplace or Good-Turing smoothing

Feature Scaling:
- Not needed for categorical features
- Important for continuous features in GNB
- Standardization or normalization

Imbalanced Classes:
- Affects prior estimates
- Stratified sampling
- Cost-sensitive variants

8.8 Best Practices:
------------------
Data Preprocessing:
- Handle missing values appropriately
- Choose suitable smoothing parameters
- Consider feature transformations

Model Validation:
- Use appropriate cross-validation
- Check calibration of probabilities
- Validate independence assumptions

Feature Engineering:
- Domain knowledge crucial
- Feature selection important
- Consider interaction terms

Production Deployment:
- Monitor for concept drift
- Efficient prediction pipelines
- A/B testing for updates

Performance Optimization:
- Precompute log probabilities
- Use sparse data structures
- Batch processing for efficiency

Guidelines for Success:
- Start with strong baselines
- Validate modeling assumptions
- Use appropriate smoothing techniques
- Consider ensemble methods
- Monitor model performance continuously
- Document preprocessing steps
- Plan for incremental updates
- Balance interpretability with accuracy
- Understand domain-specific requirements
- Test edge cases thoroughly

=======================================================
END OF DOCUMENT 