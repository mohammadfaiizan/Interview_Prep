OPTIMIZATION ALGORITHMS - SGD, ADAM, AND LEARNING RATE SCHEDULES
================================================================

TABLE OF CONTENTS:
1. Optimization Fundamentals in Deep Learning
2. Stochastic Gradient Descent (SGD)
3. Momentum-Based Methods
4. Adaptive Learning Rate Methods
5. Advanced Optimization Algorithms
6. Learning Rate Schedules
7. Optimization in Practice
8. Implementation and Practical Considerations

=======================================================

1. OPTIMIZATION FUNDAMENTALS IN DEEP LEARNING
=============================================

1.1 The Optimization Problem:
----------------------------
Objective Function:
Minimize L(θ) = (1/N) Σᵢ₌₁ᴺ ℓ(f(xᵢ; θ), yᵢ)

where:
- θ: model parameters
- f(x; θ): neural network function
- ℓ: loss function
- N: number of training examples

Challenges in Deep Learning:
- Non-convex optimization landscape
- High-dimensional parameter space
- Saddle points and local minima
- Vanishing/exploding gradients
- Large-scale datasets

1.2 Gradient-Based Optimization:
-------------------------------
Gradient Descent:
θₜ₊₁ = θₜ - η ∇_θ L(θₜ)

where η is the learning rate

Gradient Computation:
∇_θ L(θ) = (1/N) Σᵢ₌₁ᴺ ∇_θ ℓ(f(xᵢ; θ), yᵢ)

Backpropagation:
Efficient computation of gradients
Chain rule applied recursively

1.3 Optimization Landscape:
--------------------------
Loss Surface Properties:
- Non-convex with many local minima
- Saddle points more common than minima
- Flat regions and sharp minima
- Different scales across parameters

Generalization vs Optimization:
Sharp minima → poor generalization
Flat minima → better generalization
Trade-off between optimization speed and generalization

1.4 Stochastic vs Batch Optimization:
------------------------------------
Full Batch:
Use entire dataset for gradient computation
Accurate gradients but computationally expensive

Mini-batch:
Use subset of data for gradient computation
Balance between accuracy and efficiency

Stochastic:
Use single sample for gradient computation
Fast but noisy gradient estimates

1.5 Key Optimization Principles:
-------------------------------
Learning Rate:
Controls step size in parameter space
Too large → divergence, too small → slow convergence

Momentum:
Accumulate gradients over time
Accelerate convergence and reduce oscillations

Adaptive Rates:
Different learning rates for different parameters
Account for parameter-specific scales

Regularization:
Prevent overfitting during optimization
Weight decay, dropout, etc.

=======================================================

2. STOCHASTIC GRADIENT DESCENT (SGD)
====================================

2.1 Basic SGD Algorithm:
-----------------------
Update Rule:
θₜ₊₁ = θₜ - η ∇_θ L(θₜ; xₜ, yₜ)

where (xₜ, yₜ) is a single training example

Mini-batch SGD:
θₜ₊₁ = θₜ - η (1/B) Σᵢ₌₁ᴮ ∇_θ ℓ(f(xᵢ; θₜ), yᵢ)

where B is the batch size

Algorithm:
```
for epoch in range(num_epochs):
    for batch in dataloader:
        gradients = compute_gradients(batch, model)
        parameters -= learning_rate * gradients
```

2.2 SGD Properties:
------------------
Convergence Analysis:
For convex functions: O(1/T) convergence rate
For strongly convex: O(exp(-T)) convergence rate

Variance of Gradient Estimates:
Var[∇_θ L_batch] = σ²/B
Larger batches → lower variance

Escape from Local Minima:
Noise in gradient estimates helps escape
Sharp minima more likely to be escaped

2.3 Batch Size Effects:
----------------------
Small Batches:
- High gradient variance
- Better generalization
- Can escape sharp minima
- More iterations needed

Large Batches:
- Low gradient variance
- Faster convergence per epoch
- May get stuck in sharp minima
- Require more memory

Optimal Batch Size:
Trade-off between convergence speed and generalization
Often empirically determined (32, 64, 128, 256)

2.4 Learning Rate Selection:
---------------------------
Too Large:
- Loss may oscillate or diverge
- Overshoot optimal parameters
- Unstable training

Too Small:
- Very slow convergence
- May get stuck in plateaus
- Inefficient training

Learning Rate Search:
- Grid search over powers of 10
- Learning rate finder (gradually increase)
- Cross-validation for selection

2.5 SGD Variants:
----------------
SGD with Momentum:
vₜ₊₁ = γvₜ + η ∇_θ L(θₜ)
θₜ₊₁ = θₜ - vₜ₊₁

Nesterov Accelerated Gradient:
vₜ₊₁ = γvₜ + η ∇_θ L(θₜ - γvₜ)
θₜ₊₁ = θₜ - vₜ₊₁

SGD with Weight Decay:
θₜ₊₁ = θₜ - η(∇_θ L(θₜ) + λθₜ)

2.6 Implementation:
------------------
PyTorch SGD:
```python
import torch.optim as optim

optimizer = optim.SGD(model.parameters(), 
                     lr=0.01, 
                     momentum=0.9, 
                     weight_decay=1e-4)

for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
```

=======================================================

3. MOMENTUM-BASED METHODS
=========================

3.1 Classical Momentum:
----------------------
Motivation:
Accelerate convergence in consistent directions
Dampen oscillations in inconsistent directions

Mathematical Formulation:
vₜ = γvₜ₋₁ + η ∇_θ L(θₜ₋₁)
θₜ = θₜ₋₁ - vₜ

where:
- v: velocity/momentum term
- γ: momentum coefficient (typically 0.9)
- η: learning rate

Physical Interpretation:
Ball rolling down hill with friction
Momentum helps overcome small bumps

3.2 Nesterov Accelerated Gradient (NAG):
---------------------------------------
Look-Ahead Gradient:
Evaluate gradient at anticipated future position
vₜ = γvₜ₋₁ + η ∇_θ L(θₜ₋₁ - γvₜ₋₁)
θₜ = θₜ₋₁ - vₜ

Alternative Formulation:
θ̃ₜ = θₜ₋₁ - γvₜ₋₁  (look-ahead position)
vₜ = γvₜ₋₁ + η ∇_θ L(θ̃ₜ)
θₜ = θₜ₋₁ - vₜ

Benefits:
- Better convergence properties
- More responsive to gradient changes
- Reduced oscillations

3.3 Momentum Analysis:
---------------------
Convergence Rate:
O(1/T²) for convex functions with NAG
Better than O(1/T) for standard SGD

Effective Learning Rate:
In steady state: effective LR ≈ η/(1-γ)
With γ=0.9: effective LR ≈ 10η

Momentum Coefficient Selection:
- γ = 0.9: common default
- γ = 0.99: for very smooth functions
- γ = 0.5-0.8: for noisy gradients

3.4 Heavy Ball Method:
---------------------
Different Parameterization:
θₜ = θₜ₋₁ - η ∇_θ L(θₜ₋₁) + γ(θₜ₋₁ - θₜ₋₂)

Connection to Classical Momentum:
Equivalent under certain conditions
Different implementation details

3.5 Implementation Details:
--------------------------
Momentum Buffer:
Maintain running average of gradients
Initialize to zero at start

```python
class SGDMomentum:
    def __init__(self, params, lr=0.01, momentum=0.9):
        self.params = params
        self.lr = lr
        self.momentum = momentum
        self.velocity = [torch.zeros_like(p) for p in params]
    
    def step(self):
        for i, param in enumerate(self.params):
            if param.grad is not None:
                self.velocity[i] = (self.momentum * self.velocity[i] + 
                                   self.lr * param.grad)
                param.data -= self.velocity[i]
```

=======================================================

4. ADAPTIVE LEARNING RATE METHODS
=================================

4.1 Adagrad:
-----------
Motivation:
Adapt learning rate per parameter
Larger updates for infrequent parameters

Algorithm:
Gₜ = Gₜ₋₁ + (∇_θ L(θₜ₋₁))²
θₜ = θₜ₋₁ - η/√(Gₜ + ε) ⊙ ∇_θ L(θₜ₋₁)

where:
- G: accumulated squared gradients
- ε: small constant for numerical stability (1e-8)
- ⊙: element-wise multiplication

Properties:
- Learning rate decreases over time
- Different rates for each parameter
- Good for sparse gradients

Problems:
- Learning rate eventually goes to zero
- Too aggressive reduction in late stages

4.2 RMSprop:
-----------
Modification of Adagrad:
Use exponential moving average instead of cumulative sum

Algorithm:
Gₜ = βGₜ₋₁ + (1-β)(∇_θ L(θₜ₋₁))²
θₜ = θₜ₋₁ - η/√(Gₜ + ε) ⊙ ∇_θ L(θₜ₋₁)

where β is decay parameter (typically 0.9)

Benefits:
- Learning rate doesn't monotonically decrease
- Better for non-stationary objectives
- More suitable for RNNs

4.3 Adam (Adaptive Moment Estimation):
-------------------------------------
Combines Momentum and RMSprop:
Maintains both first and second moment estimates

Algorithm:
mₜ = β₁mₜ₋₁ + (1-β₁)∇_θ L(θₜ₋₁)     (first moment)
vₜ = β₂vₜ₋₁ + (1-β₂)(∇_θ L(θₜ₋₁))²  (second moment)

Bias Correction:
m̂ₜ = mₜ/(1-β₁ᵗ)
v̂ₜ = vₜ/(1-β₂ᵗ)

Parameter Update:
θₜ = θₜ₋₁ - η m̂ₜ/(√v̂ₜ + ε)

Default Hyperparameters:
- β₁ = 0.9 (momentum decay)
- β₂ = 0.999 (squared gradient decay)
- ε = 1e-8
- η = 0.001

4.4 Adam Variants:
-----------------
AdamW (Adam with Weight Decay):
Proper weight decay implementation
θₜ = θₜ₋₁ - η(m̂ₜ/(√v̂ₜ + ε) + λθₜ₋₁)

AMSGrad:
Fix Adam's convergence issues
v̂ₜ = max(v̂ₜ₋₁, vₜ/(1-β₂ᵗ))

AdaBound:
Gradually transition from Adam to SGD
Bounded learning rates

Rectified Adam (RAdam):
Better bias correction in early training
Variance rectification term

4.5 Adaptive Method Analysis:
----------------------------
Convergence Properties:
Adam may not converge for some convex functions
RMSprop generally more reliable

Generalization:
SGD often generalizes better than Adam
Adam tends to find sharper minima

Hyperparameter Sensitivity:
Adam less sensitive to learning rate
But β parameters matter

When to Use:
- Adam: fast prototyping, good default
- SGD+Momentum: when generalization crucial
- RMSprop: RNNs and non-stationary problems

4.6 Implementation:
------------------
PyTorch Adam:
```python
optimizer = optim.Adam(model.parameters(), 
                      lr=0.001, 
                      betas=(0.9, 0.999), 
                      eps=1e-8, 
                      weight_decay=0)

# Training loop
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        loss = compute_loss(batch)
        loss.backward()
        optimizer.step()
```

Custom Adam Implementation:
```python
class Adam:
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8):
        self.params = params
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps
        self.t = 0
        self.m = [torch.zeros_like(p) for p in params]
        self.v = [torch.zeros_like(p) for p in params]
    
    def step(self):
        self.t += 1
        for i, param in enumerate(self.params):
            if param.grad is not None:
                # Update biased first moment estimate
                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * param.grad
                
                # Update biased second moment estimate
                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * param.grad.pow(2)
                
                # Bias correction
                m_hat = self.m[i] / (1 - self.beta1 ** self.t)
                v_hat = self.v[i] / (1 - self.beta2 ** self.t)
                
                # Update parameters
                param.data -= self.lr * m_hat / (v_hat.sqrt() + self.eps)
```

=======================================================

5. ADVANCED OPTIMIZATION ALGORITHMS
===================================

5.1 AdaGrad Variants:
--------------------
AdaDelta:
Eliminates need for learning rate
Uses running average of parameter updates

Algorithm:
E[g²]ₜ = ρE[g²]ₜ₋₁ + (1-ρ)gₜ²
Δθₜ = -√(E[Δθ²]ₜ₋₁ + ε)/√(E[g²]ₜ + ε) · gₜ
E[Δθ²]ₜ = ρE[Δθ²]ₜ₋₁ + (1-ρ)Δθₜ²
θₜ₊₁ = θₜ + Δθₜ

AdaMax:
Variant of Adam using infinity norm
vₜ = max(β₂vₜ₋₁, |gₜ|)
θₜ₊₁ = θₜ - (η/(1-β₁ᵗ)) · mₜ/vₜ

5.2 Second-Order Methods:
------------------------
Newton's Method:
θₜ₊₁ = θₜ - η H⁻¹ ∇_θ L(θₜ)
where H is the Hessian matrix

Problems:
- Computing Hessian is O(n²) in memory
- Inverting Hessian is O(n³) in computation
- Hessian may not be positive definite

Quasi-Newton Methods:
Approximate Hessian with rank-1 updates
BFGS, L-BFGS algorithms

K-FAC (Kronecker-Factored Approximate Curvature):
Approximate Fisher information matrix
More practical second-order method

5.3 Natural Gradients:
---------------------
Concept:
Account for parameter space geometry
Use Fisher Information Matrix as metric

Natural Gradient:
θₜ₊₁ = θₜ - η F⁻¹ ∇_θ L(θₜ)
where F is Fisher Information Matrix

Benefits:
- Invariant to model parameterization
- Better convergence properties
- Accounts for parameter interactions

5.4 Gradient-Free Methods:
-------------------------
Evolutionary Strategies:
Population-based optimization
Useful when gradients unavailable

Genetic Algorithms:
Mutation and selection operations
Good for discrete optimization

Particle Swarm Optimization:
Swarm intelligence approach
Parallel exploration of solution space

5.5 Meta-Learning Optimizers:
----------------------------
Learned Optimizers:
Neural networks that learn to optimize
L2L (Learning to Learn) frameworks

AutoML for Optimization:
Automatically search for best optimizer
Hyperparameter optimization

Progressive Learning:
Start with simple optimizer
Gradually increase complexity

=======================================================

6. LEARNING RATE SCHEDULES
==========================

6.1 Why Learning Rate Scheduling:
--------------------------------
Initial Phase:
Large learning rate for rapid progress
Explore parameter space efficiently

Later Phase:
Small learning rate for fine-tuning
Converge to better solutions

Annealing Benefits:
- Better final performance
- Escape from poor local minima
- Stable convergence

6.2 Step Decay:
--------------
Reduce learning rate at fixed intervals

Schedule:
η(t) = η₀ × γ^⌊t/s⌋

where:
- η₀: initial learning rate
- γ: decay factor (e.g., 0.1)
- s: step size (e.g., 30 epochs)

Implementation:
```python
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 
                                           step_size=30, 
                                           gamma=0.1)

for epoch in range(num_epochs):
    train_one_epoch()
    scheduler.step()
```

MultiStep Decay:
Decay at specific epochs
scheduler = MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)

6.3 Exponential Decay:
---------------------
Continuous exponential reduction

Formula:
η(t) = η₀ × e^(-λt)
or
η(t) = η₀ × γ^t

where λ or γ controls decay rate

Implementation:
```python
scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)
```

6.4 Cosine Annealing:
--------------------
Smooth decrease following cosine curve

Formula:
η(t) = η_min + (η_max - η_min) × (1 + cos(πt/T))/2

where T is total number of epochs

Benefits:
- Smooth transitions
- No sudden drops
- Natural convergence curve

Cosine Annealing with Restarts:
Periodic restarts to escape local minima
SGDR (Stochastic Gradient Descent with Warm Restarts)

6.5 Polynomial Decay:
--------------------
Polynomial decrease

Formula:
η(t) = η₀ × (1 - t/T)^p

where p is polynomial power (typically 0.5 or 1.0)

Linear Decay (p=1):
η(t) = η₀ × (1 - t/T)

6.6 Adaptive Schedules:
----------------------
ReduceLROnPlateau:
Reduce when metric stops improving

```python
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.1, patience=10)

for epoch in range(num_epochs):
    train_loss = train_one_epoch()
    val_loss = validate()
    scheduler.step(val_loss)
```

CyclicLR:
Cycle between minimum and maximum learning rates
Triangular, triangular2, or exp_range policies

One Cycle Policy:
Single cycle with warm-up and cool-down
Used in super-convergence

6.7 Warmup Strategies:
---------------------
Linear Warmup:
η(t) = η_max × t/t_warmup for t ≤ t_warmup

Cosine Warmup:
η(t) = η_max × (1 - cos(πt/t_warmup))/2

Benefits:
- Stable training start
- Better for large batch sizes
- Prevents early overfitting

6.8 Advanced Scheduling:
-----------------------
Learning Rate Finding:
Gradually increase LR and monitor loss
Find optimal range before training

Automatic Scheduling:
Monitor gradient norms or loss curves
Adjust learning rate automatically

Population-Based Training:
Multiple models with different schedules
Evolve best schedules over time

=======================================================

7. OPTIMIZATION IN PRACTICE
===========================

7.1 Optimizer Selection Guidelines:
----------------------------------
For Computer Vision:
- SGD + Momentum: Best generalization
- Learning rate: 0.01-0.1
- Momentum: 0.9

For NLP/Transformers:
- Adam/AdamW: Fast convergence
- Learning rate: 1e-4 to 5e-4
- Warmup + cosine decay

For RNNs:
- RMSprop: Handles non-stationarity
- Gradient clipping essential
- Learning rate: 1e-3 to 1e-2

For GANs:
- Adam: Standard choice
- Different LRs for G and D
- β₁ = 0.5 instead of 0.9

7.2 Hyperparameter Tuning:
--------------------------
Learning Rate:
Most important hyperparameter
Grid search: [0.1, 0.01, 0.001, 0.0001]
Learning rate finder for optimal range

Batch Size:
Start with 32-128
Larger batches may need higher LR
Consider memory constraints

Momentum/Beta Parameters:
β₁ = 0.9 (good default for most cases)
β₂ = 0.999 (for Adam)
Adjust based on gradient noise

Weight Decay:
Typical values: 1e-4, 1e-5, 1e-6
Stronger regularization for overfitting

7.3 Monitoring Training:
-----------------------
Loss Curves:
- Smoothly decreasing: good
- Oscillating: LR too high
- Plateauing: LR too low or local minimum

Gradient Norms:
Monitor ||∇L||₂ across training
Sudden spikes indicate instability

Learning Rate vs Loss:
Plot to find optimal LR range
Sweet spot before loss diverges

Parameter Updates:
Ratio of update to parameter magnitude
Should be around 1e-3

7.4 Common Optimization Problems:
--------------------------------
Exploding Gradients:
- Symptoms: Loss becomes NaN
- Solutions: Gradient clipping, lower LR

Vanishing Gradients:
- Symptoms: No learning in early layers
- Solutions: Better initialization, skip connections

Loss Plateaus:
- Symptoms: Training stagnates
- Solutions: LR scheduling, different optimizer

Poor Generalization:
- Symptoms: Train-val gap
- Solutions: Lower LR, regularization, different optimizer

7.5 Debugging Optimization:
--------------------------
Gradient Checking:
Numerical vs analytical gradients
Ensure backprop implementation correct

Overfit Single Batch:
Model should perfectly fit one batch
Tests optimization capability

Learning Rate Sensitivity:
Try range of learning rates
Understand sensitivity to hyperparameters

Parameter Initialization:
Poor initialization can hurt optimization
Use proven initialization schemes

7.6 Optimization Best Practices:
-------------------------------
Start Simple:
Begin with SGD + momentum
Add complexity only if needed

Monitor Everything:
Track loss, gradients, learning rates
Use visualization tools (TensorBoard)

Reproducibility:
Set random seeds
Save optimizer states

Checkpointing:
Save model and optimizer state
Resume training from checkpoints

Early Stopping:
Monitor validation metrics
Stop when no improvement

=======================================================

8. IMPLEMENTATION AND PRACTICAL CONSIDERATIONS
==============================================

8.1 PyTorch Optimization Framework:
----------------------------------
Basic Setup:
```python
import torch.optim as optim

# Define optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        # Zero gradients
        optimizer.zero_grad()
        
        # Forward pass
        output = model(data)
        loss = criterion(output, target)
        
        # Backward pass
        loss.backward()
        
        # Update parameters
        optimizer.step()
```

Optimizer State Management:
```python
# Save optimizer state
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}, 'checkpoint.pth')

# Load optimizer state
checkpoint = torch.load('checkpoint.pth')
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

8.2 Advanced Training Loops:
----------------------------
Mixed Precision Training:
```python
from torch.cuda.amp import GradScaler, autocast

scaler = GradScaler()

for data, target in train_loader:
    optimizer.zero_grad()
    
    with autocast():
        output = model(data)
        loss = criterion(output, target)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

Gradient Accumulation:
```python
accumulation_steps = 4

for i, (data, target) in enumerate(train_loader):
    output = model(data)
    loss = criterion(output, target) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

8.3 Custom Optimizers:
---------------------
Base Optimizer Class:
```python
class CustomOptimizer:
    def __init__(self, params, lr=0.01):
        self.params = list(params)
        self.lr = lr
        self.state = {}
    
    def zero_grad(self):
        for param in self.params:
            if param.grad is not None:
                param.grad.zero_()
    
    def step(self):
        for param in self.params:
            if param.grad is None:
                continue
            
            # Custom update rule
            param.data -= self.lr * param.grad
```

Implementing Adam:
```python
class AdamOptimizer:
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8):
        self.params = list(params)
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps
        self.step_count = 0
        
        # Initialize state
        self.m = [torch.zeros_like(p) for p in self.params]
        self.v = [torch.zeros_like(p) for p in self.params]
    
    def step(self):
        self.step_count += 1
        
        for i, param in enumerate(self.params):
            if param.grad is None:
                continue
            
            grad = param.grad
            
            # Update biased first moment estimate
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            
            # Update biased second moment estimate
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad.pow(2)
            
            # Bias correction
            m_hat = self.m[i] / (1 - self.beta1 ** self.step_count)
            v_hat = self.v[i] / (1 - self.beta2 ** self.step_count)
            
            # Update parameters
            param.data -= self.lr * m_hat / (v_hat.sqrt() + self.eps)
```

8.4 Learning Rate Scheduling Implementation:
-------------------------------------------
Custom LR Scheduler:
```python
class CustomScheduler:
    def __init__(self, optimizer, schedule_func):
        self.optimizer = optimizer
        self.schedule_func = schedule_func
        self.step_count = 0
    
    def step(self):
        self.step_count += 1
        new_lr = self.schedule_func(self.step_count)
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = new_lr

# Cosine annealing
def cosine_schedule(step, total_steps, lr_max, lr_min=0):
    return lr_min + (lr_max - lr_min) * 0.5 * (1 + math.cos(math.pi * step / total_steps))

scheduler = CustomScheduler(optimizer, 
                           lambda step: cosine_schedule(step, 1000, 0.01))
```

8.5 Optimization Monitoring:
---------------------------
Gradient Monitoring:
```python
def monitor_gradients(model):
    total_norm = 0
    param_count = 0
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
            param_count += 1
            
            print(f'{name}: {param_norm:.4f}')
    
    total_norm = total_norm ** (1. / 2)
    print(f'Total gradient norm: {total_norm:.4f}')
    
    return total_norm
```

Parameter Update Ratios:
```python
def compute_update_ratios(model, optimizer):
    ratios = {}
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            update = optimizer.param_groups[0]['lr'] * param.grad
            ratio = torch.norm(update) / torch.norm(param.data)
            ratios[name] = ratio.item()
    
    return ratios
```

8.6 Distributed Optimization:
-----------------------------
Data Parallel Training:
```python
# DistributedDataParallel
from torch.nn.parallel import DistributedDataParallel as DDP

model = DDP(model, device_ids=[local_rank])
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Scale learning rate with world size
lr = base_lr * world_size
optimizer = optim.Adam(model.parameters(), lr=lr)
```

Gradient Synchronization:
```python
# All-reduce gradients across processes
def sync_gradients(model):
    for param in model.parameters():
        if param.grad is not None:
            torch.distributed.all_reduce(param.grad.data, op=torch.distributed.ReduceOp.SUM)
            param.grad.data /= torch.distributed.get_world_size()
```

8.7 Memory-Efficient Optimization:
---------------------------------
Gradient Checkpointing:
Trade computation for memory
Recompute activations during backward pass

```python
# PyTorch gradient checkpointing
import torch.utils.checkpoint as checkpoint

def forward_with_checkpointing(model, x):
    return checkpoint.checkpoint(model, x)
```

Optimizer Memory:
```python
# Zero optimizer (DeepSpeed)
# Partition optimizer states across devices
# Reduce memory per device

# Example pseudocode
def zero_optimizer_step():
    # Gather parameters for this rank
    gather_parameters()
    
    # Compute gradients
    compute_gradients()
    
    # Update parameters
    update_parameters()
    
    # Broadcast updated parameters
    broadcast_parameters()
```

8.8 Best Practices Summary:
--------------------------
Optimizer Selection:
1. Start with proven defaults (Adam/SGD+momentum)
2. Consider task-specific recommendations
3. Test multiple optimizers if needed
4. Monitor convergence and generalization

Hyperparameter Tuning:
1. Learning rate most important
2. Use learning rate finder
3. Adjust batch size with LR
4. Monitor gradient norms

Training Monitoring:
1. Track multiple metrics
2. Visualize training curves
3. Check gradient flow
4. Monitor memory usage

Debugging:
1. Overfit single batch first
2. Check gradient computation
3. Verify data pipeline
4. Test different initializations

Production Deployment:
1. Save complete training state
2. Version control hyperparameters
3. Monitor training stability
4. Plan for distributed scaling

Success Guidelines:
1. Understand optimization theory
2. Use appropriate optimizer for task
3. Tune hyperparameters systematically
4. Monitor training dynamics
5. Debug optimization issues
6. Consider computational constraints
7. Plan for scalability
8. Document optimization choices

=======================================================
END OF DOCUMENT 