DISTRIBUTED TRAINING AND PARALLELISM - Scaling Deep Learning
===========================================================

TABLE OF CONTENTS:
1. Distributed Training Fundamentals
2. Data Parallelism
3. Model Parallelism
4. Pipeline Parallelism
5. Distributed Training Frameworks
6. Gradient Synchronization Strategies
7. Advanced Parallelization Techniques
8. Implementation and Practical Guidelines

=======================================================

1. DISTRIBUTED TRAINING FUNDAMENTALS
====================================

1.1 Motivation for Distributed Training:
----------------------------------------
Scale Limitations:
Single GPU memory constraints
Large models don't fit on single device

Training Speed:
Accelerate training with multiple devices
Reduce wall-clock training time

Dataset Size:
Handle massive datasets efficiently
Process more data in parallel

Model Complexity:
Train larger, more complex models
Achieve better performance

1.2 Types of Parallelism:
------------------------
Data Parallelism:
Same model on multiple devices
Different data batches per device

Model Parallelism:
Split model across devices
Each device computes part of model

Pipeline Parallelism:
Split model into stages
Pipeline execution across stages

Hybrid Parallelism:
Combine multiple parallelism types
Complex but efficient strategies

1.3 Communication Patterns:
--------------------------
All-Reduce:
Aggregate gradients across all devices
Most common pattern

All-Gather:
Collect data from all devices
Useful for parameter synchronization

Broadcast:
Send data from one device to all
Parameter distribution

Point-to-Point:
Direct communication between devices
Used in pipeline parallelism

1.4 Challenges in Distributed Training:
--------------------------------------
Communication Overhead:
Network bandwidth limitations
Synchronization costs

Load Balancing:
Uneven work distribution
Stragglers slow down training

Memory Management:
Efficient memory usage across devices
Avoid memory bottlenecks

Fault Tolerance:
Handle device failures gracefully
Recover from errors

Reproducibility:
Ensure consistent results
Handle randomness across devices

1.5 Performance Metrics:
-----------------------
Scaling Efficiency:
Speedup = T_1 / T_N
Efficiency = Speedup / N

where T_1 is single device time, T_N is N-device time

Communication-to-Computation Ratio:
Ratio of communication time to computation time
Lower ratios indicate better efficiency

Throughput:
Samples processed per second
Important for production systems

Memory Utilization:
Effective use of available memory
Minimize memory waste

=======================================================

2. DATA PARALLELISM
==================

2.1 Data Parallelism Concept:
----------------------------
Replicated Model:
Same model copy on each device
Each device processes different data

Gradient Aggregation:
Collect gradients from all devices
Average gradients before update

Synchronous vs Asynchronous:
- Synchronous: Wait for all devices
- Asynchronous: Don't wait for stragglers

Mathematical Framework:
θ_t+1 = θ_t - η * (1/N) * Σᵢ ∇Lᵢ(θ_t)

where N is number of devices

2.2 Synchronous Data Parallelism:
--------------------------------
All-Reduce Operation:
1. Each device computes local gradients
2. All-reduce aggregates gradients
3. All devices update with same gradients

Advantages:
- Deterministic training
- Equivalent to large batch training
- Good convergence properties

Disadvantages:
- Slowest device determines speed
- Communication overhead
- Requires synchronization

Implementation:
```python
# PyTorch DistributedDataParallel
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_ddp(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    
def cleanup_ddp():
    dist.destroy_process_group()

# Wrap model with DDP
model = DDP(model, device_ids=[local_rank])
```

2.3 Asynchronous Data Parallelism:
---------------------------------
Parameter Server Architecture:
Central parameter server
Workers send gradients asynchronously

Stale Gradients:
Workers may use outdated parameters
Can lead to convergence issues

Bounded Staleness:
Limit how stale gradients can be
Balance between sync and async

Advantages:
- No synchronization overhead
- Fault tolerant
- Good for heterogeneous systems

Disadvantages:
- Convergence issues
- Complex implementation
- Non-deterministic training

2.4 Large Batch Training:
------------------------
Linear Scaling Rule:
Scale learning rate with batch size
lr = base_lr * (batch_size / base_batch_size)

Warmup Strategy:
Gradually increase learning rate
Stabilize large batch training

Layer-wise Adaptive Rate Scaling (LARS):
Adapt learning rate per layer
lr_layer = lr * ||W|| / ||∇W||

Batch Size Considerations:
Larger batches → better hardware utilization
But may hurt generalization

2.5 Gradient Compression:
------------------------
Motivation:
Reduce communication overhead
Compress gradients before transmission

Quantization:
Reduce gradient precision
Trade accuracy for communication speed

Sparsification:
Send only significant gradients
Top-k or threshold-based selection

Error Feedback:
Accumulate compression errors
Add to next gradient computation

Implementation:
```python
class GradientCompression:
    def __init__(self, compression_ratio=0.1):
        self.compression_ratio = compression_ratio
        self.error_feedback = {}
    
    def compress_gradients(self, gradients):
        compressed_grads = {}
        
        for name, grad in gradients.items():
            # Add error feedback
            if name in self.error_feedback:
                grad += self.error_feedback[name]
            
            # Top-k compression
            flat_grad = grad.flatten()
            k = int(len(flat_grad) * self.compression_ratio)
            
            _, indices = torch.topk(torch.abs(flat_grad), k)
            compressed = torch.zeros_like(flat_grad)
            compressed[indices] = flat_grad[indices]
            
            # Store error for feedback
            self.error_feedback[name] = flat_grad - compressed
            
            compressed_grads[name] = compressed.reshape(grad.shape)
        
        return compressed_grads
```

2.6 Data Loading for Distributed Training:
------------------------------------------
Distributed Sampler:
Ensure each device gets different data
Avoid data overlap between devices

```python
from torch.utils.data.distributed import DistributedSampler

# Create distributed sampler
sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)

# Create data loader
dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)
```

Data Sharding:
Partition dataset across devices
Each device loads its partition

Prefetching:
Overlap data loading with computation
Use multiple worker processes

=======================================================

3. MODEL PARALLELISM
====================

3.1 Model Parallelism Concept:
------------------------------
Model Splitting:
Split model across multiple devices
Each device computes part of model

When Needed:
Model too large for single device
Memory constraints require splitting

Types:
- Tensor Parallelism: Split tensors
- Layer Parallelism: Split layers
- Expert Parallelism: Split experts (MoE)

Communication Requirements:
Activations passed between devices
Forward and backward passes

3.2 Tensor Parallelism:
----------------------
Row-wise Parallelism:
Split weight matrix by rows
Y = X * W → Y₁ = X * W₁, Y₂ = X * W₂

Column-wise Parallelism:
Split weight matrix by columns
Y = X * W → Y = [X * W₁ | X * W₂]

Implementation:
```python
class ColumnParallelLinear(nn.Module):
    def __init__(self, input_size, output_size, world_size, rank):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.world_size = world_size
        self.rank = rank
        
        # Split output dimension
        assert output_size % world_size == 0
        self.output_size_per_partition = output_size // world_size
        
        self.weight = nn.Parameter(torch.randn(
            input_size, self.output_size_per_partition
        ))
        
    def forward(self, input):
        # Compute local output
        output_parallel = torch.matmul(input, self.weight)
        
        # All-gather to get full output
        output_list = [torch.zeros_like(output_parallel) for _ in range(self.world_size)]
        dist.all_gather(output_list, output_parallel)
        output = torch.cat(output_list, dim=-1)
        
        return output
```

3.3 Layer-wise Model Parallelism:
--------------------------------
Sequential Splitting:
Split model into sequential stages
Each device handles consecutive layers

Memory Efficiency:
Each device only stores its layers
Reduced memory per device

Pipeline Bubbles:
Devices idle during sequential execution
Inefficient resource utilization

Activation Checkpointing:
Store activations at stage boundaries
Recompute internal activations

3.4 Mixture of Experts (MoE):
----------------------------
Expert Parallelism:
Different experts on different devices
Route tokens to appropriate experts

Sparse Activation:
Only subset of experts activated
Efficient scaling of model capacity

Load Balancing:
Ensure even expert utilization
Avoid expert collapse

Routing Algorithm:
Learn which expert to use
Top-k expert selection

Implementation:
```python
class MixtureOfExperts(nn.Module):
    def __init__(self, input_dim, expert_dim, num_experts, top_k=2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        
        # Router network
        self.router = nn.Linear(input_dim, num_experts)
        
        # Expert networks
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, expert_dim),
                nn.ReLU(),
                nn.Linear(expert_dim, input_dim)
            ) for _ in range(num_experts)
        ])
    
    def forward(self, x):
        # Compute routing probabilities
        router_logits = self.router(x)
        routing_weights = F.softmax(router_logits, dim=-1)
        
        # Select top-k experts
        top_k_weights, top_k_indices = torch.topk(routing_weights, self.top_k, dim=-1)
        top_k_weights = F.softmax(top_k_weights, dim=-1)
        
        # Compute expert outputs
        outputs = torch.zeros_like(x)
        for i in range(self.top_k):
            expert_idx = top_k_indices[:, i]
            expert_weight = top_k_weights[:, i].unsqueeze(-1)
            
            for batch_idx, exp_idx in enumerate(expert_idx):
                expert_output = self.experts[exp_idx](x[batch_idx:batch_idx+1])
                outputs[batch_idx] += expert_weight[batch_idx] * expert_output.squeeze(0)
        
        return outputs
```

3.5 Memory Optimization:
-----------------------
Activation Recomputation:
Trade computation for memory
Recompute activations during backward pass

Memory Pool:
Shared memory pool across devices
Efficient memory allocation

Gradient Offloading:
Store gradients on CPU
Overlap with computation

Mixed Precision:
Use FP16 for memory efficiency
Maintain FP32 for numerical stability

=======================================================

4. PIPELINE PARALLELISM
=======================

4.1 Pipeline Parallelism Concept:
--------------------------------
Stage-wise Execution:
Split model into pipeline stages
Each stage on different device

Micro-batching:
Split batch into micro-batches
Pipeline micro-batches through stages

Overlapped Execution:
Multiple micro-batches in pipeline
Improved device utilization

Pipeline Bubble:
Idle time at beginning and end
Efficiency depends on pipeline depth

4.2 Pipeline Scheduling:
-----------------------
GPipe Scheduling:
Sequential forward, then sequential backward
Simple but has large bubbles

PipeDream Scheduling:
Interleave forward and backward
Reduced pipeline bubbles

1F1B (One Forward One Backward):
Alternate forward and backward passes
Memory efficient

Implementation:
```python
class PipelineParallel(nn.Module):
    def __init__(self, stages, devices):
        super().__init__()
        self.stages = nn.ModuleList(stages)
        self.devices = devices
        self.num_stages = len(stages)
        
        # Move stages to appropriate devices
        for i, stage in enumerate(self.stages):
            stage.to(self.devices[i])
    
    def forward(self, x, micro_batch_size):
        batch_size = x.size(0)
        num_micro_batches = batch_size // micro_batch_size
        
        # Split into micro-batches
        micro_batches = torch.chunk(x, num_micro_batches, dim=0)
        
        # Pipeline execution
        outputs = []
        stage_inputs = [None] * self.num_stages
        
        for step in range(num_micro_batches + self.num_stages - 1):
            # Forward pass
            for stage_id in range(self.num_stages):
                micro_batch_id = step - stage_id
                
                if 0 <= micro_batch_id < num_micro_batches:
                    if stage_id == 0:
                        # First stage gets input
                        stage_input = micro_batches[micro_batch_id].to(self.devices[stage_id])
                    else:
                        # Later stages get from previous stage
                        stage_input = stage_inputs[stage_id]
                    
                    # Compute stage output
                    with torch.cuda.device(self.devices[stage_id]):
                        stage_output = self.stages[stage_id](stage_input)
                    
                    if stage_id == self.num_stages - 1:
                        # Last stage produces final output
                        outputs.append(stage_output.cpu())
                    else:
                        # Pass to next stage
                        stage_inputs[stage_id + 1] = stage_output.to(self.devices[stage_id + 1])
        
        return torch.cat(outputs, dim=0)
```

4.3 Memory Management in Pipelines:
----------------------------------
Activation Stashing:
Store activations needed for backward pass
Balance memory and recomputation

Gradient Accumulation:
Accumulate gradients across micro-batches
Apply updates after full batch

Memory Pool:
Reuse memory across pipeline stages
Avoid fragmentation

4.4 Pipeline Optimization:
-------------------------
Stage Balancing:
Balance computation across stages
Minimize pipeline bubbles

Micro-batch Sizing:
Trade-off between memory and efficiency
Smaller micro-batches → less memory

Asynchronous Communication:
Overlap communication with computation
Hide communication latency

Dynamic Scheduling:
Adapt scheduling based on stage timing
Handle heterogeneous devices

=======================================================

5. DISTRIBUTED TRAINING FRAMEWORKS
==================================

5.1 PyTorch Distributed:
-----------------------
DistributedDataParallel (DDP):
Built-in data parallelism
Efficient gradient synchronization

Process Groups:
Organize processes for communication
Support multiple backends (NCCL, MPI, Gloo)

Setup:
```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # Initialize process group
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def main(rank, world_size):
    setup(rank, world_size)
    
    # Create model and move to GPU
    model = MyModel().to(rank)
    model = DDP(model, device_ids=[rank])
    
    # Training loop
    for data, targets in dataloader:
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
    
    dist.destroy_process_group()
```

5.2 Horovod:
-----------
Unified Framework:
Supports TensorFlow, PyTorch, MXNet
Consistent API across frameworks

Ring All-Reduce:
Efficient gradient aggregation
Better than parameter server

Automatic Mixed Precision:
Built-in support for FP16 training
Simplified mixed precision

Implementation:
```python
import horovod.torch as hvd

# Initialize Horovod
hvd.init()

# Pin GPU
torch.cuda.set_device(hvd.local_rank())

# Scale learning rate
optimizer = torch.optim.SGD(model.parameters(), lr=0.01 * hvd.size())

# Wrap optimizer
optimizer = hvd.DistributedOptimizer(optimizer)

# Broadcast parameters
hvd.broadcast_parameters(model.state_dict(), root_rank=0)
hvd.broadcast_optimizer_state(optimizer, root_rank=0)
```

5.3 DeepSpeed:
-------------
ZeRO (Zero Redundancy Optimizer):
Partition optimizer states, gradients, parameters
Dramatically reduce memory usage

ZeRO Stages:
- ZeRO-1: Partition optimizer states
- ZeRO-2: Partition gradients
- ZeRO-3: Partition parameters

3D Parallelism:
Combine data, model, and pipeline parallelism
Handle very large models

Configuration:
```json
{
  "train_batch_size": 256,
  "train_micro_batch_size_per_gpu": 32,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 1e-4
    }
  },
  "zero_optimization": {
    "stage": 2,
    "allgather_partitions": true,
    "allgather_bucket_size": 5e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 5e8,
    "contiguous_gradients": true
  },
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  }
}
```

5.4 FairScale:
-------------
FSDP (Fully Sharded Data Parallel):
PyTorch implementation of ZeRO
Shard parameters, gradients, optimizer states

Activation Checkpointing:
Trade computation for memory
Automatic checkpoint selection

Pipeline Parallelism:
Built-in pipeline parallel support
Efficient scheduling algorithms

5.5 Mesh TensorFlow:
-------------------
Tensor Layout:
Specify how tensors are distributed
Flexible sharding strategies

Graph Compilation:
Compile computation graphs
Optimize communication patterns

Multi-dimensional Parallelism:
Support complex parallelism strategies
Batch, model, expert parallelism

=======================================================

6. GRADIENT SYNCHRONIZATION STRATEGIES
======================================

6.1 All-Reduce Algorithms:
--------------------------
Ring All-Reduce:
Arrange devices in ring topology
Pass gradients around ring

Bandwidth Optimal:
O(N) communication complexity
Each device sends/receives same amount

Latency Considerations:
O(N) latency due to ring structure
Not optimal for small messages

Tree All-Reduce:
Arrange devices in tree structure
Logarithmic depth for aggregation

Implementation:
```python
def ring_all_reduce(tensor, world_size, rank):
    """Ring all-reduce implementation"""
    # Split tensor into chunks
    chunk_size = tensor.numel() // world_size
    chunks = tensor.chunk(world_size)
    
    # Reduce-scatter phase
    for i in range(world_size - 1):
        send_chunk_idx = (rank - i) % world_size
        recv_chunk_idx = (rank - i - 1) % world_size
        
        # Send to next rank, receive from previous rank
        send_rank = (rank + 1) % world_size
        recv_rank = (rank - 1) % world_size
        
        # Simulate send/receive
        chunks[recv_chunk_idx] += received_chunk
    
    # All-gather phase
    for i in range(world_size - 1):
        send_chunk_idx = (rank - i + 1) % world_size
        recv_chunk_idx = (rank - i) % world_size
        
        send_rank = (rank + 1) % world_size
        recv_rank = (rank - 1) % world_size
        
        # Simulate send/receive
        chunks[recv_chunk_idx] = received_chunk
    
    return torch.cat(chunks)
```

6.2 Hierarchical All-Reduce:
----------------------------
Two-level Hierarchy:
Intra-node and inter-node communication
Leverage fast intra-node connections

NCCL Implementation:
NVIDIA's optimized communication library
Automatically selects best algorithm

Topology Awareness:
Consider network topology
Optimize communication patterns

6.3 Asynchronous Gradient Updates:
---------------------------------
Local SGD:
Perform multiple local updates
Periodic parameter synchronization

Elastic Averaging SGD:
Exponential moving average of parameters
Soft synchronization between workers

FedAvg (Federated Averaging):
Weighted average of local models
Used in federated learning

Implementation:
```python
class LocalSGD:
    def __init__(self, model, optimizer, sync_period=10):
        self.model = model
        self.optimizer = optimizer
        self.sync_period = sync_period
        self.step_count = 0
    
    def step(self):
        self.optimizer.step()
        self.step_count += 1
        
        # Synchronize periodically
        if self.step_count % self.sync_period == 0:
            self.synchronize_parameters()
    
    def synchronize_parameters(self):
        for param in self.model.parameters():
            dist.all_reduce(param.data, op=dist.ReduceOp.SUM)
            param.data /= dist.get_world_size()
```

6.4 Gradient Compression:
------------------------
Quantization:
Reduce gradient precision
1-bit SGD, TernGrad

Sparsification:
Send only large gradients
Top-k, random-k selection

Error Feedback:
Accumulate quantization errors
Add to next gradient

Sketching:
Compress using sketching techniques
Count-sketch, random projection

6.5 Communication Scheduling:
----------------------------
Gradient Bucketing:
Group gradients by size
Efficient all-reduce operations

Overlapping Communication:
Overlap with backward pass
Hide communication latency

Priority-based Scheduling:
Prioritize important gradients
Reduce critical path length

=======================================================

7. ADVANCED PARALLELIZATION TECHNIQUES
======================================

7.1 3D Parallelism:
------------------
Tensor + Pipeline + Data:
Combine all three parallelism types
Handle very large models efficiently

Megatron-LM:
Large language model training
Tensor and pipeline parallelism

Partitioning Strategy:
Decide how to split across dimensions
Consider communication costs

Optimization:
Minimize communication overhead
Balance load across devices

7.2 Expert Parallelism:
----------------------
Switch Transformer:
Route tokens to different experts
Scale model capacity efficiently

Load Balancing:
Ensure even expert utilization
Auxiliary loss for balancing

Communication Optimization:
All-to-all communication pattern
Optimize token routing

GShard:
Google's scaling approach
Expert parallelism for large models

7.3 Sequence Parallelism:
------------------------
Split Long Sequences:
Distribute sequence across devices
Useful for very long contexts

Attention Parallelism:
Parallelize attention computation
Split along sequence dimension

Communication Pattern:
All-gather and reduce-scatter
Synchronize attention outputs

7.4 Heterogeneous Training:
--------------------------
Mixed Device Types:
CPUs, GPUs, TPUs together
Optimize for each device type

Dynamic Load Balancing:
Adapt to device capabilities
Handle performance differences

Resource Allocation:
Assign work based on capacity
Consider memory and compute

7.5 Fault Tolerance:
-------------------
Checkpointing:
Regular model state saves
Resume from last checkpoint

Redundant Computation:
Replicate critical computations
Handle device failures

Dynamic Reconfiguration:
Adapt to failed devices
Continue training with remaining devices

Implementation:
```python
class FaultTolerantTrainer:
    def __init__(self, model, checkpoint_dir, save_frequency=1000):
        self.model = model
        self.checkpoint_dir = checkpoint_dir
        self.save_frequency = save_frequency
        self.step_count = 0
    
    def train_step(self, data, target):
        try:
            # Normal training step
            loss = self.forward_backward(data, target)
            self.step_count += 1
            
            # Periodic checkpointing
            if self.step_count % self.save_frequency == 0:
                self.save_checkpoint()
            
            return loss
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                # Handle OOM gracefully
                self.handle_oom()
            else:
                # Other errors
                self.handle_error(e)
    
    def save_checkpoint(self):
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'step_count': self.step_count
        }
        torch.save(checkpoint, f"{self.checkpoint_dir}/checkpoint_{self.step_count}.pt")
```

=======================================================

8. IMPLEMENTATION AND PRACTICAL GUIDELINES
==========================================

8.1 Environment Setup:
---------------------
Multi-GPU Single Node:
```bash
# Using torchrun (recommended)
torchrun --nproc_per_node=4 train_script.py

# Using python -m torch.distributed.launch (deprecated)
python -m torch.distributed.launch --nproc_per_node=4 train_script.py
```

Multi-Node Setup:
```bash
# Node 0 (master)
torchrun --nnodes=2 --node_rank=0 --master_addr="192.168.1.1" \
         --master_port=12355 --nproc_per_node=4 train_script.py

# Node 1
torchrun --nnodes=2 --node_rank=1 --master_addr="192.168.1.1" \
         --master_port=12355 --nproc_per_node=4 train_script.py
```

SLURM Integration:
```bash
#!/bin/bash
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --gres=gpu:4

srun torchrun --nnodes=$SLURM_NNODES --node_rank=$SLURM_NODEID \
              --master_addr=$(hostname) --master_port=12355 \
              --nproc_per_node=4 train_script.py
```

8.2 Code Structure:
------------------
Distributed Training Template:
```python
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

def train(rank, world_size):
    setup(rank, world_size)
    
    # Create model and move to GPU
    model = MyModel().to(rank)
    model = DDP(model, device_ids=[rank])
    
    # Create distributed sampler
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)
    
    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # Training loop
    for epoch in range(num_epochs):
        sampler.set_epoch(epoch)  # Important for shuffling
        
        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.to(rank), target.to(rank)
            
            optimizer.zero_grad()
            output = model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()
    
    cleanup()

if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size)
```

8.3 Performance Optimization:
----------------------------
Communication Optimization:
```python
# Bucket gradients for efficient all-reduce
model = DDP(model, device_ids=[rank], bucket_cap_mb=25)

# Find unused parameters
model = DDP(model, device_ids=[rank], find_unused_parameters=True)

# Use NCCL backend for GPU communication
dist.init_process_group("nccl", rank=rank, world_size=world_size)
```

Memory Optimization:
```python
# Gradient checkpointing
from torch.utils.checkpoint import checkpoint

def forward_with_checkpointing(self, x):
    return checkpoint(self.layer, x)

# Mixed precision training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    output = model(data)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

8.4 Monitoring and Debugging:
-----------------------------
Performance Monitoring:
```python
class PerformanceMonitor:
    def __init__(self):
        self.step_times = []
        self.communication_times = []
    
    def start_step(self):
        self.step_start = time.time()
    
    def end_step(self):
        step_time = time.time() - self.step_start
        self.step_times.append(step_time)
    
    def start_communication(self):
        self.comm_start = time.time()
    
    def end_communication(self):
        comm_time = time.time() - self.comm_start
        self.communication_times.append(comm_time)
    
    def get_stats(self):
        avg_step_time = np.mean(self.step_times)
        avg_comm_time = np.mean(self.communication_times)
        comm_ratio = avg_comm_time / avg_step_time
        
        return {
            'avg_step_time': avg_step_time,
            'avg_comm_time': avg_comm_time,
            'comm_ratio': comm_ratio
        }
```

Debugging Tools:
```python
# Check gradient synchronization
def check_grad_sync(model):
    for name, param in model.named_parameters():
        if param.grad is not None:
            # All-reduce gradient
            dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
            param.grad.data /= dist.get_world_size()
            
            # Check if gradients are the same across devices
            grad_norm = torch.norm(param.grad.data)
            print(f"Rank {dist.get_rank()}, {name}: grad_norm = {grad_norm}")

# Memory usage monitoring
def print_memory_usage():
    if torch.cuda.is_available():
        print(f"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
        print(f"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
```

8.5 Best Practices:
------------------
Scaling Guidelines:
1. Start with single GPU, verify correctness
2. Scale to multi-GPU on single node
3. Scale to multi-node when needed
4. Monitor efficiency at each step

Communication Optimization:
1. Use appropriate backends (NCCL for GPUs)
2. Minimize communication frequency
3. Overlap communication with computation
4. Use gradient compression when beneficial

Memory Management:
1. Use mixed precision training
2. Apply gradient checkpointing
3. Optimize batch sizes
4. Monitor memory usage

Debugging Strategy:
1. Test on small scale first
2. Check gradient synchronization
3. Monitor performance metrics
4. Use profiling tools

8.6 Common Issues and Solutions:
-------------------------------
Hanging Processes:
- Check network connectivity
- Verify process group initialization
- Use timeout in dist.init_process_group

Out of Memory:
- Reduce batch size
- Use gradient checkpointing
- Apply model parallelism

Poor Scaling:
- Check communication overhead
- Optimize data loading
- Balance computation

Convergence Issues:
- Verify gradient synchronization
- Check learning rate scaling
- Monitor loss curves

8.7 Production Deployment:
-------------------------
Container Orchestration:
```yaml
# Kubernetes deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: distributed-training
spec:
  replicas: 4
  selector:
    matchLabels:
      app: training
  template:
    metadata:
      labels:
        app: training
    spec:
      containers:
      - name: trainer
        image: my-training-image
        resources:
          limits:
            nvidia.com/gpu: 1
        env:
        - name: WORLD_SIZE
          value: "4"
        - name: RANK
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
```

Auto-scaling:
Monitor training metrics
Scale resources based on demand
Handle dynamic resource allocation

Fault Recovery:
Implement checkpointing
Graceful handling of failures
Automatic restart mechanisms

8.8 Success Guidelines:
----------------------
Planning:
1. Understand model and data requirements
2. Choose appropriate parallelism strategy
3. Estimate communication costs
4. Plan for fault tolerance

Implementation:
1. Start simple and scale gradually
2. Use established frameworks
3. Monitor performance continuously
4. Test thoroughly before production

Optimization:
1. Profile communication patterns
2. Optimize data loading pipeline
3. Balance memory and computation
4. Consider hardware topology

Deployment:
1. Plan for production scaling
2. Implement monitoring systems
3. Handle fault recovery
4. Document configuration choices

=======================================================
END OF DOCUMENT 