TRANSFER LEARNING AND FINE-TUNING - Pre-trained Models and Domain Adaptation
===========================================================================

TABLE OF CONTENTS:
1. Transfer Learning Fundamentals
2. Pre-trained Model Architectures
3. Fine-tuning Strategies
4. Domain Adaptation Techniques
5. Feature Extraction vs Fine-tuning
6. Multi-task and Meta-learning
7. Advanced Transfer Learning
8. Implementation and Practical Guidelines

=======================================================

1. TRANSFER LEARNING FUNDAMENTALS
=================================

1.1 Transfer Learning Motivation:
--------------------------------
Data Efficiency:
Leverage knowledge from large datasets
Reduce training time and data requirements

Computational Efficiency:
Avoid training from scratch
Start with meaningful representations

Performance Improvement:
Pre-trained features often better than random
Especially beneficial for small datasets

Practical Benefits:
- Faster convergence
- Better final performance
- Reduced computational costs
- Lower data requirements

1.2 Transfer Learning Taxonomy:
------------------------------
Domain Transfer:
Source domain ≠ Target domain
Example: ImageNet → Medical Images

Task Transfer:
Source task ≠ Target task
Example: Classification → Detection

Instance Transfer:
Different data distributions
Example: Different populations

Feature Representation Transfer:
Share learned representations
Most common in deep learning

1.3 When Transfer Learning Works:
--------------------------------
Related Domains:
Source and target domains should be related
Visual similarity helps for vision tasks

Hierarchical Features:
Low-level features often transferable
High-level features more task-specific

Sufficient Source Data:
Source domain should have abundant data
Better pre-trained representations

Quality of Pre-training:
Well-trained source models transfer better
Consider pre-training dataset size and diversity

1.4 Transfer Learning Scenarios:
-------------------------------
High Similarity, Large Target Dataset:
Fine-tune entire network
Adjust learning rates carefully

High Similarity, Small Target Dataset:
Freeze early layers, fine-tune later layers
Prevent overfitting

Low Similarity, Large Target Dataset:
Fine-tune more layers
May train longer

Low Similarity, Small Target Dataset:
Use as feature extractor
Train only classifier

1.5 Negative Transfer:
---------------------
When Transfer Hurts:
Pre-trained model hurts target performance
Source and target too dissimilar

Causes:
- Domain mismatch
- Task mismatch
- Poor source model
- Inappropriate transfer strategy

Mitigation:
- Better domain selection
- Gradual unfreezing
- Adaptive learning rates
- Domain adaptation techniques

=======================================================

2. PRE-TRAINED MODEL ARCHITECTURES
==================================

2.1 Computer Vision Models:
---------------------------
ImageNet Pre-trained Models:
- ResNet family (ResNet-18, 50, 101, 152)
- VGG networks (VGG-16, VGG-19)
- Inception/GoogLeNet variants
- DenseNet architectures
- EfficientNet models

Modern Architectures:
- Vision Transformers (ViT)
- Swin Transformers
- ConvNeXt
- RegNet families

Specialized Models:
- Object Detection: R-CNN family
- Semantic Segmentation: DeepLab, U-Net
- Instance Segmentation: Mask R-CNN

2.2 Natural Language Processing:
-------------------------------
Language Models:
- BERT (Bidirectional Encoder Representations)
- GPT family (GPT-1, GPT-2, GPT-3, GPT-4)
- RoBERTa (Robustly Optimized BERT)
- ELECTRA
- T5 (Text-to-Text Transfer Transformer)

Domain-Specific Models:
- BioBERT (Biomedical)
- FinBERT (Finance)
- ClinicalBERT (Clinical notes)
- SciBERT (Scientific literature)

Multilingual Models:
- mBERT (Multilingual BERT)
- XLM-R (Cross-lingual Language Model)
- M2M-100 (Many-to-Many translation)

2.3 Model Selection Criteria:
----------------------------
Task Similarity:
Choose models pre-trained on similar tasks
Classification → classification transfer well

Domain Similarity:
Visual similarity for vision tasks
Language similarity for NLP tasks

Model Size:
Larger models often transfer better
But may be overkill for simple tasks

Computational Constraints:
Consider inference speed requirements
Memory and computational limitations

Data Availability:
Large target datasets can handle complex models
Small datasets benefit from simpler models

2.4 Pre-trained Model Sources:
-----------------------------
Official Model Zoos:
- TorchVision (PyTorch vision models)
- Transformers (HuggingFace NLP models)
- TensorFlow Hub
- ONNX Model Zoo

Community Resources:
- Papers With Code
- GitHub repositories
- Academic releases

Commercial Platforms:
- AWS SageMaker
- Google Cloud AI
- Azure Machine Learning

2.5 Model Versioning and Updates:
---------------------------------
Model Evolution:
Pre-trained models improve over time
Consider using latest versions

Backward Compatibility:
Ensure compatibility with your codebase
Test thoroughly before switching

Performance Tracking:
Compare different model versions
Benchmark on your specific tasks

=======================================================

3. FINE-TUNING STRATEGIES
=========================

3.1 Full Fine-tuning:
--------------------
Concept:
Update all model parameters
Start with pre-trained weights

Algorithm:
1. Load pre-trained model
2. Replace final layer for target task
3. Train entire network on target data
4. Use lower learning rate than training from scratch

Benefits:
- Maximum adaptation to target domain
- Best performance when data is abundant
- Can handle significant domain shift

Risks:
- Overfitting with small datasets
- Catastrophic forgetting of source knowledge
- Requires careful hyperparameter tuning

3.2 Layer-wise Fine-tuning:
--------------------------
Progressive Unfreezing:
Start with frozen early layers
Gradually unfreeze layers during training

Strategy:
1. Freeze all layers except classifier
2. Train classifier for few epochs
3. Unfreeze last few layers
4. Continue training with lower learning rate
5. Repeat until desired layers are unfrozen

Implementation:
```python
def progressive_unfreezing(model, dataloader, num_epochs_per_stage):
    # Stage 1: Train only classifier
    for param in model.features.parameters():
        param.requires_grad = False
    train_stage(model, dataloader, num_epochs_per_stage)
    
    # Stage 2: Unfreeze last block
    for param in model.features[-1].parameters():
        param.requires_grad = True
    train_stage(model, dataloader, num_epochs_per_stage)
    
    # Continue unfreezing...
```

3.3 Discriminative Learning Rates:
----------------------------------
Concept:
Different learning rates for different layers
Lower rates for early layers, higher for later layers

Rationale:
- Early layers learn general features
- Later layers need more adaptation
- Prevent destroying pre-trained features

Implementation:
```python
def setup_discriminative_lr(model, base_lr, factor=0.1):
    param_groups = []
    
    # Early layers with lower learning rate
    param_groups.append({
        'params': model.features[:5].parameters(),
        'lr': base_lr * factor
    })
    
    # Middle layers with medium learning rate
    param_groups.append({
        'params': model.features[5:10].parameters(),
        'lr': base_lr * factor * 2
    })
    
    # Later layers with higher learning rate
    param_groups.append({
        'params': model.classifier.parameters(),
        'lr': base_lr
    })
    
    return param_groups
```

3.4 Gradual Unfreezing:
----------------------
Howard-Ruder Method:
Unfreeze one layer at a time
Train for fixed number of epochs before unfreezing next

Warm-up Strategy:
1. Train final layer only
2. Unfreeze last hidden layer
3. Train both layers
4. Continue unfreezing backwards

Benefits:
- Stable training
- Prevents catastrophic forgetting
- Good for small datasets

3.5 Learning Rate Scheduling:
----------------------------
Cyclical Learning Rates:
Cycle learning rate during fine-tuning
Help escape local minima

Warm Restart:
Periodic learning rate resets
Allow exploration of different minima

Cosine Annealing:
Smooth learning rate decay
Often works well for fine-tuning

Implementation:
```python
def cosine_annealing_warm_restarts(optimizer, T_0, T_mult=1, eta_min=0):
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=T_0, T_mult=T_mult, eta_min=eta_min
    )
    return scheduler
```

3.6 Regularization for Fine-tuning:
----------------------------------
Weight Decay:
Prevent overfitting during fine-tuning
Typical values: 1e-4 to 1e-6

Dropout:
Add dropout during fine-tuning
Be careful not to affect pre-trained features

Early Stopping:
Monitor validation performance
Stop when performance plateaus

Data Augmentation:
Increase effective dataset size
Especially important for small target datasets

=======================================================

4. DOMAIN ADAPTATION TECHNIQUES
===============================

4.1 Unsupervised Domain Adaptation:
----------------------------------
Domain Discrepancy Minimization:
Minimize difference between source and target domains
Use domain-invariant features

Maximum Mean Discrepancy (MMD):
MMD(X_s, X_t) = ||μ_s - μ_t||²_H

where μ_s, μ_t are mean embeddings in RKHS

CORAL (Correlation Alignment):
Align second-order statistics
L_CORAL = (1/4d²)||C_s - C_t||²_F

where C_s, C_t are covariance matrices

Adversarial Domain Adaptation:
Domain classifier tries to distinguish domains
Feature extractor tries to fool domain classifier

4.2 DANN (Domain-Adversarial Neural Networks):
---------------------------------------------
Architecture:
Feature extractor + Label predictor + Domain classifier

Loss Function:
L = L_cls(y_s, ŷ_s) - λL_domain(d, d̂)

Training:
Minimax optimization
Feature extractor maximizes domain confusion

Implementation:
```python
class DANN(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        self.classifier = nn.Linear(hidden_dim, num_classes)
        self.domain_classifier = nn.Sequential(
            GradientReversalLayer(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        features = self.feature_extractor(x)
        class_pred = self.classifier(features)
        domain_pred = self.domain_classifier(features)
        return class_pred, domain_pred
```

4.3 Self-Training and Pseudo-Labeling:
-------------------------------------
Concept:
Use model predictions on target domain as labels
Iteratively retrain with pseudo-labels

Algorithm:
1. Train on source domain
2. Generate predictions on target domain
3. Select high-confidence predictions as pseudo-labels
4. Retrain on source + pseudo-labeled target data
5. Repeat until convergence

Confidence Thresholding:
Only use predictions above confidence threshold
Balance between precision and recall

4.4 Gradual Domain Adaptation:
-----------------------------
Intermediate Domains:
Create gradual transition between domains
Use intermediate synthetic data

Progressive Adaptation:
Gradually shift from source to target
Slowly adapt model parameters

Domain Interpolation:
Linear interpolation between domains
Create smooth transition path

4.5 Multi-source Domain Adaptation:
----------------------------------
Multiple Source Domains:
Leverage multiple related source domains
Better coverage of target domain

Domain Weighting:
Weight source domains by similarity to target
More similar domains get higher weights

Ensemble Methods:
Train separate models on each source domain
Combine predictions for target domain

=======================================================

5. FEATURE EXTRACTION VS FINE-TUNING
====================================

5.1 Feature Extraction Approach:
-------------------------------
Concept:
Use pre-trained model as fixed feature extractor
Train only final classifier

When to Use:
- Small target dataset
- Very different target domain
- Limited computational resources
- Want to preserve pre-trained features

Implementation:
```python
def feature_extraction_setup(pretrained_model, num_classes):
    # Freeze all parameters
    for param in pretrained_model.parameters():
        param.requires_grad = False
    
    # Replace classifier
    pretrained_model.classifier = nn.Linear(
        pretrained_model.classifier.in_features, 
        num_classes
    )
    
    # Only classifier parameters require gradients
    return pretrained_model
```

Benefits:
- Fast training
- Less prone to overfitting
- Computationally efficient
- Preserves general features

Limitations:
- Limited adaptation to target domain
- May not achieve best performance
- Features may not be optimal for target task

5.2 Fine-tuning Approach:
------------------------
Concept:
Update all or subset of pre-trained parameters
Adapt model to target domain/task

When to Use:
- Sufficient target data
- Related source and target domains
- Want maximum performance
- Have computational resources

Benefits:
- Maximum adaptation capability
- Best performance potential
- Can handle domain shift
- Learns task-specific features

Limitations:
- Risk of overfitting
- Computational overhead
- May destroy useful pre-trained features
- Requires careful hyperparameter tuning

5.3 Hybrid Approaches:
---------------------
Selective Fine-tuning:
Fine-tune some layers, freeze others
Balance between adaptation and preservation

Layer-wise Freezing:
Freeze early layers, fine-tune later layers
Preserve low-level features, adapt high-level

Adaptive Fine-tuning:
Determine which layers to fine-tune
Based on target dataset characteristics

5.4 Decision Framework:
----------------------
Dataset Size:
- Large dataset: Full fine-tuning
- Medium dataset: Selective fine-tuning
- Small dataset: Feature extraction

Domain Similarity:
- High similarity: Fine-tuning works well
- Low similarity: Feature extraction safer

Computational Budget:
- High budget: Fine-tuning
- Low budget: Feature extraction

Performance Requirements:
- Maximum performance: Fine-tuning
- Good enough: Feature extraction

5.5 Evaluation and Comparison:
-----------------------------
Performance Metrics:
Compare accuracy, F1-score, etc.
Consider both validation and test performance

Training Time:
Measure wall-clock training time
Consider computational efficiency

Memory Usage:
Monitor GPU memory consumption
Important for deployment

Robustness:
Test on different data distributions
Evaluate generalization capability

=======================================================

6. MULTI-TASK AND META-LEARNING
===============================

6.1 Multi-task Transfer Learning:
--------------------------------
Concept:
Learn multiple related tasks simultaneously
Share representations across tasks

Shared Representations:
Common feature extractor
Task-specific heads

Benefits:
- Improved data efficiency
- Better generalization
- Reduced overfitting
- Knowledge sharing between tasks

Architecture:
```python
class MultiTaskModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, task_configs):
        super().__init__()
        # Shared feature extractor
        self.shared_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Task-specific heads
        self.task_heads = nn.ModuleDict()
        for task_name, num_classes in task_configs.items():
            self.task_heads[task_name] = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x, task_name):
        shared_features = self.shared_encoder(x)
        return self.task_heads[task_name](shared_features)
```

6.2 Task Weighting:
------------------
Uniform Weighting:
Equal weight for all tasks
Simple but may not be optimal

Performance-based Weighting:
Weight tasks by performance
Emphasize poorly performing tasks

Uncertainty Weighting:
Weight tasks by prediction uncertainty
More uncertain tasks get higher weights

Dynamic Weighting:
Adjust weights during training
Adapt to changing task difficulties

6.3 Meta-Learning for Transfer:
------------------------------
Learn to Learn:
Learn how to quickly adapt to new tasks
Few-shot learning capability

MAML (Model-Agnostic Meta-Learning):
Learn initialization that can quickly adapt
θ* = θ - α∇_θ L_task(θ)

Reptile:
Simplified meta-learning algorithm
θ ← θ + ε(φ - θ)

where φ is task-specific adapted parameters

6.4 Few-Shot Learning:
---------------------
N-way K-shot Learning:
N classes with K examples each
Learn to classify with minimal data

Prototypical Networks:
Learn prototype representations for each class
Classify based on distance to prototypes

Matching Networks:
Attention-based matching
Compare query to support examples

6.5 Domain-Agnostic Meta-Learning:
---------------------------------
Domain-Invariant Features:
Learn features that work across domains
Minimize domain-specific information

Meta-Domain Adaptation:
Meta-learn domain adaptation strategies
Quickly adapt to new domains

Implementation:
```python
class DomainAgnosticMetaLearner:
    def __init__(self, model, meta_lr=0.01, adapt_lr=0.1):
        self.model = model
        self.meta_lr = meta_lr
        self.adapt_lr = adapt_lr
        self.meta_optimizer = optim.Adam(model.parameters(), lr=meta_lr)
    
    def meta_train_step(self, support_tasks, query_tasks):
        meta_loss = 0
        
        for support_task, query_task in zip(support_tasks, query_tasks):
            # Clone model for task adaptation
            adapted_model = deepcopy(self.model)
            
            # Adapt to support task
            support_loss = self.compute_loss(adapted_model, support_task)
            adapted_params = self.gradient_step(adapted_model, support_loss)
            
            # Evaluate on query task
            query_loss = self.compute_loss_with_params(adapted_params, query_task)
            meta_loss += query_loss
        
        # Meta-update
        self.meta_optimizer.zero_grad()
        meta_loss.backward()
        self.meta_optimizer.step()
```

=======================================================

7. ADVANCED TRANSFER LEARNING
=============================

7.1 Neural Architecture Search for Transfer:
--------------------------------------------
Transfer NAS:
Search for architectures that transfer well
Consider transferability in architecture design

Progressive Search:
Start with simple architectures
Gradually increase complexity

Efficiency-aware Search:
Balance performance and efficiency
Consider deployment constraints

7.2 Knowledge Distillation:
--------------------------
Teacher-Student Framework:
Large pre-trained model as teacher
Smaller model as student

Soft Targets:
Use teacher's soft predictions
L_KD = KL(p_teacher || p_student)

Feature Distillation:
Match intermediate representations
Align feature maps between teacher and student

Implementation:
```python
def knowledge_distillation_loss(student_logits, teacher_logits, targets, alpha=0.7, temperature=3):
    # Soft target loss
    kd_loss = nn.KLDivLoss()(
        F.log_softmax(student_logits / temperature, dim=1),
        F.softmax(teacher_logits / temperature, dim=1)
    ) * (temperature ** 2)
    
    # Hard target loss
    ce_loss = nn.CrossEntropyLoss()(student_logits, targets)
    
    # Combined loss
    return alpha * kd_loss + (1 - alpha) * ce_loss
```

7.3 Continual Learning:
----------------------
Catastrophic Forgetting:
Neural networks forget previous tasks
Major challenge in continual learning

Elastic Weight Consolidation (EWC):
Protect important weights from change
L_EWC = L_new + λ Σᵢ Fᵢ(θᵢ - θᵢ*)²

Progressive Neural Networks:
Add new parameters for new tasks
Preserve old task performance

PackNet:
Prune and pack multiple tasks
Efficient parameter sharing

7.4 Cross-Modal Transfer:
------------------------
Vision-Language Transfer:
Transfer between vision and language
CLIP-style models

Audio-Visual Transfer:
Learn shared representations
Cross-modal correspondences

Multi-Modal Pre-training:
Train on multiple modalities
Rich representation learning

7.5 Self-Supervised Transfer:
----------------------------
Contrastive Learning:
Learn representations through contrastive tasks
SimCLR, MoCo, SwAV

Masked Language/Image Modeling:
Predict masked parts of input
BERT, MAE (Masked Autoencoders)

Rotation Prediction:
Predict image rotations
Learn visual representations

Implementation:
```python
class ContrastiveLearning(nn.Module):
    def __init__(self, encoder, projection_dim=128, temperature=0.07):
        super().__init__()
        self.encoder = encoder
        self.projection = nn.Sequential(
            nn.Linear(encoder.output_dim, projection_dim),
            nn.ReLU(),
            nn.Linear(projection_dim, projection_dim)
        )
        self.temperature = temperature
    
    def forward(self, x1, x2):
        # Encode and project
        z1 = self.projection(self.encoder(x1))
        z2 = self.projection(self.encoder(x2))
        
        # Normalize
        z1 = F.normalize(z1, dim=1)
        z2 = F.normalize(z2, dim=1)
        
        # Compute contrastive loss
        logits = torch.mm(z1, z2.t()) / self.temperature
        labels = torch.arange(z1.size(0))
        
        loss = F.cross_entropy(logits, labels)
        return loss
```

=======================================================

8. IMPLEMENTATION AND PRACTICAL GUIDELINES
==========================================

8.1 PyTorch Transfer Learning:
-----------------------------
Loading Pre-trained Models:
```python
import torchvision.models as models

# Load pre-trained ResNet
model = models.resnet50(pretrained=True)

# Modify final layer for target task
num_classes = 10
model.fc = nn.Linear(model.fc.in_features, num_classes)

# Option: Initialize new layer
nn.init.xavier_uniform_(model.fc.weight)
nn.init.zeros_(model.fc.bias)
```

Feature Extraction Setup:
```python
def setup_feature_extraction(model, num_classes):
    # Freeze all parameters
    for param in model.parameters():
        param.requires_grad = False
    
    # Replace and unfreeze classifier
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    
    # Only train classifier
    optimizer = optim.SGD(model.fc.parameters(), lr=0.001)
    return model, optimizer
```

Fine-tuning Setup:
```python
def setup_fine_tuning(model, num_classes, base_lr=0.001):
    # Replace classifier
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    
    # Different learning rates for different parts
    params = [
        {'params': model.features.parameters(), 'lr': base_lr * 0.1},
        {'params': model.classifier.parameters(), 'lr': base_lr}
    ]
    
    optimizer = optim.SGD(params, momentum=0.9, weight_decay=1e-4)
    return model, optimizer
```

8.2 HuggingFace Transformers:
----------------------------
Loading Pre-trained Models:
```python
from transformers import AutoModel, AutoTokenizer

# Load pre-trained BERT
model_name = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Add classification head
classifier = nn.Linear(model.config.hidden_size, num_classes)
```

Fine-tuning BERT:
```python
from transformers import AdamW, get_linear_schedule_with_warmup

# Setup optimizer
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {
        'params': [p for n, p in model.named_parameters() 
                  if not any(nd in n for nd in no_decay)],
        'weight_decay': 0.01
    },
    {
        'params': [p for n, p in model.named_parameters() 
                  if any(nd in n for nd in no_decay)],
        'weight_decay': 0.0
    }
]

optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)

# Learning rate scheduler
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0.1 * total_steps,
    num_training_steps=total_steps
)
```

8.3 Data Loading and Preprocessing:
----------------------------------
Vision Data:
```python
# Different transforms for pre-trained models
normalize = transforms.Normalize(
    mean=[0.485, 0.456, 0.406],  # ImageNet statistics
    std=[0.229, 0.224, 0.225]
)

train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    normalize
])

val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    normalize
])
```

Text Data:
```python
def prepare_text_data(texts, labels, tokenizer, max_length=512):
    encodings = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=max_length,
        return_tensors='pt'
    )
    
    dataset = TensorDataset(
        encodings['input_ids'],
        encodings['attention_mask'],
        torch.tensor(labels)
    )
    
    return dataset
```

8.4 Training Loops:
------------------
Basic Fine-tuning Loop:
```python
def train_model(model, train_loader, val_loader, num_epochs=10):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    
    best_acc = 0.0
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
        
        # Validation phase
        model.eval()
        val_acc = evaluate_model(model, val_loader)
        
        # Save best model
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
        
        scheduler.step()
    
    return model
```

Progressive Unfreezing:
```python
def progressive_unfreezing(model, train_loader, val_loader):
    # Stage 1: Train only classifier
    for param in model.features.parameters():
        param.requires_grad = False
    
    train_stage(model, train_loader, val_loader, epochs=5)
    
    # Stage 2: Unfreeze last block
    for param in model.features[-1].parameters():
        param.requires_grad = True
    
    train_stage(model, train_loader, val_loader, epochs=5)
    
    # Stage 3: Unfreeze all
    for param in model.parameters():
        param.requires_grad = True
    
    train_stage(model, train_loader, val_loader, epochs=10)
```

8.5 Evaluation and Monitoring:
-----------------------------
Performance Tracking:
```python
def evaluate_transfer_learning(source_model, target_model, test_loader):
    metrics = {}
    
    # Baseline: Random initialization
    random_model = create_random_model()
    metrics['random'] = evaluate_model(random_model, test_loader)
    
    # Feature extraction
    feature_model = setup_feature_extraction(source_model)
    metrics['feature_extraction'] = evaluate_model(feature_model, test_loader)
    
    # Fine-tuning
    metrics['fine_tuning'] = evaluate_model(target_model, test_loader)
    
    return metrics
```

Layer Analysis:
```python
def analyze_layer_importance(model, dataloader, layer_names):
    importances = {}
    
    for layer_name in layer_names:
        # Get layer
        layer = dict(model.named_modules())[layer_name]
        
        # Hook to capture gradients
        gradients = []
        def hook(module, grad_input, grad_output):
            gradients.append(grad_output[0])
        
        handle = layer.register_backward_hook(hook)
        
        # Forward and backward pass
        model.train()
        for inputs, targets in dataloader:
            outputs = model(inputs)
            loss = F.cross_entropy(outputs, targets)
            loss.backward()
            break
        
        # Compute importance
        importance = torch.norm(gradients[0]).item()
        importances[layer_name] = importance
        
        handle.remove()
    
    return importances
```

8.6 Best Practices:
------------------
Model Selection:
1. Choose models pre-trained on related domains
2. Consider computational constraints
3. Test multiple architectures
4. Use latest model versions

Fine-tuning Strategy:
1. Start with feature extraction
2. Gradually unfreeze layers
3. Use discriminative learning rates
4. Monitor for overfitting

Hyperparameter Tuning:
1. Learning rate most critical
2. Adjust batch size if needed
3. Use appropriate schedulers
4. Consider regularization

Data Considerations:
1. Ensure proper preprocessing
2. Handle domain shift carefully
3. Augment data appropriately
4. Balance dataset if needed

8.7 Common Pitfalls:
-------------------
Learning Rate Issues:
- Too high: Destroys pre-trained features
- Too low: Slow convergence
- Solution: Use discriminative rates

Overfitting:
- Small target dataset
- Complex model
- Solution: Regularization, feature extraction

Catastrophic Forgetting:
- Fine-tuning destroys source knowledge
- Solution: Gradual unfreezing, lower rates

Domain Mismatch:
- Source and target too different
- Solution: Better source selection, domain adaptation

8.8 Success Guidelines:
----------------------
Planning:
1. Analyze source-target similarity
2. Consider data size and quality
3. Define success metrics clearly
4. Plan computational resources

Implementation:
1. Start with proven baselines
2. Implement incrementally
3. Monitor training carefully
4. Save intermediate results

Evaluation:
1. Compare multiple strategies
2. Use appropriate metrics
3. Test on held-out data
4. Analyze failure cases

Deployment:
1. Optimize for inference
2. Monitor performance drift
3. Plan for model updates
4. Document transfer choices

=======================================================
END OF DOCUMENT 