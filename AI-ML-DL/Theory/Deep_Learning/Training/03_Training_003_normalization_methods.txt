NORMALIZATION METHODS - BATCHNORM, LAYERNORM, GROUPNORM, AND MORE
===================================================================

TABLE OF CONTENTS:
1. Normalization Fundamentals
2. Batch Normalization Deep Dive
3. Layer Normalization
4. Group Normalization
5. Instance Normalization
6. Weight Normalization
7. Advanced Normalization Techniques
8. Implementation and Practical Guidelines

=======================================================

1. NORMALIZATION FUNDAMENTALS
=============================

1.1 The Normalization Problem:
-----------------------------
Internal Covariate Shift:
Distribution of layer inputs changes during training
Makes optimization difficult and unstable

Gradient Flow Issues:
Poor normalization leads to:
- Vanishing gradients in deep networks
- Exploding gradients
- Slow convergence

Scale Sensitivity:
Different input scales affect learning rates
Some features dominate others

Activation Saturation:
Sigmoid/tanh activations saturate with large inputs
Leads to vanishing gradients

1.2 Normalization Principles:
----------------------------
Zero Mean, Unit Variance:
Transform inputs to standard distribution
x_norm = (x - μ) / σ

Learnable Parameters:
Scale (γ) and shift (β) parameters
Allow network to undo normalization if needed
y = γ * x_norm + β

Differentiability:
All operations must be differentiable
Enable end-to-end gradient-based training

1.3 Types of Normalization:
--------------------------
By Computation Axis:
- Batch: Normalize across batch dimension
- Layer: Normalize across feature dimension
- Instance: Normalize per sample
- Group: Normalize within feature groups

By Application Domain:
- Computer Vision: BatchNorm, GroupNorm
- NLP: LayerNorm
- Style Transfer: InstanceNorm
- Generative Models: SpectralNorm

1.4 When to Apply Normalization:
-------------------------------
Network Depth:
Deeper networks benefit more from normalization
Essential for very deep networks (50+ layers)

Gradient Problems:
When experiencing vanishing/exploding gradients
Unstable training dynamics

Slow Convergence:
When training is slow or unstable
Different scales across features

Architecture Type:
- CNNs: BatchNorm standard
- RNNs/Transformers: LayerNorm preferred
- GANs: SpectralNorm common

1.5 Normalization Effects:
-------------------------
Training Acceleration:
Allows higher learning rates
Faster convergence

Gradient Conditioning:
Better conditioned optimization landscape
More stable gradients

Regularization:
Implicit regularization effect
Reduces overfitting

Reduced Initialization Sensitivity:
Less dependent on weight initialization
More robust training

=======================================================

2. BATCH NORMALIZATION DEEP DIVE
=================================

2.1 Batch Normalization Algorithm:
----------------------------------
Training Mode:
μ_B = (1/m) Σᵢ₌₁ᵐ xᵢ           (batch mean)
σ²_B = (1/m) Σᵢ₌₁ᵐ (xᵢ - μ_B)²    (batch variance)
x̂ᵢ = (xᵢ - μ_B) / √(σ²_B + ε)     (normalize)
yᵢ = γx̂ᵢ + β                      (scale and shift)

Inference Mode:
x̂ᵢ = (xᵢ - μ_running) / √(σ²_running + ε)
yᵢ = γx̂ᵢ + β

Running Statistics Update:
μ_running = (1-α) * μ_running + α * μ_B
σ²_running = (1-α) * σ²_running + α * σ²_B

where α is momentum (typically 0.1)

2.2 Batch Normalization Gradients:
----------------------------------
Backward Pass Derivations:
∂L/∂γ = Σᵢ (∂L/∂yᵢ) * x̂ᵢ
∂L/∂β = Σᵢ (∂L/∂yᵢ)

∂L/∂x̂ᵢ = (∂L/∂yᵢ) * γ

∂L/∂σ²_B = Σᵢ (∂L/∂x̂ᵢ) * (xᵢ - μ_B) * (-1/2) * (σ²_B + ε)^(-3/2)

∂L/∂μ_B = Σᵢ (∂L/∂x̂ᵢ) * (-1/√(σ²_B + ε)) + (∂L/∂σ²_B) * Σᵢ (-2(xᵢ - μ_B))/m

∂L/∂xᵢ = (∂L/∂x̂ᵢ) * (1/√(σ²_B + ε)) + (∂L/∂σ²_B) * (2(xᵢ - μ_B))/m + (∂L/∂μ_B)/m

2.3 Batch Normalization Variants:
---------------------------------
Batch Renormalization:
Addresses train-test discrepancy
Clips running statistics updates

Moving Average Variant:
Different momentum schedules
Adaptive momentum based on batch size

Sync Batch Normalization:
Synchronize statistics across devices
Important for distributed training

Virtual Batch Normalization:
Use reference batch for normalization
More stable for GANs

2.4 Placement in Networks:
-------------------------
Standard Placement:
Conv/Linear → BatchNorm → Activation

Pre-activation:
BatchNorm → Activation → Conv/Linear
Often works better for very deep networks

Post-activation:
Conv/Linear → Activation → BatchNorm
Less common but sometimes used

2.5 BatchNorm Implementation:
----------------------------
PyTorch Implementation:
```python
class BatchNorm2d(nn.Module):
    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        
        if self.affine:
            self.weight = nn.Parameter(torch.ones(num_features))
            self.bias = nn.Parameter(torch.zeros(num_features))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)
        
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))
    
    def forward(self, input):
        if self.training:
            # Compute batch statistics
            mean = input.mean([0, 2, 3])
            var = input.var([0, 2, 3], unbiased=False)
            
            # Update running statistics
            with torch.no_grad():
                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean
                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var
                self.num_batches_tracked += 1
            
            # Normalize
            input_normalized = (input - mean[None, :, None, None]) / torch.sqrt(var[None, :, None, None] + self.eps)
        else:
            # Use running statistics
            input_normalized = (input - self.running_mean[None, :, None, None]) / torch.sqrt(self.running_var[None, :, None, None] + self.eps)
        
        # Apply affine transformation
        if self.affine:
            input_normalized = input_normalized * self.weight[None, :, None, None] + self.bias[None, :, None, None]
        
        return input_normalized
```

2.6 BatchNorm Challenges:
-------------------------
Small Batch Size:
Poor statistics estimation
May hurt performance

Batch Size Dependency:
Different behavior for different batch sizes
Inconsistency between train/test

Memory Requirements:
Store running statistics
Additional parameters

Distribution Shift:
Train-test distribution mismatch
Can cause performance degradation

=======================================================

3. LAYER NORMALIZATION
======================

3.1 Layer Normalization Motivation:
----------------------------------
Batch Independence:
Normalize across features, not batch
Independent of batch size

RNN Compatibility:
Variable sequence lengths
Batch statistics don't make sense

Online Learning:
Single sample processing
No batch dimension available

Consistent Behavior:
Same computation in training and inference
No train-test discrepancy

3.2 Layer Normalization Algorithm:
---------------------------------
Forward Pass:
μ = (1/H) Σᵢ₌₁ᴴ aᵢ           (mean across features)
σ² = (1/H) Σᵢ₌₁ᴴ (aᵢ - μ)²    (variance across features)
â = (a - μ) / √(σ² + ε)      (normalize)
y = γâ + β                   (scale and shift)

where H is the number of hidden units in the layer

Backward Pass:
∂L/∂γ = Σᵢ (∂L/∂yᵢ) * âᵢ
∂L/∂β = Σᵢ (∂L/∂yᵢ)

∂L/∂â = (∂L/∂y) ⊙ γ

∂L/∂σ² = Σᵢ (∂L/∂âᵢ) * (aᵢ - μ) * (-1/2) * (σ² + ε)^(-3/2)

∂L/∂μ = Σᵢ (∂L/∂âᵢ) * (-1/√(σ² + ε)) + (∂L/∂σ²) * Σᵢ (-2(aᵢ - μ))/H

∂L/∂a = (∂L/∂â) * (1/√(σ² + ε)) + (∂L/∂σ²) * (2(a - μ))/H + (∂L/∂μ)/H

3.3 Layer Normalization Applications:
------------------------------------
Recurrent Neural Networks:
Normalize hidden states
Stabilize training of deep RNNs

Transformers:
Standard normalization choice
Applied to attention and feed-forward layers

Reinforcement Learning:
Normalize value function inputs
Stabilize policy gradient methods

Generative Models:
Normalize generator inputs
Improve training stability

3.4 LayerNorm Implementation:
----------------------------
PyTorch Implementation:
```python
class LayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):
        super().__init__()
        self.normalized_shape = normalized_shape
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        
        if self.elementwise_affine:
            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.bias = nn.Parameter(torch.zeros(normalized_shape))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)
    
    def forward(self, input):
        mean = input.mean(dim=-1, keepdim=True)
        var = input.var(dim=-1, keepdim=True, unbiased=False)
        
        input_normalized = (input - mean) / torch.sqrt(var + self.eps)
        
        if self.elementwise_affine:
            input_normalized = input_normalized * self.weight + self.bias
        
        return input_normalized
```

Manual Implementation:
```python
def layer_norm(x, gamma, beta, eps=1e-5):
    # x shape: [batch_size, seq_len, hidden_size] or [batch_size, hidden_size]
    
    # Compute statistics along the last dimension
    mean = x.mean(dim=-1, keepdim=True)
    var = x.var(dim=-1, keepdim=True, unbiased=False)
    
    # Normalize
    x_norm = (x - mean) / torch.sqrt(var + eps)
    
    # Scale and shift
    return gamma * x_norm + beta
```

3.5 LayerNorm vs BatchNorm:
--------------------------
Computational Differences:
- LayerNorm: Statistics across features
- BatchNorm: Statistics across batch

Use Cases:
- LayerNorm: RNNs, Transformers, variable batch sizes
- BatchNorm: CNNs, large batch training

Performance:
- LayerNorm: Slower for CNNs, better for sequences
- BatchNorm: Faster for CNNs, problems with small batches

Memory:
- LayerNorm: No running statistics
- BatchNorm: Stores running mean/variance

=======================================================

4. GROUP NORMALIZATION
======================

4.1 Group Normalization Concept:
--------------------------------
Motivation:
Bridge between LayerNorm and InstanceNorm
Divide channels into groups

Algorithm:
1. Divide C channels into G groups
2. Compute statistics within each group
3. Normalize within groups

Mathematical Formulation:
For group g containing channels in S_g:
μ_g = (1/|S_g|HW) Σ_{i∈S_g} Σ_{h,w} x_{i,h,w}
σ²_g = (1/|S_g|HW) Σ_{i∈S_g} Σ_{h,w} (x_{i,h,w} - μ_g)²
ŷ_{i,h,w} = γᵢ * (x_{i,h,w} - μ_g)/√(σ²_g + ε) + βᵢ

4.2 Group Configuration:
-----------------------
Number of Groups:
- G = 1: Layer Normalization
- G = C: Instance Normalization  
- G = C/2: Common choice
- G = 32: Often works well

Group Size:
Channels per group = C/G
Should divide evenly

Channel Assignment:
Consecutive channels in same group
Could be different assignments

4.3 GroupNorm Benefits:
----------------------
Batch Independence:
Like LayerNorm, independent of batch size
Stable across different batch sizes

Better than LayerNorm for Vision:
Accounts for spatial correlations
Better than full layer normalization

Computational Efficiency:
More efficient than LayerNorm
Parallelizable across groups

4.4 GroupNorm Implementation:
----------------------------
PyTorch Implementation:
```python
class GroupNorm(nn.Module):
    def __init__(self, num_groups, num_channels, eps=1e-5, affine=True):
        super().__init__()
        self.num_groups = num_groups
        self.num_channels = num_channels
        self.eps = eps
        self.affine = affine
        
        if self.affine:
            self.weight = nn.Parameter(torch.ones(num_channels))
            self.bias = nn.Parameter(torch.zeros(num_channels))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)
    
    def forward(self, input):
        N, C, H, W = input.size()
        
        # Reshape to [N, num_groups, C//num_groups, H, W]
        input = input.view(N, self.num_groups, C // self.num_groups, H, W)
        
        # Compute statistics over group dimension
        mean = input.mean(dim=[2, 3, 4], keepdim=True)
        var = input.var(dim=[2, 3, 4], keepdim=True, unbiased=False)
        
        # Normalize
        input = (input - mean) / torch.sqrt(var + self.eps)
        
        # Reshape back
        input = input.view(N, C, H, W)
        
        # Apply affine transformation
        if self.affine:
            input = input * self.weight[None, :, None, None] + self.bias[None, :, None, None]
        
        return input
```

4.5 GroupNorm Applications:
--------------------------
Object Detection:
Faster R-CNN, Mask R-CNN
Better than BatchNorm for small batches

Semantic Segmentation:
DeepLab, U-Net variants
Good for variable input sizes

Video Understanding:
3D CNNs with temporal dimension
Consistent across temporal axis

Transfer Learning:
Fine-tuning with different batch sizes
More robust than BatchNorm

=======================================================

5. INSTANCE NORMALIZATION
=========================

5.1 Instance Normalization Concept:
----------------------------------
Per-Sample Normalization:
Normalize each sample independently
Each channel normalized separately

Mathematical Formulation:
μ_{n,c} = (1/HW) Σ_{h,w} x_{n,c,h,w}
σ²_{n,c} = (1/HW) Σ_{h,w} (x_{n,c,h,w} - μ_{n,c})²
ŷ_{n,c,h,w} = γ_c * (x_{n,c,h,w} - μ_{n,c})/√(σ²_{n,c} + ε) + β_c

Key Properties:
- Independent across samples
- Independent across channels
- Spatial statistics computed per channel

5.2 Style Transfer Applications:
-------------------------------
Neural Style Transfer:
Remove style information during normalization
Allow style to be added back

Conditional Instance Normalization:
Different normalization parameters per style
y = γ_s * InstanceNorm(x) + β_s

AdaIN (Adaptive Instance Normalization):
Use style statistics for normalization
μ_style, σ_style from style image

5.3 InstanceNorm Implementation:
------------------------------
PyTorch Implementation:
```python
class InstanceNorm2d(nn.Module):
    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=False):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        
        if self.affine:
            self.weight = nn.Parameter(torch.ones(num_features))
            self.bias = nn.Parameter(torch.zeros(num_features))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)
    
    def forward(self, input):
        N, C, H, W = input.size()
        
        # Compute statistics per instance per channel
        input_reshaped = input.view(N, C, -1)
        mean = input_reshaped.mean(dim=2, keepdim=True)
        var = input_reshaped.var(dim=2, keepdim=True, unbiased=False)
        
        # Normalize
        input_normalized = (input_reshaped - mean) / torch.sqrt(var + self.eps)
        input_normalized = input_normalized.view(N, C, H, W)
        
        # Apply affine transformation
        if self.affine:
            input_normalized = input_normalized * self.weight[None, :, None, None] + self.bias[None, :, None, None]
        
        return input_normalized
```

5.4 When to Use InstanceNorm:
----------------------------
Style Transfer:
Remove content-dependent style
Enable style manipulation

Image Generation:
GANs for image synthesis
Remove batch dependencies

Domain Adaptation:
Normalize domain-specific statistics
Improve cross-domain transfer

Texture Synthesis:
Focus on texture patterns
Remove global statistics

=======================================================

6. WEIGHT NORMALIZATION
=======================

6.1 Weight Normalization Concept:
---------------------------------
Parameter Reparameterization:
Separate magnitude and direction
w = g * v/||v||

where:
- g: magnitude (scalar)
- v: direction vector
- ||v||: L2 norm of v

Benefits:
- Normalizes weight vectors
- Improves conditioning
- Faster than batch normalization

6.2 Weight Normalization Algorithm:
----------------------------------
Forward Pass:
w = g * v/||v||₂
y = w^T x + b

Backward Pass:
∂L/∂g = (∂L/∂w)^T (v/||v||₂)
∂L/∂v = (g/||v||₂) * (∂L/∂w - (∂L/∂w)^T(v/||v||₂) * (v/||v||₂))

Update Rules:
g ← g - α * ∂L/∂g
v ← v - α * ∂L/∂v

6.3 Weight Normalization Implementation:
---------------------------------------
PyTorch Implementation:
```python
def weight_norm(module, name='weight', dim=0):
    WeightNorm.apply(module, name, dim)
    return module

class WeightNorm:
    def __init__(self, name, dim):
        self.name = name
        self.dim = dim
    
    def compute_weight(self, module):
        g = getattr(module, self.name + '_g')
        v = getattr(module, self.name + '_v')
        return g * v / torch.norm(v, dim=self.dim, keepdim=True)
    
    @staticmethod
    def apply(module, name, dim):
        fn = WeightNorm(name, dim)
        
        weight = getattr(module, name)
        
        # Separate magnitude and direction
        g = torch.norm(weight, dim=dim, keepdim=True)
        v = weight / g
        
        # Remove original weight
        delattr(module, name)
        
        # Add new parameters
        module.register_parameter(name + '_g', nn.Parameter(g))
        module.register_parameter(name + '_v', nn.Parameter(v))
        
        # Register hook
        module.register_forward_pre_hook(fn)
        
        return fn
    
    def __call__(self, module, inputs):
        weight = self.compute_weight(module)
        setattr(module, self.name, weight)
```

Manual Implementation:
```python
class WeightNormalizedLinear(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # Direction parameter
        self.v = nn.Parameter(torch.randn(out_features, in_features))
        # Magnitude parameter
        self.g = nn.Parameter(torch.norm(self.v, dim=1, keepdim=True))
        
        if bias:
            self.bias = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter('bias', None)
    
    def forward(self, input):
        # Compute normalized weight
        weight = self.g * self.v / torch.norm(self.v, dim=1, keepdim=True)
        return F.linear(input, weight, self.bias)
```

6.4 Weight Normalization Benefits:
---------------------------------
Improved Conditioning:
Better conditioned optimization
Faster convergence

Batch Independence:
No batch size dependency
Consistent train/test behavior

Computational Efficiency:
Lower overhead than batch normalization
No running statistics

Gradient Flow:
Better gradient flow properties
Reduced vanishing gradients

6.5 Weight Normalization Limitations:
------------------------------------
Not Always Better:
May not outperform batch normalization
Task and architecture dependent

Limited Regularization:
Less regularization effect
May need additional techniques

Initialization Sensitivity:
Still sensitive to initialization
Careful initialization needed

=======================================================

7. ADVANCED NORMALIZATION TECHNIQUES
====================================

7.1 Spectral Normalization:
---------------------------
Concept:
Normalize by largest singular value
Control Lipschitz constant

Formulation:
W_SN = W / σ(W)

where σ(W) is spectral norm (largest singular value)

Power Iteration:
Efficient computation of spectral norm
u^(t+1) = W^T v^(t) / ||W^T v^(t)||
v^(t+1) = W u^(t+1) / ||W u^(t+1)||
σ(W) ≈ u^T W v

Applications:
- GAN training stabilization
- Lipschitz constrained networks
- Improved generalization

7.2 Self-Normalizing Networks:
-----------------------------
SELU Activation:
Scaled Exponential Linear Unit
Self-normalizing properties

Formulation:
SELU(x) = λ * ELU(x) if x > 0
         λ * α * (exp(x) - 1) if x ≤ 0

where λ ≈ 1.0507, α ≈ 1.6733

Properties:
- Zero mean, unit variance preservation
- No explicit normalization needed
- Automatic regularization

7.3 Switchable Normalization:
----------------------------
Concept:
Learn to switch between normalization types
Combine BatchNorm, LayerNorm, InstanceNorm

Formulation:
SN(x) = γ * (Σₖ wₖ * Normₖ(x)) + β

where wₖ are learned weights and Normₖ are different normalizations

Benefits:
- Automatic normalization selection
- Task-adaptive normalization
- Better than fixed choices

7.4 Differentiable GroupNorm:
----------------------------
Learnable Groups:
Learn optimal group assignment
Continuous relaxation of discrete groups

Gated GroupNorm:
Add gating mechanism
Control information flow between groups

Adaptive Group Size:
Learn number of groups
Task-specific group configuration

7.5 Cross-Normalization:
-----------------------
Batch-Instance Normalization:
Combine BatchNorm and InstanceNorm
Learn mixing ratio

Cross-domain Normalization:
Normalize across different domains
Improve domain adaptation

Multi-scale Normalization:
Normalize at multiple scales
Better for multi-scale features

=======================================================

8. IMPLEMENTATION AND PRACTICAL GUIDELINES
==========================================

8.1 Choosing Normalization Method:
---------------------------------
Task-Based Guidelines:
- Image Classification: BatchNorm
- Object Detection: GroupNorm
- Semantic Segmentation: GroupNorm/BatchNorm
- Style Transfer: InstanceNorm
- NLP/Transformers: LayerNorm
- GANs: SpectralNorm

Batch Size Considerations:
- Large batches (>16): BatchNorm
- Small batches (<8): GroupNorm/LayerNorm
- Variable batches: LayerNorm/GroupNorm
- Single sample: InstanceNorm/LayerNorm

Architecture Considerations:
- CNNs: BatchNorm/GroupNorm
- RNNs: LayerNorm
- Transformers: LayerNorm
- ResNets: BatchNorm with proper placement

8.2 Implementation Best Practices:
---------------------------------
Placement Guidelines:
```python
# Standard placement
def conv_bn_relu(in_channels, out_channels):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, 3, padding=1),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(inplace=True)
    )

# Pre-activation (often better for deep networks)
def bn_relu_conv(in_channels, out_channels):
    return nn.Sequential(
        nn.BatchNorm2d(in_channels),
        nn.ReLU(inplace=True),
        nn.Conv2d(in_channels, out_channels, 3, padding=1)
    )
```

Initialization:
```python
def init_normalization(module):
    if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
        nn.init.constant_(module.weight, 1)
        nn.init.constant_(module.bias, 0)
    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):
        nn.init.constant_(module.weight, 1)
        nn.init.constant_(module.bias, 0)
```

8.3 Training Considerations:
---------------------------
Learning Rate Adjustment:
Higher learning rates possible with normalization
Start with 2-10x higher than without normalization

Momentum in BatchNorm:
Default momentum = 0.1
Larger momentum for smaller datasets

Epsilon Value:
Numerical stability parameter
Default 1e-5 usually sufficient

Mixed Precision:
Normalization layers compatible
No special considerations needed

8.4 Debugging Normalization:
----------------------------
Check Statistics:
Monitor running means and variances
Should be reasonable (means near 0, vars near 1)

Gradient Flow:
Check gradients through normalization layers
Should not be vanishing or exploding

Train-Test Consistency:
Ensure proper train/eval mode switching
Check running statistics update

Performance Comparison:
Compare with and without normalization
Ablation studies for different types

8.5 Custom Normalization Implementation:
---------------------------------------
Generic Normalization Framework:
```python
class GenericNorm(nn.Module):
    def __init__(self, normalized_shape, norm_type='batch', num_groups=32, eps=1e-5):
        super().__init__()
        self.norm_type = norm_type
        self.eps = eps
        
        if norm_type == 'batch':
            self.norm = nn.BatchNorm2d(normalized_shape, eps=eps)
        elif norm_type == 'layer':
            self.norm = nn.LayerNorm(normalized_shape, eps=eps)
        elif norm_type == 'group':
            self.norm = nn.GroupNorm(num_groups, normalized_shape, eps=eps)
        elif norm_type == 'instance':
            self.norm = nn.InstanceNorm2d(normalized_shape, eps=eps)
        else:
            raise ValueError(f"Unknown norm type: {norm_type}")
    
    def forward(self, x):
        return self.norm(x)
```

Adaptive Normalization:
```python
class AdaptiveNorm(nn.Module):
    def __init__(self, num_features):
        super().__init__()
        self.bn = nn.BatchNorm2d(num_features)
        self.ln = nn.LayerNorm(num_features)
        self.in_norm = nn.InstanceNorm2d(num_features)
        
        # Learnable weights for combining normalizations
        self.weights = nn.Parameter(torch.ones(3))
        
    def forward(self, x):
        bn_out = self.bn(x)
        
        # Reshape for LayerNorm
        b, c, h, w = x.shape
        ln_input = x.permute(0, 2, 3, 1).reshape(b * h * w, c)
        ln_out = self.ln(ln_input).reshape(b, h, w, c).permute(0, 3, 1, 2)
        
        in_out = self.in_norm(x)
        
        # Weighted combination
        weights = F.softmax(self.weights, dim=0)
        return weights[0] * bn_out + weights[1] * ln_out + weights[2] * in_out
```

8.6 Performance Optimization:
----------------------------
Memory Efficiency:
```python
# Use inplace operations where possible
def efficient_bn_relu(x):
    return F.relu(F.batch_norm(x, running_mean, running_var, weight, bias, training), inplace=True)
```

Computational Efficiency:
```python
# Fused operations
def fused_conv_bn(x, conv_weight, bn_weight, bn_bias, bn_mean, bn_var):
    # Fuse convolution and batch normalization
    # Avoid separate convolution and normalization passes
    pass
```

8.7 Migration Between Normalization Types:
-----------------------------------------
Converting Models:
```python
def convert_norm_type(model, old_norm, new_norm):
    for name, module in model.named_modules():
        if isinstance(module, old_norm):
            # Get module parameters
            num_features = module.num_features
            
            # Create new normalization
            new_module = new_norm(num_features)
            
            # Copy weights if compatible
            if hasattr(module, 'weight') and hasattr(new_module, 'weight'):
                new_module.weight.data.copy_(module.weight.data)
            if hasattr(module, 'bias') and hasattr(new_module, 'bias'):
                new_module.bias.data.copy_(module.bias.data)
            
            # Replace module
            parent = model
            attrs = name.split('.')
            for attr in attrs[:-1]:
                parent = getattr(parent, attr)
            setattr(parent, attrs[-1], new_module)
```

8.8 Best Practices Summary:
--------------------------
Selection Guidelines:
1. Start with proven defaults for your domain
2. Consider batch size and computational constraints
3. Test multiple normalization types
4. Use task-specific recommendations

Implementation:
1. Place normalization appropriately in architecture
2. Initialize normalization layers properly
3. Handle train/eval modes correctly
4. Monitor normalization statistics

Training:
1. Adjust learning rates for normalization
2. Use appropriate momentum values
3. Monitor gradient flow
4. Compare with and without normalization

Debugging:
1. Check running statistics
2. Verify train-test consistency
3. Monitor layer outputs
4. Use ablation studies

Success Guidelines:
1. Understand normalization theory and motivation
2. Choose appropriate method for task and architecture
3. Implement correctly with proper modes
4. Monitor training dynamics and statistics
5. Consider computational and memory constraints
6. Test different normalization strategies
7. Optimize for production deployment
8. Stay updated with recent advances

=======================================================
END OF DOCUMENT 