DISTRIBUTED TRAINING AND SCALING
=================================

Table of Contents:
1. Distributed Training Fundamentals
2. Data Parallelism Strategies
3. Model Parallelism Techniques
4. Distributed System Architectures
5. Communication and Synchronization
6. Framework-Specific Implementations
7. Performance Optimization
8. Best Practices and Implementation

================================================================================
1. DISTRIBUTED TRAINING FUNDAMENTALS
================================================================================

1.1 Why Distributed Training?
-----------------------------
**Scale Requirements:**
- Large datasets that don't fit in single machine memory
- Complex models requiring extensive compute resources
- Faster training times through parallelization
- Cost-effective utilization of cloud resources

**Business Drivers:**
- Reduced time-to-market for ML models
- Ability to train larger, more accurate models
- Efficient resource utilization and cost optimization
- Scalability for growing data and model complexity

1.2 Types of Parallelism
------------------------
**Data Parallelism:**
- Same model replicated across multiple devices/nodes
- Different data batches processed simultaneously
- Gradients aggregated and synchronized across replicas
- Most common and straightforward approach

**Model Parallelism:**
- Model partitioned across multiple devices/nodes
- Different parts of model execute on different devices
- Required for models too large for single device memory
- More complex communication patterns

**Pipeline Parallelism:**
- Model divided into sequential stages
- Different stages execute on different devices
- Enables training of very deep models
- Balances computation and memory requirements

**Hybrid Parallelism:**
- Combines data, model, and pipeline parallelism
- Optimizes for specific model and hardware configurations
- Maximizes resource utilization and training efficiency

1.3 Distributed Training Challenges
----------------------------------
**Communication Overhead:**
- Network bandwidth limitations
- Gradient synchronization costs
- Load balancing across nodes
- Fault tolerance and recovery

**Memory Management:**
- Efficient memory utilization across devices
- Gradient accumulation strategies
- Model state synchronization
- Dynamic memory allocation

**Consistency and Convergence:**
- Maintaining training stability
- Handling asynchronous updates
- Learning rate scaling strategies
- Reproducibility across runs

================================================================================
2. DATA PARALLELISM STRATEGIES
================================================================================

2.1 Synchronous Data Parallelism
--------------------------------
```python
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler

class DistributedTrainer:
    def __init__(self, model, train_dataset, val_dataset, config):
        self.config = config
        self.setup_distributed()
        
        # Move model to GPU and wrap with DDP
        self.model = model.to(self.device)
        self.model = DDP(self.model, device_ids=[self.local_rank])
        
        # Setup distributed samplers
        self.train_sampler = DistributedSampler(
            train_dataset,
            num_replicas=self.world_size,
            rank=self.rank,
            shuffle=True
        )
        
        self.val_sampler = DistributedSampler(
            val_dataset,
            num_replicas=self.world_size,
            rank=self.rank,
            shuffle=False
        )
        
        # Setup data loaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=config['batch_size'],
            sampler=self.train_sampler,
            num_workers=config['num_workers'],
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=config['batch_size'],
            sampler=self.val_sampler,
            num_workers=config['num_workers'],
            pin_memory=True
        )
        
        # Setup optimizer with learning rate scaling
        scaled_lr = config['learning_rate'] * self.world_size
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=scaled_lr,
            weight_decay=config['weight_decay']
        )
        
        self.criterion = nn.CrossEntropyLoss()
        
    def setup_distributed(self):
        """Initialize distributed training environment"""
        
        # Get distributed training parameters
        self.rank = int(os.environ.get('RANK', 0))
        self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
        self.world_size = int(os.environ.get('WORLD_SIZE', 1))
        
        # Initialize process group
        dist.init_process_group(
            backend='nccl',  # Use NCCL for GPU communication
            init_method='env://',
            world_size=self.world_size,
            rank=self.rank
        )
        
        # Set device
        torch.cuda.set_device(self.local_rank)
        self.device = torch.device(f'cuda:{self.local_rank}')
        
    def train_epoch(self, epoch):
        """Train one epoch with distributed training"""
        
        self.model.train()
        self.train_sampler.set_epoch(epoch)  # Ensure different shuffling each epoch
        
        total_loss = 0
        num_batches = 0
        
        for batch_idx, (data, targets) in enumerate(self.train_loader):
            data, targets = data.to(self.device), targets.to(self.device)
            
            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(data)
            loss = self.criterion(outputs, targets)
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping (optional)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # Optimizer step (gradients automatically averaged across processes)
            self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 100 == 0 and self.rank == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        # Calculate average loss across all processes
        avg_loss = total_loss / num_batches
        avg_loss_tensor = torch.tensor(avg_loss).to(self.device)
        dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)
        avg_loss = avg_loss_tensor.item() / self.world_size
        
        return avg_loss
    
    def validate(self):
        """Validation with distributed evaluation"""
        
        self.model.eval()
        total_loss = 0
        correct = 0
        total_samples = 0
        
        with torch.no_grad():
            for data, targets in self.val_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                
                outputs = self.model(data)
                loss = self.criterion(outputs, targets)
                
                total_loss += loss.item()
                
                # Calculate accuracy
                pred = outputs.argmax(dim=1, keepdim=True)
                correct += pred.eq(targets.view_as(pred)).sum().item()
                total_samples += targets.size(0)
        
        # Aggregate metrics across all processes
        metrics = torch.tensor([total_loss, correct, total_samples]).to(self.device)
        dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
        
        avg_loss = metrics[0].item() / self.world_size
        accuracy = metrics[1].item() / metrics[2].item()
        
        return avg_loss, accuracy
    
    def save_checkpoint(self, epoch, filepath):
        """Save model checkpoint (only from rank 0)"""
        
        if self.rank == 0:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': self.model.module.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'config': self.config
            }
            torch.save(checkpoint, filepath)
    
    def cleanup(self):
        """Cleanup distributed training"""
        dist.destroy_process_group()

# Usage example for launching distributed training
def launch_distributed_training():
    """Launch script for distributed training"""
    
    # Configuration
    config = {
        'batch_size': 32,
        'learning_rate': 0.001,
        'weight_decay': 1e-4,
        'num_workers': 4,
        'epochs': 100
    }
    
    # Initialize trainer
    model = YourModel()  # Your model definition
    train_dataset = YourDataset()  # Your dataset
    val_dataset = YourValidationDataset()
    
    trainer = DistributedTrainer(model, train_dataset, val_dataset, config)
    
    # Training loop
    for epoch in range(config['epochs']):
        train_loss = trainer.train_epoch(epoch)
        val_loss, val_accuracy = trainer.validate()
        
        if trainer.rank == 0:
            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, '
                  f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')
            
            # Save checkpoint
            if epoch % 10 == 0:
                trainer.save_checkpoint(epoch, f'checkpoint_epoch_{epoch}.pt')
    
    trainer.cleanup()

# Launch command (example)
"""
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 \
    --master_addr="192.168.1.1" --master_port=12355 \
    distributed_training.py
"""
```

2.2 Asynchronous Data Parallelism
---------------------------------
```python
class AsynchronousTrainer:
    def __init__(self, model, parameter_server_config):
        self.model = model
        self.ps_config = parameter_server_config
        self.worker_id = self.ps_config['worker_id']
        self.num_workers = self.ps_config['num_workers']
        
        # Connect to parameter server
        self.ps_client = ParameterServerClient(
            ps_address=self.ps_config['ps_address'],
            worker_id=self.worker_id
        )
        
    def train_async(self, train_loader, epochs):
        """Asynchronous training with parameter server"""
        
        for epoch in range(epochs):
            for batch_idx, (data, targets) in enumerate(train_loader):
                
                # Pull latest parameters from parameter server
                if batch_idx % self.ps_config['sync_frequency'] == 0:
                    latest_params = self.ps_client.pull_parameters()
                    self._update_model_parameters(latest_params)
                
                # Local forward and backward pass
                self.model.zero_grad()
                outputs = self.model(data)
                loss = F.cross_entropy(outputs, targets)
                loss.backward()
                
                # Push gradients to parameter server
                gradients = self._extract_gradients()
                self.ps_client.push_gradients(gradients, batch_idx)
                
                if batch_idx % 100 == 0:
                    print(f'Worker {self.worker_id}, Epoch {epoch}, '
                          f'Batch {batch_idx}, Loss: {loss.item():.4f}')

class ParameterServerClient:
    def __init__(self, ps_address, worker_id):
        self.ps_address = ps_address
        self.worker_id = worker_id
        self.session = requests.Session()
        
    def pull_parameters(self):
        """Pull latest parameters from parameter server"""
        
        response = self.session.get(f'{self.ps_address}/parameters')
        parameters = response.json()
        return parameters
    
    def push_gradients(self, gradients, step):
        """Push gradients to parameter server"""
        
        payload = {
            'worker_id': self.worker_id,
            'gradients': gradients,
            'step': step
        }
        
        response = self.session.post(
            f'{self.ps_address}/gradients',
            json=payload
        )
        
        return response.status_code == 200

# Gradient compression for efficient communication
class GradientCompressor:
    def __init__(self, compression_type='topk'):
        self.compression_type = compression_type
        
    def compress_gradients(self, gradients, compression_ratio=0.1):
        """Compress gradients for efficient communication"""
        
        if self.compression_type == 'topk':
            return self._topk_compression(gradients, compression_ratio)
        elif self.compression_type == 'quantization':
            return self._quantization_compression(gradients)
        else:
            return gradients
    
    def _topk_compression(self, gradients, k_ratio):
        """Top-k sparsification"""
        
        compressed_grads = {}
        
        for name, grad in gradients.items():
            # Flatten gradient
            flat_grad = grad.flatten()
            
            # Select top-k elements
            k = max(1, int(len(flat_grad) * k_ratio))
            _, top_indices = torch.topk(torch.abs(flat_grad), k)
            
            # Create sparse representation
            compressed_grads[name] = {
                'indices': top_indices.tolist(),
                'values': flat_grad[top_indices].tolist(),
                'shape': grad.shape
            }
        
        return compressed_grads
    
    def decompress_gradients(self, compressed_grads):
        """Decompress gradients"""
        
        decompressed = {}
        
        for name, comp_grad in compressed_grads.items():
            # Reconstruct gradient tensor
            grad_tensor = torch.zeros(comp_grad['shape']).flatten()
            indices = torch.tensor(comp_grad['indices'])
            values = torch.tensor(comp_grad['values'])
            grad_tensor[indices] = values
            
            decompressed[name] = grad_tensor.reshape(comp_grad['shape'])
        
        return decompressed
```

================================================================================
3. MODEL PARALLELISM TECHNIQUES
================================================================================

3.1 Layer-wise Model Parallelism
--------------------------------
```python
import torch
import torch.nn as nn
from torch.distributed.pipeline.sync import Pipe

class LayerParallelModel(nn.Module):
    def __init__(self, num_layers, layer_size, num_devices):
        super().__init__()
        
        self.num_devices = num_devices
        self.layers_per_device = num_layers // num_devices
        
        # Create model layers distributed across devices
        self.layer_groups = nn.ModuleList()
        
        for device_id in range(num_devices):
            device = torch.device(f'cuda:{device_id}')
            
            # Create layers for this device
            device_layers = nn.Sequential()
            start_layer = device_id * self.layers_per_device
            end_layer = min((device_id + 1) * self.layers_per_device, num_layers)
            
            for layer_idx in range(start_layer, end_layer):
                if layer_idx == 0:
                    # First layer
                    device_layers.add_module(
                        f'layer_{layer_idx}',
                        nn.Linear(layer_size, layer_size)
                    )
                else:
                    device_layers.add_module(
                        f'layer_{layer_idx}',
                        nn.Linear(layer_size, layer_size)
                    )
                device_layers.add_module(f'relu_{layer_idx}', nn.ReLU())
            
            # Move layers to device
            device_layers = device_layers.to(device)
            self.layer_groups.append(device_layers)
    
    def forward(self, x):
        """Forward pass across multiple devices"""
        
        for device_id, layer_group in enumerate(self.layer_groups):
            # Move input to current device
            device = torch.device(f'cuda:{device_id}')
            x = x.to(device)
            
            # Process through device layers
            x = layer_group(x)
        
        return x

# Pipeline parallelism with PyTorch Pipe
class PipelineParallelModel:
    def __init__(self, model_layers, devices, chunks=8):
        self.devices = devices
        self.chunks = chunks
        
        # Create pipeline model
        self.model = Pipe(
            model_layers,
            balance=self._calculate_balance(len(model_layers), len(devices)),
            devices=devices,
            chunks=chunks
        )
    
    def _calculate_balance(self, num_layers, num_devices):
        """Calculate layer distribution across devices"""
        
        base_layers = num_layers // num_devices
        extra_layers = num_layers % num_devices
        
        balance = [base_layers] * num_devices
        
        # Distribute extra layers
        for i in range(extra_layers):
            balance[i] += 1
        
        return balance
    
    def train_step(self, data, targets, optimizer, criterion):
        """Training step with pipeline parallelism"""
        
        # Move data to first device
        data = data.to(self.devices[0])
        targets = targets.to(self.devices[-1])
        
        optimizer.zero_grad()
        
        # Forward pass through pipeline
        outputs = self.model(data)
        
        # Calculate loss on last device
        loss = criterion(outputs, targets)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        return loss.item()

# Tensor parallelism for large models
class TensorParallelLinear(nn.Module):
    def __init__(self, in_features, out_features, num_devices):
        super().__init__()
        
        self.in_features = in_features
        self.out_features = out_features
        self.num_devices = num_devices
        
        # Split output features across devices
        self.out_features_per_device = out_features // num_devices
        
        # Create weight matrices for each device
        self.weights = nn.ParameterList()
        self.biases = nn.ParameterList()
        
        for device_id in range(num_devices):
            device = torch.device(f'cuda:{device_id}')
            
            weight = nn.Parameter(torch.randn(
                self.out_features_per_device, 
                in_features
            ).to(device))
            bias = nn.Parameter(torch.randn(
                self.out_features_per_device
            ).to(device))
            
            self.weights.append(weight)
            self.biases.append(bias)
    
    def forward(self, x):
        """Forward pass with tensor parallelism"""
        
        outputs = []
        
        for device_id in range(self.num_devices):
            device = torch.device(f'cuda:{device_id}')
            
            # Move input to device
            x_device = x.to(device)
            
            # Compute partial output
            output = F.linear(x_device, self.weights[device_id], self.biases[device_id])
            outputs.append(output)
        
        # Concatenate outputs from all devices
        final_output = torch.cat(outputs, dim=-1)
        
        return final_output

class ModelParallelTrainer:
    def __init__(self, model, devices):
        self.model = model
        self.devices = devices
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.AdamW(model.parameters(), lr=0.001)
        
    def train_epoch(self, data_loader):
        """Train one epoch with model parallelism"""
        
        self.model.train()
        total_loss = 0
        
        for batch_idx, (data, targets) in enumerate(data_loader):
            loss = self.train_step(data, targets)
            total_loss += loss
            
            if batch_idx % 100 == 0:
                print(f'Batch {batch_idx}, Loss: {loss:.4f}')
        
        return total_loss / len(data_loader)
    
    def train_step(self, data, targets):
        """Single training step"""
        
        self.optimizer.zero_grad()
        
        # Forward pass
        outputs = self.model(data)
        
        # Move targets to output device
        targets = targets.to(outputs.device)
        loss = self.criterion(outputs, targets)
        
        # Backward pass
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
```

3.2 Expert Parallelism (Mixture of Experts)
-------------------------------------------
```python
class MixtureOfExperts(nn.Module):
    def __init__(self, input_dim, expert_dim, num_experts, num_devices, top_k=2):
        super().__init__()
        
        self.num_experts = num_experts
        self.num_devices = num_devices
        self.top_k = top_k
        self.experts_per_device = num_experts // num_devices
        
        # Gating network (shared across all devices)
        self.gate = nn.Linear(input_dim, num_experts)
        
        # Expert networks distributed across devices
        self.experts = nn.ModuleList()
        
        for device_id in range(num_devices):
            device = torch.device(f'cuda:{device_id}')
            device_experts = nn.ModuleList()
            
            for _ in range(self.experts_per_device):
                expert = nn.Sequential(
                    nn.Linear(input_dim, expert_dim),
                    nn.ReLU(),
                    nn.Linear(expert_dim, input_dim)
                ).to(device)
                device_experts.append(expert)
            
            self.experts.append(device_experts)
    
    def forward(self, x):
        """Forward pass through mixture of experts"""
        
        batch_size = x.shape[0]
        
        # Compute gating scores
        gate_logits = self.gate(x)
        gate_scores = F.softmax(gate_logits, dim=-1)
        
        # Select top-k experts
        top_k_scores, top_k_indices = torch.topk(gate_scores, self.top_k, dim=-1)
        
        # Normalize top-k scores
        top_k_scores = top_k_scores / top_k_scores.sum(dim=-1, keepdim=True)
        
        # Initialize output
        output = torch.zeros_like(x)
        
        # Process through selected experts
        for batch_idx in range(batch_size):
            for k in range(self.top_k):
                expert_idx = top_k_indices[batch_idx, k].item()
                score = top_k_scores[batch_idx, k]
                
                # Determine which device the expert is on
                device_id = expert_idx // self.experts_per_device
                local_expert_idx = expert_idx % self.experts_per_device
                
                # Move input to expert device
                device = torch.device(f'cuda:{device_id}')
                x_device = x[batch_idx:batch_idx+1].to(device)
                
                # Process through expert
                expert_output = self.experts[device_id][local_expert_idx](x_device)
                
                # Move output back and accumulate
                expert_output = expert_output.to(x.device)
                output[batch_idx:batch_idx+1] += score * expert_output
        
        return output

# Dynamic expert routing for load balancing
class DynamicExpertRouter:
    def __init__(self, num_experts, num_devices):
        self.num_experts = num_experts
        self.num_devices = num_devices
        self.expert_loads = torch.zeros(num_experts)
        self.device_loads = torch.zeros(num_devices)
        
    def route_to_experts(self, gate_scores, top_k=2):
        """Route inputs to experts with load balancing"""
        
        batch_size = gate_scores.shape[0]
        routing_decisions = []
        
        # Get top-k experts for each sample
        top_k_scores, top_k_indices = torch.topk(gate_scores, top_k, dim=-1)
        
        for batch_idx in range(batch_size):
            sample_routing = []
            
            for k in range(top_k):
                expert_idx = top_k_indices[batch_idx, k].item()
                score = top_k_scores[batch_idx, k].item()
                
                # Check if expert is overloaded
                if self.expert_loads[expert_idx] > self._get_load_threshold():
                    # Find alternative expert with lower load
                    alternative_expert = self._find_alternative_expert(expert_idx)
                    if alternative_expert is not None:
                        expert_idx = alternative_expert
                
                # Update load tracking
                self.expert_loads[expert_idx] += 1
                device_id = expert_idx // (self.num_experts // self.num_devices)
                self.device_loads[device_id] += 1
                
                sample_routing.append({
                    'expert_idx': expert_idx,
                    'score': score,
                    'device_id': device_id
                })
            
            routing_decisions.append(sample_routing)
        
        return routing_decisions
    
    def _get_load_threshold(self):
        """Calculate load threshold for expert balancing"""
        avg_load = self.expert_loads.mean()
        return avg_load * 1.5  # Allow 50% above average
    
    def _find_alternative_expert(self, original_expert):
        """Find alternative expert with lower load"""
        
        # Find experts with load below threshold
        low_load_experts = torch.where(
            self.expert_loads < self._get_load_threshold()
        )[0]
        
        if len(low_load_experts) > 0:
            # Return expert with lowest load
            return low_load_experts[self.expert_loads[low_load_experts].argmin()].item()
        
        return None
    
    def reset_loads(self):
        """Reset load tracking (call periodically)"""
        self.expert_loads.zero_()
        self.device_loads.zero_()
```

================================================================================
4. DISTRIBUTED SYSTEM ARCHITECTURES
================================================================================

4.1 Parameter Server Architecture
---------------------------------
```python
class ParameterServer:
    def __init__(self, model_params, num_workers, learning_rate=0.001):
        self.parameters = model_params
        self.num_workers = num_workers
        self.learning_rate = learning_rate
        
        # Gradient accumulation
        self.gradient_buffer = {}
        self.update_counts = {}
        
        # Asynchronous training state
        self.global_step = 0
        self.staleness_threshold = 10
        
        # Initialize gradient buffers
        for name, param in self.parameters.items():
            self.gradient_buffer[name] = torch.zeros_like(param)
            self.update_counts[name] = 0
    
    def push_gradients(self, worker_id, gradients, worker_step):
        """Receive gradients from worker"""
        
        # Check staleness
        staleness = self.global_step - worker_step
        if staleness > self.staleness_threshold:
            print(f"Rejecting stale gradients from worker {worker_id}, "
                  f"staleness: {staleness}")
            return False
        
        # Apply staleness compensation
        staleness_factor = 1.0 / (1.0 + staleness * 0.1)
        
        # Accumulate gradients
        for name, grad in gradients.items():
            if name in self.gradient_buffer:
                self.gradient_buffer[name] += grad * staleness_factor
                self.update_counts[name] += 1
        
        # Update parameters if enough gradients received
        self._maybe_update_parameters()
        
        return True
    
    def _maybe_update_parameters(self):
        """Update parameters when sufficient gradients accumulated"""
        
        # Check if we have gradients from enough workers
        min_updates = min(self.update_counts.values()) if self.update_counts else 0
        
        if min_updates >= max(1, self.num_workers // 2):  # At least half the workers
            self._apply_updates()
            self._reset_gradients()
            self.global_step += 1
    
    def _apply_updates(self):
        """Apply accumulated gradients to parameters"""
        
        for name, param in self.parameters.items():
            if name in self.gradient_buffer:
                # Average gradients
                avg_gradient = self.gradient_buffer[name] / self.update_counts[name]
                
                # Apply update
                param.data -= self.learning_rate * avg_gradient
    
    def _reset_gradients(self):
        """Reset gradient accumulation"""
        
        for name in self.gradient_buffer:
            self.gradient_buffer[name].zero_()
            self.update_counts[name] = 0
    
    def pull_parameters(self, worker_id):
        """Send current parameters to worker"""
        
        return {
            'parameters': self.parameters.copy(),
            'global_step': self.global_step
        }

class ParameterServerWorker:
    def __init__(self, worker_id, model, ps_address):
        self.worker_id = worker_id
        self.model = model
        self.ps_client = ParameterServerClient(ps_address, worker_id)
        self.local_step = 0
        
    def train_batch(self, data, targets):
        """Train on local batch and sync with parameter server"""
        
        # Pull latest parameters periodically
        if self.local_step % 10 == 0:
            self._sync_parameters()
        
        # Local forward and backward pass
        self.model.zero_grad()
        outputs = self.model(data)
        loss = F.cross_entropy(outputs, targets)
        loss.backward()
        
        # Extract gradients
        gradients = {}
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                gradients[name] = param.grad.clone()
        
        # Push gradients to parameter server
        success = self.ps_client.push_gradients(gradients, self.local_step)
        
        if success:
            self.local_step += 1
        
        return loss.item()
    
    def _sync_parameters(self):
        """Synchronize local parameters with parameter server"""
        
        response = self.ps_client.pull_parameters()
        
        if 'parameters' in response:
            # Update local model parameters
            for name, param in self.model.named_parameters():
                if name in response['parameters']:
                    param.data.copy_(response['parameters'][name])
            
            # Update local step to match global step
            self.local_step = response.get('global_step', self.local_step)

# Hierarchical parameter server for large scale
class HierarchicalParameterServer:
    def __init__(self, num_shards, shard_size):
        self.num_shards = num_shards
        self.shard_size = shard_size
        self.shards = []
        
        # Create parameter server shards
        for shard_id in range(num_shards):
            shard = ParameterServerShard(shard_id, shard_size)
            self.shards.append(shard)
        
        # Coordinator for inter-shard communication
        self.coordinator = ParameterServerCoordinator(self.shards)
    
    def route_request(self, parameter_name, request_type, **kwargs):
        """Route request to appropriate shard"""
        
        # Determine shard based on parameter name hash
        shard_id = hash(parameter_name) % self.num_shards
        shard = self.shards[shard_id]
        
        if request_type == 'push_gradient':
            return shard.push_gradient(parameter_name, **kwargs)
        elif request_type == 'pull_parameter':
            return shard.pull_parameter(parameter_name)
        else:
            raise ValueError(f"Unknown request type: {request_type}")
```

4.2 Ring AllReduce Architecture
------------------------------
```python
class RingAllReduce:
    def __init__(self, world_size, rank, device):
        self.world_size = world_size
        self.rank = rank
        self.device = device
        
        # Setup communication ring
        self.left_neighbor = (rank - 1) % world_size
        self.right_neighbor = (rank + 1) % world_size
    
    def allreduce(self, tensor):
        """Perform ring allreduce on tensor"""
        
        # Divide tensor into chunks for each rank
        chunk_size = tensor.numel() // self.world_size
        chunks = tensor.split(chunk_size)
        
        # Ensure we have exactly world_size chunks
        if len(chunks) > self.world_size:
            # Merge last chunks if necessary
            last_chunk = torch.cat(chunks[self.world_size-1:])
            chunks = chunks[:self.world_size-1] + (last_chunk,)
        
        # Phase 1: Reduce-scatter
        for step in range(self.world_size - 1):
            # Determine which chunk to send/receive
            send_chunk_idx = (self.rank - step) % self.world_size
            recv_chunk_idx = (self.rank - step - 1) % self.world_size
            
            # Send chunk to right neighbor
            send_tensor = chunks[send_chunk_idx].clone()
            
            # Receive chunk from left neighbor (simulated)
            recv_tensor = self._communicate_with_neighbor(
                send_tensor, self.right_neighbor, self.left_neighbor
            )
            
            # Add received chunk to local chunk
            chunks = list(chunks)
            chunks[recv_chunk_idx] = chunks[recv_chunk_idx] + recv_tensor
            chunks = tuple(chunks)
        
        # Phase 2: All-gather
        for step in range(self.world_size - 1):
            # Determine which chunk to send/receive
            send_chunk_idx = (self.rank - step + 1) % self.world_size
            recv_chunk_idx = (self.rank - step) % self.world_size
            
            # Send chunk to right neighbor
            send_tensor = chunks[send_chunk_idx].clone()
            
            # Receive chunk from left neighbor
            recv_tensor = self._communicate_with_neighbor(
                send_tensor, self.right_neighbor, self.left_neighbor
            )
            
            # Replace local chunk with received chunk
            chunks = list(chunks)
            chunks[recv_chunk_idx] = recv_tensor
            chunks = tuple(chunks)
        
        # Reconstruct final tensor
        result_tensor = torch.cat(chunks)
        return result_tensor
    
    def _communicate_with_neighbor(self, send_tensor, send_to, receive_from):
        """Simulate communication with neighbor (in practice use actual communication)"""
        
        # In real implementation, this would use MPI, NCCL, or other communication backend
        # For simulation, we'll just return the send_tensor (as if received from neighbor)
        return send_tensor.clone()

class OptimizedAllReduce:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
        
    def hierarchical_allreduce(self, tensor, node_groups):
        """Hierarchical allreduce for multi-node clusters"""
        
        # Phase 1: Intra-node allreduce
        intra_node_result = self._intra_node_allreduce(tensor, node_groups)
        
        # Phase 2: Inter-node allreduce (only one rank per node participates)
        if self._is_node_representative(node_groups):
            inter_node_result = self._inter_node_allreduce(intra_node_result, node_groups)
        else:
            inter_node_result = intra_node_result
        
        # Phase 3: Broadcast result within node
        final_result = self._intra_node_broadcast(inter_node_result, node_groups)
        
        return final_result
    
    def _intra_node_allreduce(self, tensor, node_groups):
        """Allreduce within node using high-bandwidth interconnect"""
        
        # Use NCCL or similar for GPU-to-GPU communication within node
        # This is typically much faster than inter-node communication
        
        node_id = self._get_node_id(node_groups)
        ranks_in_node = node_groups[node_id]
        
        # Perform allreduce among ranks in same node
        if len(ranks_in_node) > 1:
            # Use optimized intra-node communication
            result = self._fast_intra_node_reduce(tensor, ranks_in_node)
        else:
            result = tensor.clone()
        
        return result
    
    def _inter_node_allreduce(self, tensor, node_groups):
        """Allreduce between nodes using network"""
        
        # Only node representatives participate
        num_nodes = len(node_groups)
        
        if num_nodes > 1:
            # Use ring allreduce or tree allreduce for inter-node communication
            result = self._network_allreduce(tensor, num_nodes)
        else:
            result = tensor.clone()
        
        return result
    
    def _adaptive_allreduce(self, tensor, bandwidth_info):
        """Adaptive allreduce based on network conditions"""
        
        tensor_size = tensor.numel() * tensor.element_size()
        available_bandwidth = bandwidth_info['bandwidth_mbps']
        
        # Choose algorithm based on tensor size and bandwidth
        if tensor_size < 1024 * 1024:  # < 1MB
            # Use tree allreduce for small tensors
            return self._tree_allreduce(tensor)
        elif available_bandwidth > 10000:  # > 10 Gbps
            # Use ring allreduce for high bandwidth
            return self.allreduce(tensor)
        else:
            # Use hierarchical allreduce for low bandwidth
            return self.hierarchical_allreduce(tensor, bandwidth_info['node_groups'])

# Gradient compression for communication efficiency
class CommunicationOptimizer:
    def __init__(self, compression_ratio=0.1):
        self.compression_ratio = compression_ratio
        
    def compress_and_communicate(self, gradients):
        """Compress gradients before communication"""
        
        compressed_gradients = {}
        
        for name, grad in gradients.items():
            # Apply compression
            compressed = self._compress_gradient(grad)
            compressed_gradients[name] = compressed
        
        # Communicate compressed gradients
        communicated_gradients = self._communicate_gradients(compressed_gradients)
        
        # Decompress received gradients
        decompressed_gradients = {}
        for name, compressed in communicated_gradients.items():
            decompressed = self._decompress_gradient(compressed)
            decompressed_gradients[name] = decompressed
        
        return decompressed_gradients
    
    def _compress_gradient(self, gradient):
        """Apply gradient compression"""
        
        # Top-k sparsification
        flat_grad = gradient.flatten()
        k = max(1, int(len(flat_grad) * self.compression_ratio))
        
        # Find top-k elements by magnitude
        _, top_indices = torch.topk(torch.abs(flat_grad), k)
        
        compressed = {
            'indices': top_indices,
            'values': flat_grad[top_indices],
            'shape': gradient.shape,
            'original_norm': torch.norm(gradient).item()
        }
        
        return compressed
    
    def _decompress_gradient(self, compressed):
        """Decompress gradient"""
        
        # Reconstruct sparse gradient
        gradient = torch.zeros(compressed['shape']).flatten()
        gradient[compressed['indices']] = compressed['values']
        gradient = gradient.reshape(compressed['shape'])
        
        # Apply error compensation
        current_norm = torch.norm(gradient).item()
        original_norm = compressed['original_norm']
        
        if current_norm > 0:
            scale_factor = original_norm / current_norm
            gradient *= scale_factor
        
        return gradient
```

================================================================================
5. COMMUNICATION AND SYNCHRONIZATION
================================================================================

5.1 Communication Backends
--------------------------
```python
import torch.distributed as dist

class CommunicationManager:
    def __init__(self, backend='nccl', world_size=None, rank=None):
        self.backend = backend
        self.world_size = world_size or int(os.environ.get('WORLD_SIZE', 1))
        self.rank = rank or int(os.environ.get('RANK', 0))
        
        # Initialize process group
        dist.init_process_group(
            backend=backend,
            world_size=self.world_size,
            rank=self.rank
        )
        
        # Communication patterns
        self.communication_patterns = {
            'allreduce': self._allreduce,
            'allgather': self._allgather,
            'broadcast': self._broadcast,
            'reduce': self._reduce,
            'scatter': self._scatter,
            'gather': self._gather
        }
    
    def _allreduce(self, tensor, op=dist.ReduceOp.SUM, async_op=False):
        """All-reduce operation"""
        
        handle = dist.all_reduce(tensor, op=op, async_op=async_op)
        
        if not async_op:
            # Divide by world size to get average
            if op == dist.ReduceOp.SUM:
                tensor.div_(self.world_size)
        
        return handle
    
    def _allgather(self, tensor, async_op=False):
        """All-gather operation"""
        
        # Create tensor list for gathering
        tensor_list = [torch.zeros_like(tensor) for _ in range(self.world_size)]
        
        handle = dist.all_gather(tensor_list, tensor, async_op=async_op)
        
        if not async_op:
            return tensor_list
        
        return handle, tensor_list
    
    def _broadcast(self, tensor, src=0, async_op=False):
        """Broadcast operation"""
        
        handle = dist.broadcast(tensor, src=src, async_op=async_op)
        return handle
    
    def synchronized_operation(self, operation, *args, **kwargs):
        """Execute operation with synchronization"""
        
        # Synchronize before operation
        dist.barrier()
        
        start_time = time.time()
        result = operation(*args, **kwargs)
        end_time = time.time()
        
        # Synchronize after operation
        dist.barrier()
        
        # Collect timing information
        timing_tensor = torch.tensor(end_time - start_time).to(f'cuda:{self.rank}')
        dist.all_reduce(timing_tensor, op=dist.ReduceOp.SUM)
        avg_time = timing_tensor.item() / self.world_size
        
        if self.rank == 0:
            print(f"Operation completed in {avg_time:.4f} seconds (average)")
        
        return result

class AsynchronousCommunicator:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
        self.pending_operations = {}
        self.operation_counter = 0
        
    def async_allreduce(self, tensor, tag=None):
        """Asynchronous all-reduce"""
        
        if tag is None:
            tag = f"allreduce_{self.operation_counter}"
            self.operation_counter += 1
        
        # Start asynchronous all-reduce
        handle = dist.all_reduce(tensor, async_op=True)
        
        # Store handle for later completion
        self.pending_operations[tag] = {
            'handle': handle,
            'tensor': tensor,
            'operation': 'allreduce'
        }
        
        return tag
    
    def wait_for_operation(self, tag):
        """Wait for specific operation to complete"""
        
        if tag in self.pending_operations:
            op_info = self.pending_operations[tag]
            
            # Wait for completion
            op_info['handle'].wait()
            
            # Post-process if needed
            if op_info['operation'] == 'allreduce':
                op_info['tensor'].div_(self.world_size)
            
            # Remove from pending operations
            del self.pending_operations[tag]
            
            return op_info['tensor']
        
        return None
    
    def wait_for_all(self):
        """Wait for all pending operations"""
        
        for tag in list(self.pending_operations.keys()):
            self.wait_for_operation(tag)
    
    def overlapped_compute_communication(self, model, data, targets, optimizer):
        """Overlap computation with communication"""
        
        # Forward pass
        outputs = model(data)
        loss = F.cross_entropy(outputs, targets)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        
        # Start asynchronous gradient communication
        communication_handles = []
        
        for name, param in model.named_parameters():
            if param.grad is not None:
                # Start async allreduce for each gradient
                handle = self.async_allreduce(param.grad.data, tag=f"grad_{name}")
                communication_handles.append(handle)
        
        # Perform other computations while communication happens
        # (e.g., logging, metrics calculation)
        
        # Wait for all communications to complete
        for handle in communication_handles:
            self.wait_for_operation(handle)
        
        # Apply optimizer step
        optimizer.step()
        
        return loss.item()

# Fault-tolerant communication
class FaultTolerantCommunicator:
    def __init__(self, world_size, rank, timeout=30):
        self.world_size = world_size
        self.rank = rank
        self.timeout = timeout
        self.failed_ranks = set()
        
    def resilient_allreduce(self, tensor, max_retries=3):
        """Fault-tolerant all-reduce with retries"""
        
        for attempt in range(max_retries):
            try:
                # Check for failed ranks
                active_ranks = self._get_active_ranks()
                
                if len(active_ranks) < self.world_size // 2:
                    raise RuntimeError("Too many failed ranks for consensus")
                
                # Perform all-reduce with timeout
                with self._timeout_context():
                    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
                    tensor.div_(len(active_ranks))
                
                return tensor
                
            except (RuntimeError, TimeoutError) as e:
                print(f"Communication failed on attempt {attempt + 1}: {e}")
                
                if attempt < max_retries - 1:
                    # Update failed ranks and retry
                    self._update_failed_ranks()
                    time.sleep(1.0)  # Brief pause before retry
                else:
                    raise e
        
        raise RuntimeError("All communication attempts failed")
    
    def _get_active_ranks(self):
        """Get list of currently active ranks"""
        
        all_ranks = set(range(self.world_size))
        active_ranks = all_ranks - self.failed_ranks
        return list(active_ranks)
    
    def _update_failed_ranks(self):
        """Update list of failed ranks"""
        
        # Attempt to detect failed ranks through heartbeat or other mechanism
        # This is a simplified implementation
        
        for rank in range(self.world_size):
            if rank != self.rank and rank not in self.failed_ranks:
                try:
                    # Try to communicate with rank
                    test_tensor = torch.tensor(1.0).to(f'cuda:{self.rank}')
                    dist.send(test_tensor, dst=rank)
                except:
                    self.failed_ranks.add(rank)
                    print(f"Detected failed rank: {rank}")
    
    def _timeout_context(self):
        """Context manager for operation timeout"""
        
        return TimeoutContext(self.timeout)

class TimeoutContext:
    def __init__(self, timeout):
        self.timeout = timeout
        
    def __enter__(self):
        # Set up timeout mechanism
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        # Clean up timeout mechanism
        pass
```

================================================================================
6. FRAMEWORK-SPECIFIC IMPLEMENTATIONS
================================================================================

6.1 PyTorch Distributed Training
--------------------------------
```python
# Complete PyTorch distributed training setup
import torch.multiprocessing as mp
from torch.distributed import init_process_group, destroy_process_group

def setup_distributed(rank, world_size, backend='nccl'):
    """Setup distributed training environment"""
    
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    init_process_group(backend, rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup_distributed():
    """Cleanup distributed training"""
    destroy_process_group()

def train_distributed(rank, world_size, model_class, train_dataset, config):
    """Main distributed training function"""
    
    # Setup distributed environment
    setup_distributed(rank, world_size)
    
    # Create model and move to GPU
    model = model_class(**config['model_params'])
    model = model.to(rank)
    model = DDP(model, device_ids=[rank])
    
    # Setup data loading
    sampler = DistributedSampler(
        train_dataset, 
        num_replicas=world_size, 
        rank=rank,
        shuffle=True
    )
    
    dataloader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        sampler=sampler,
        num_workers=config['num_workers'],
        pin_memory=True
    )
    
    # Setup optimizer and loss
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'] * world_size,  # Scale learning rate
        weight_decay=config['weight_decay']
    )
    
    criterion = nn.CrossEntropyLoss()
    
    # Training loop
    for epoch in range(config['epochs']):
        sampler.set_epoch(epoch)  # Important for proper shuffling
        
        model.train()
        total_loss = 0
        
        for batch_idx, (data, targets) in enumerate(dataloader):
            data, targets = data.to(rank), targets.to(rank)
            
            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, targets)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            total_loss += loss.item()
            
            if batch_idx % 100 == 0 and rank == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        # Save checkpoint from rank 0
        if rank == 0 and epoch % 10 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.module.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': total_loss / len(dataloader)
            }, f'checkpoint_epoch_{epoch}.pt')
    
    cleanup_distributed()

def launch_distributed_training(model_class, train_dataset, config, world_size=4):
    """Launch distributed training across multiple processes"""
    
    mp.spawn(
        train_distributed,
        args=(world_size, model_class, train_dataset, config),
        nprocs=world_size,
        join=True
    )

# Advanced PyTorch distributed features
class AdvancedDistributedTrainer:
    def __init__(self, model, world_size, rank):
        self.model = model
        self.world_size = world_size
        self.rank = rank
        
        # Advanced DDP configuration
        self.model = DDP(
            model,
            device_ids=[rank],
            output_device=rank,
            find_unused_parameters=True,  # For models with conditional execution
            gradient_as_bucket_view=True,  # Memory optimization
            static_graph=False  # Set to True if graph structure is static
        )
        
        # Communication hooks for custom behavior
        self.model.register_comm_hook(None, self._gradient_compression_hook)
    
    def _gradient_compression_hook(self, state, bucket):
        """Custom communication hook for gradient compression"""
        
        # Compress gradients before communication
        compressed_tensor = self._compress_gradients(bucket.buffer())
        
        # Perform all-reduce on compressed tensor
        fut = dist.all_reduce(compressed_tensor, async_op=True).get_future()
        
        # Decompress after communication
        return fut.then(lambda fut: self._decompress_gradients(fut.value()[0]))
    
    def _compress_gradients(self, tensor):
        """Compress gradients for efficient communication"""
        
        # Example: Top-k sparsification
        k = max(1, int(tensor.numel() * 0.1))  # Keep top 10%
        _, top_indices = torch.topk(torch.abs(tensor.flatten()), k)
        
        compressed = torch.zeros_like(tensor.flatten())
        compressed[top_indices] = tensor.flatten()[top_indices]
        
        return compressed.reshape(tensor.shape)
    
    def _decompress_gradients(self, tensor):
        """Decompress gradients after communication"""
        
        # Apply error correction or scaling if needed
        return tensor / self.world_size
    
    def train_with_gradient_accumulation(self, dataloader, accumulation_steps=4):
        """Training with gradient accumulation for large effective batch sizes"""
        
        self.model.train()
        optimizer = optim.AdamW(self.model.parameters(), lr=0.001)
        
        for batch_idx, (data, targets) in enumerate(dataloader):
            data, targets = data.to(self.rank), targets.to(self.rank)
            
            # Scale loss by accumulation steps
            with self.model.no_sync() if (batch_idx + 1) % accumulation_steps != 0 else nullcontext():
                outputs = self.model(data)
                loss = F.cross_entropy(outputs, targets) / accumulation_steps
                loss.backward()
            
            # Update weights every accumulation_steps
            if (batch_idx + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
```

6.2 TensorFlow Distributed Training
----------------------------------
```python
import tensorflow as tf
from tensorflow.keras import mixed_precision

class TensorFlowDistributedTrainer:
    def __init__(self, strategy_type='mirrored', num_gpus=None):
        
        # Configure mixed precision for better performance
        mixed_precision.set_global_policy('mixed_float16')
        
        # Create distribution strategy
        if strategy_type == 'mirrored':
            if num_gpus:
                devices = [f'/gpu:{i}' for i in range(num_gpus)]
                self.strategy = tf.distribute.MirroredStrategy(devices=devices)
            else:
                self.strategy = tf.distribute.MirroredStrategy()
                
        elif strategy_type == 'multiworker':
            self.strategy = tf.distribute.MultiWorkerMirroredStrategy()
            
        elif strategy_type == 'tpu':
            resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
            tf.config.experimental_connect_to_cluster(resolver)
            tf.tpu.experimental.initialize_tpu_system(resolver)
            self.strategy = tf.distribute.TPUStrategy(resolver)
        
        self.num_replicas = self.strategy.num_replicas_in_sync
        
    def create_distributed_dataset(self, dataset, batch_size):
        """Create distributed dataset"""
        
        # Scale batch size by number of replicas
        global_batch_size = batch_size * self.num_replicas
        
        # Distribute dataset
        distributed_dataset = self.strategy.experimental_distribute_dataset(
            dataset.batch(global_batch_size)
        )
        
        return distributed_dataset
    
    def create_model(self, model_fn):
        """Create model within distribution strategy scope"""
        
        with self.strategy.scope():
            model = model_fn()
            
            # Scale learning rate
            base_learning_rate = 0.001
            learning_rate = base_learning_rate * self.num_replicas
            
            # Create optimizer
            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
            
            # Wrap optimizer for mixed precision
            optimizer = mixed_precision.LossScaleOptimizer(optimizer)
            
            # Compile model
            model.compile(
                optimizer=optimizer,
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy']
            )
            
            return model
    
    def train_step(self, model, batch):
        """Custom training step for distributed training"""
        
        def step_fn(inputs):
            images, labels = inputs
            
            with tf.GradientTape() as tape:
                predictions = model(images, training=True)
                loss = tf.keras.losses.sparse_categorical_crossentropy(
                    labels, predictions, from_logits=True
                )
                
                # Scale loss for mixed precision
                scaled_loss = model.optimizer.get_scaled_loss(loss)
            
            # Calculate gradients
            scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)
            gradients = model.optimizer.get_unscaled_gradients(scaled_gradients)
            
            # Apply gradients
            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
            
            return loss
        
        # Distribute training step across replicas
        per_replica_losses = self.strategy.run(step_fn, args=(batch,))
        
        # Reduce losses across replicas
        return self.strategy.reduce(
            tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None
        )
    
    def train_distributed(self, model, train_dataset, val_dataset, epochs):
        """Complete distributed training loop"""
        
        # Training metrics
        train_loss = tf.keras.metrics.Mean(name='train_loss')
        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
        
        # Training loop
        for epoch in range(epochs):
            # Reset metrics
            train_loss.reset_states()
            train_accuracy.reset_states()
            
            # Training epoch
            for batch in train_dataset:
                loss = self.train_step(model, batch)
                train_loss.update_state(loss)
                
                # Update accuracy
                images, labels = batch
                predictions = model(images, training=False)
                train_accuracy.update_state(labels, predictions)
            
            # Validation
            val_loss, val_accuracy = model.evaluate(val_dataset, verbose=0)
            
            print(f'Epoch {epoch + 1}, '
                  f'Train Loss: {train_loss.result():.4f}, '
                  f'Train Accuracy: {train_accuracy.result():.4f}, '
                  f'Val Loss: {val_loss:.4f}, '
                  f'Val Accuracy: {val_accuracy:.4f}')
            
            # Save checkpoint
            if epoch % 10 == 0:
                model.save_weights(f'checkpoint_epoch_{epoch}.h5')

# Multi-worker distributed training setup
def setup_multiworker_training():
    """Setup for multi-worker distributed training"""
    
    # Configure cluster
    tf_config = {
        'cluster': {
            'worker': ['localhost:12345', 'localhost:12346']
        },
        'task': {
            'type': 'worker',
            'index': 0  # Set appropriately for each worker
        }
    }
    
    os.environ['TF_CONFIG'] = json.dumps(tf_config)
    
    # Create strategy
    strategy = tf.distribute.MultiWorkerMirroredStrategy(
        communication_options=tf.distribute.experimental.CommunicationOptions(
            implementation=tf.distribute.experimental.CommunicationImplementation.NCCL
        )
    )
    
    return strategy

# Custom training loop with strategy
class CustomDistributedTraining:
    def __init__(self, strategy):
        self.strategy = strategy
        
    @tf.function
    def distributed_train_step(self, model, optimizer, batch):
        """Distributed training step with tf.function"""
        
        def train_step(inputs):
            images, labels = inputs
            
            with tf.GradientTape() as tape:
                logits = model(images, training=True)
                
                # Calculate loss
                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(
                    logits=logits, labels=labels
                )
                loss = tf.reduce_mean(cross_entropy)
                
                # Add regularization losses
                loss += tf.add_n(model.losses)
            
            # Calculate gradients
            gradients = tape.gradient(loss, model.trainable_variables)
            
            # Apply gradients
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))
            
            return loss
        
        # Run on all replicas
        per_replica_losses = self.strategy.run(train_step, args=(batch,))
        
        # Reduce across replicas
        return self.strategy.reduce(
            tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None
        )
```

================================================================================
7. PERFORMANCE OPTIMIZATION
================================================================================

7.1 Communication Optimization
------------------------------
```python
class CommunicationOptimizer:
    def __init__(self, world_size, compression_enabled=True):
        self.world_size = world_size
        self.compression_enabled = compression_enabled
        self.communication_stats = {
            'total_bytes_sent': 0,
            'total_bytes_compressed': 0,
            'compression_time': 0,
            'communication_time': 0
        }
        
    def optimize_gradient_communication(self, gradients):
        """Optimize gradient communication using various techniques"""
        
        start_time = time.time()
        
        # 1. Gradient compression
        if self.compression_enabled:
            compressed_grads = self._compress_gradients(gradients)
            self.communication_stats['compression_time'] += time.time() - start_time
        else:
            compressed_grads = gradients
        
        # 2. Communication scheduling
        comm_start = time.time()
        communicated_grads = self._scheduled_communication(compressed_grads)
        self.communication_stats['communication_time'] += time.time() - comm_start
        
        # 3. Decompression
        if self.compression_enabled:
            final_grads = self._decompress_gradients(communicated_grads)
        else:
            final_grads = communicated_grads
        
        return final_grads
    
    def _compress_gradients(self, gradients):
        """Multi-level gradient compression"""
        
        compressed = {}
        
        for name, grad in gradients.items():
            original_size = grad.numel() * grad.element_size()
            self.communication_stats['total_bytes_sent'] += original_size
            
            # Choose compression method based on gradient characteristics
            if grad.numel() > 10000:  # Large gradients
                compressed[name] = self._topk_compression(grad, k_ratio=0.1)
            elif torch.std(grad) < 0.001:  # Low variance gradients
                compressed[name] = self._quantization_compression(grad, bits=8)
            else:  # Default compression
                compressed[name] = self._threshold_compression(grad, threshold=0.001)
            
            compressed_size = self._estimate_compressed_size(compressed[name])
            self.communication_stats['total_bytes_compressed'] += compressed_size
        
        return compressed
    
    def _topk_compression(self, gradient, k_ratio=0.1):
        """Top-k sparsification with error feedback"""
        
        flat_grad = gradient.flatten()
        k = max(1, int(len(flat_grad) * k_ratio))
        
        # Find top-k elements
        _, top_indices = torch.topk(torch.abs(flat_grad), k)
        
        # Create sparse representation
        compressed = {
            'type': 'topk',
            'indices': top_indices,
            'values': flat_grad[top_indices],
            'shape': gradient.shape,
            'k': k
        }
        
        return compressed
    
    def _quantization_compression(self, gradient, bits=8):
        """Gradient quantization"""
        
        # Calculate quantization parameters
        min_val = torch.min(gradient)
        max_val = torch.max(gradient)
        
        # Quantize to specified bit width
        num_levels = 2 ** bits
        scale = (max_val - min_val) / (num_levels - 1)
        
        # Quantize
        quantized = torch.round((gradient - min_val) / scale)
        quantized = quantized.clamp(0, num_levels - 1)
        
        compressed = {
            'type': 'quantization',
            'quantized_values': quantized.to(torch.uint8),
            'min_val': min_val,
            'max_val': max_val,
            'shape': gradient.shape,
            'bits': bits
        }
        
        return compressed
    
    def _scheduled_communication(self, gradients):
        """Schedule communication to minimize network congestion"""
        
        # Prioritize smaller gradients first
        sorted_grads = sorted(
            gradients.items(),
            key=lambda x: self._estimate_compressed_size(x[1])
        )
        
        communicated = {}
        
        for name, grad in sorted_grads:
            # Simulate communication (in practice, use actual distributed ops)
            communicated[name] = self._communicate_gradient(grad)
        
        return communicated
    
    def _communicate_gradient(self, gradient):
        """Simulate gradient communication"""
        
        # In real implementation, this would use distributed communication
        # For now, just return the gradient (simulating perfect communication)
        return gradient
    
    def _decompress_gradients(self, compressed_grads):
        """Decompress received gradients"""
        
        decompressed = {}
        
        for name, compressed in compressed_grads.items():
            if compressed['type'] == 'topk':
                decompressed[name] = self._decompress_topk(compressed)
            elif compressed['type'] == 'quantization':
                decompressed[name] = self._decompress_quantization(compressed)
            else:
                decompressed[name] = compressed
        
        return decompressed
    
    def _decompress_topk(self, compressed):
        """Decompress top-k compressed gradient"""
        
        # Reconstruct sparse gradient
        gradient = torch.zeros(compressed['shape']).flatten()
        gradient[compressed['indices']] = compressed['values']
        
        return gradient.reshape(compressed['shape'])
    
    def _decompress_quantization(self, compressed):
        """Decompress quantized gradient"""
        
        # Dequantize
        scale = (compressed['max_val'] - compressed['min_val']) / (2 ** compressed['bits'] - 1)
        
        dequantized = compressed['quantized_values'].float() * scale + compressed['min_val']
        
        return dequantized.reshape(compressed['shape'])
    
    def get_communication_stats(self):
        """Get communication statistics"""
        
        total_sent = self.communication_stats['total_bytes_sent']
        total_compressed = self.communication_stats['total_bytes_compressed']
        
        compression_ratio = total_compressed / total_sent if total_sent > 0 else 1.0
        
        return {
            'compression_ratio': compression_ratio,
            'total_mb_sent': total_sent / (1024 * 1024),
            'total_mb_compressed': total_compressed / (1024 * 1024),
            'compression_time_ms': self.communication_stats['compression_time'] * 1000,
            'communication_time_ms': self.communication_stats['communication_time'] * 1000
        }

# Adaptive communication strategies
class AdaptiveCommunicationStrategy:
    def __init__(self, initial_strategy='allreduce'):
        self.current_strategy = initial_strategy
        self.performance_history = []
        self.adaptation_threshold = 10  # Adapt after 10 measurements
        
    def select_communication_strategy(self, model_size, network_bandwidth, num_workers):
        """Select optimal communication strategy based on conditions"""
        
        # Decision matrix for communication strategy
        if model_size < 100 * 1024 * 1024:  # < 100MB
            if num_workers <= 8:
                return 'allreduce'
            else:
                return 'hierarchical_allreduce'
        else:  # Large models
            if network_bandwidth > 10:  # > 10 Gbps
                return 'pipeline_parallel'
            else:
                return 'parameter_server'
    
    def adapt_strategy(self, current_performance):
        """Adapt communication strategy based on performance"""
        
        self.performance_history.append(current_performance)
        
        if len(self.performance_history) >= self.adaptation_threshold:
            # Analyze performance trends
            recent_performance = self.performance_history[-5:]
            avg_performance = sum(recent_performance) / len(recent_performance)
            
            # If performance is degrading, try different strategy
            if avg_performance > 1.2 * current_performance:  # 20% worse
                self._switch_strategy()
            
            # Reset history
            self.performance_history = self.performance_history[-5:]
    
    def _switch_strategy(self):
        """Switch to alternative communication strategy"""
        
        strategy_alternatives = {
            'allreduce': 'hierarchical_allreduce',
            'hierarchical_allreduce': 'parameter_server',
            'parameter_server': 'allreduce',
            'pipeline_parallel': 'allreduce'
        }
        
        old_strategy = self.current_strategy
        self.current_strategy = strategy_alternatives.get(old_strategy, 'allreduce')
        
        print(f"Switching communication strategy from {old_strategy} to {self.current_strategy}")
```

7.2 Memory Optimization
-----------------------
```python
class MemoryOptimizer:
    def __init__(self, model, max_memory_gb=16):
        self.model = model
        self.max_memory_gb = max_memory_gb
        self.memory_stats = {}
        
    def optimize_memory_usage(self, batch_size, sequence_length=None):
        """Optimize memory usage for distributed training"""
        
        # 1. Gradient checkpointing
        self._enable_gradient_checkpointing()
        
        # 2. Optimize batch size
        optimal_batch_size = self._find_optimal_batch_size(batch_size)
        
        # 3. Mixed precision training
        self._configure_mixed_precision()
        
        # 4. Memory-efficient attention (for transformer models)
        if sequence_length:
            self._optimize_attention_memory(sequence_length)
        
        return {
            'optimal_batch_size': optimal_batch_size,
            'memory_optimizations': [
                'gradient_checkpointing',
                'mixed_precision',
                'optimized_attention'
            ]
        }
    
    def _enable_gradient_checkpointing(self):
        """Enable gradient checkpointing to trade compute for memory"""
        
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
        else:
            # Manual gradient checkpointing for custom models
            for module in self.model.modules():
                if hasattr(module, 'checkpoint'):
                    module.checkpoint = True
    
    def _find_optimal_batch_size(self, initial_batch_size):
        """Find optimal batch size that fits in memory"""
        
        batch_size = initial_batch_size
        
        while batch_size > 1:
            try:
                # Test with current batch size
                memory_used = self._estimate_memory_usage(batch_size)
                
                if memory_used <= self.max_memory_gb * 1024 * 1024 * 1024:
                    return batch_size
                
                # Reduce batch size
                batch_size = batch_size // 2
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    batch_size = batch_size // 2
                    torch.cuda.empty_cache()
                else:
                    raise e
        
        return max(1, batch_size)
    
    def _estimate_memory_usage(self, batch_size):
        """Estimate memory usage for given batch size"""
        
        # Count parameters
        param_memory = sum(p.numel() * p.element_size() for p in self.model.parameters())
        
        # Estimate activation memory (rough approximation)
        # This would be more sophisticated in practice
        activation_memory = batch_size * 1024 * 1024  # 1MB per sample (rough estimate)
        
        # Gradient memory (same as parameters)
        gradient_memory = param_memory
        
        # Optimizer state memory (Adam uses 2x parameter memory)
        optimizer_memory = param_memory * 2
        
        total_memory = param_memory + activation_memory + gradient_memory + optimizer_memory
        
        return total_memory
    
    def _configure_mixed_precision(self):
        """Configure mixed precision training"""
        
        # Enable automatic mixed precision
        if hasattr(torch.cuda, 'amp'):
            # This would be integrated with the training loop
            pass
    
    def memory_efficient_forward_backward(self, model, data, targets, optimizer, scaler=None):
        """Memory-efficient forward and backward pass"""
        
        # Clear cache before forward pass
        torch.cuda.empty_cache()
        
        # Forward pass with gradient accumulation
        with torch.cuda.amp.autocast() if scaler else nullcontext():
            outputs = model(data)
            loss = F.cross_entropy(outputs, targets)
        
        # Backward pass
        if scaler:
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            optimizer.step()
        
        optimizer.zero_grad()
        
        # Clear cache after backward pass
        torch.cuda.empty_cache()
        
        return loss.item()
    
    def monitor_memory_usage(self):
        """Monitor and log memory usage"""
        
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            memory_reserved = torch.cuda.memory_reserved() / 1024**3  # GB
            
            self.memory_stats = {
                'memory_allocated_gb': memory_allocated,
                'memory_reserved_gb': memory_reserved,
                'memory_utilization': memory_allocated / memory_reserved if memory_reserved > 0 else 0
            }
        
        return self.memory_stats

# Zero Redundancy Optimizer (ZeRO) implementation
class ZeROOptimizer:
    def __init__(self, optimizer, model, world_size, stage=2):
        self.base_optimizer = optimizer
        self.model = model
        self.world_size = world_size
        self.stage = stage  # ZeRO stage (1, 2, or 3)
        self.rank = dist.get_rank()
        
        # Partition optimizer states
        if stage >= 1:
            self._partition_optimizer_states()
        
        # Partition gradients
        if stage >= 2:
            self._partition_gradients()
        
        # Partition parameters
        if stage >= 3:
            self._partition_parameters()
    
    def _partition_optimizer_states(self):
        """Partition optimizer states across workers (ZeRO-1)"""
        
        # Calculate parameter partition for this rank
        total_params = sum(p.numel() for p in self.model.parameters())
        params_per_rank = total_params // self.world_size
        
        start_idx = self.rank * params_per_rank
        end_idx = start_idx + params_per_rank
        
        if self.rank == self.world_size - 1:
            end_idx = total_params  # Last rank takes remaining parameters
        
        # Store partition information
        self.param_partition = (start_idx, end_idx)
        
        # Only maintain optimizer states for assigned parameters
        self.local_params = []
        current_idx = 0
        
        for param in self.model.parameters():
            param_end = current_idx + param.numel()
            
            # Check if this parameter overlaps with our partition
            if current_idx < end_idx and param_end > start_idx:
                self.local_params.append(param)
            
            current_idx = param_end
    
    def step(self):
        """Optimizer step with ZeRO optimizations"""
        
        if self.stage >= 2:
            # Gather gradients for assigned parameters
            self._gather_gradients()
        
        # Apply optimizer step only to local parameters
        if self.stage >= 1:
            # Create temporary optimizer with only local parameters
            local_optimizer = type(self.base_optimizer)(
                self.local_params,
                **self.base_optimizer.defaults
            )
            local_optimizer.step()
        else:
            self.base_optimizer.step()
        
        if self.stage >= 3:
            # Broadcast updated parameters
            self._broadcast_parameters()
    
    def _gather_gradients(self):
        """Gather gradients for assigned parameters"""
        
        for param in self.local_params:
            if param.grad is not None:
                # All-gather gradients from all ranks
                grad_list = [torch.zeros_like(param.grad) for _ in range(self.world_size)]
                dist.all_gather(grad_list, param.grad)
                
                # Sum gradients
                param.grad = sum(grad_list)
    
    def _broadcast_parameters(self):
        """Broadcast updated parameters to all ranks"""
        
        for param in self.local_params:
            # Broadcast parameter from owner rank to all other ranks
            dist.broadcast(param.data, src=self.rank)
```

================================================================================
8. BEST PRACTICES AND IMPLEMENTATION
================================================================================

8.1 Implementation Guidelines
-----------------------------
```python
class DistributedTrainingBestPractices:
    @staticmethod
    def get_implementation_checklist():
        return {
            'preparation': [
                'Profile single-GPU training performance first',
                'Identify communication vs computation bottlenecks',
                'Choose appropriate parallelism strategy',
                'Plan data loading and preprocessing pipeline',
                'Design fault tolerance mechanisms'
            ],
            'optimization': [
                'Scale learning rate with batch size/world size',
                'Implement gradient clipping for stability',
                'Use appropriate communication compression',
                'Optimize memory usage with mixed precision',
                'Balance communication and computation overlap'
            ],
            'monitoring': [
                'Track per-GPU utilization and memory usage',
                'Monitor communication overhead and bandwidth',
                'Log training metrics from rank 0 only',
                'Implement convergence monitoring',
                'Set up fault detection and recovery'
            ],
            'deployment': [
                'Test on target hardware configuration',
                'Validate scaling efficiency across node counts',
                'Implement checkpoint saving and loading',
                'Plan for dynamic scaling requirements',
                'Document hardware and network requirements'
            ]
        }
    
    @staticmethod
    def get_common_pitfalls():
        return {
            'data_loading': [
                'Not using DistributedSampler properly',
                'Insufficient data loading workers',
                'Data preprocessing becoming bottleneck',
                'Inconsistent data across workers'
            ],
            'communication': [
                'Not overlapping communication with computation',
                'Using inefficient communication patterns',
                'Ignoring network topology',
                'Excessive gradient synchronization frequency'
            ],
            'training_stability': [
                'Not scaling learning rate appropriately',
                'Inconsistent random seeds across workers',
                'Different model initialization across workers',
                'Gradient explosion in distributed setting'
            ],
            'resource_utilization': [
                'Poor load balancing across workers',
                'Memory inefficiency in distributed setting',
                'CPU bottlenecks during GPU training',
                'Storage I/O becoming bottleneck'
            ]
        }

class DistributedTrainingMetrics:
    def __init__(self, world_size, rank):
        self.world_size = world_size
        self.rank = rank
        self.metrics = {
            'training_time': [],
            'communication_time': [],
            'computation_time': [],
            'memory_usage': [],
            'gpu_utilization': []
        }
        
    def record_training_step(self, step_time, comm_time, comp_time):
        """Record metrics for training step"""
        
        self.metrics['training_time'].append(step_time)
        self.metrics['communication_time'].append(comm_time)
        self.metrics['computation_time'].append(comp_time)
        
        # Calculate efficiency metrics
        efficiency = comp_time / step_time if step_time > 0 else 0
        comm_overhead = comm_time / step_time if step_time > 0 else 0
        
        return {
            'step_time': step_time,
            'computation_efficiency': efficiency,
            'communication_overhead': comm_overhead
        }
    
    def calculate_scaling_efficiency(self, single_gpu_baseline):
        """Calculate distributed training scaling efficiency"""
        
        if not self.metrics['training_time']:
            return None
        
        avg_step_time = sum(self.metrics['training_time']) / len(self.metrics['training_time'])
        
        # Ideal speedup would be world_size
        ideal_step_time = single_gpu_baseline / self.world_size
        
        # Actual speedup
        actual_speedup = single_gpu_baseline / avg_step_time
        
        # Scaling efficiency
        scaling_efficiency = actual_speedup / self.world_size
        
        return {
            'actual_speedup': actual_speedup,
            'ideal_speedup': self.world_size,
            'scaling_efficiency': scaling_efficiency,
            'avg_step_time': avg_step_time
        }
    
    def generate_performance_report(self):
        """Generate comprehensive performance report"""
        
        if not any(self.metrics.values()):
            return {'error': 'No metrics recorded'}
        
        report = {}
        
        for metric_name, values in self.metrics.items():
            if values:
                report[metric_name] = {
                    'mean': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values),
                    'std': np.std(values) if len(values) > 1 else 0
                }
        
        # Calculate derived metrics
        if self.metrics['communication_time'] and self.metrics['training_time']:
            comm_times = self.metrics['communication_time']
            total_times = self.metrics['training_time']
            
            comm_ratios = [c/t for c, t in zip(comm_times, total_times) if t > 0]
            
            report['communication_overhead'] = {
                'mean_ratio': sum(comm_ratios) / len(comm_ratios) if comm_ratios else 0,
                'max_ratio': max(comm_ratios) if comm_ratios else 0
            }
        
        return report

# Production deployment configuration
DISTRIBUTED_TRAINING_CONFIG = {
    'small_scale': {
        'description': 'Single node, multiple GPUs',
        'nodes': 1,
        'gpus_per_node': 4,
        'strategy': 'data_parallel',
        'communication': 'nccl',
        'optimizations': ['mixed_precision', 'gradient_checkpointing']
    },
    'medium_scale': {
        'description': 'Multiple nodes, high-speed interconnect',
        'nodes': 4,
        'gpus_per_node': 8,
        'strategy': 'data_parallel',
        'communication': 'hierarchical_allreduce',
        'optimizations': ['gradient_compression', 'communication_overlap']
    },
    'large_scale': {
        'description': 'Large cluster, model parallelism',
        'nodes': 16,
        'gpus_per_node': 8,
        'strategy': 'hybrid_parallel',
        'communication': 'parameter_server',
        'optimizations': ['zero_optimizer', 'pipeline_parallel']
    }
}

# Launch script template
def create_launch_script(config_name, script_path, args):
    """Create launch script for distributed training"""
    
    config = DISTRIBUTED_TRAINING_CONFIG[config_name]
    
    if config['nodes'] == 1:
        # Single node launch
        launch_cmd = f"""
torchrun --nproc_per_node={config['gpus_per_node']} \\
    {script_path} {' '.join(args)}
"""
    else:
        # Multi-node launch
        launch_cmd = f"""
torchrun --nnodes={config['nodes']} \\
    --nproc_per_node={config['gpus_per_node']} \\
    --node_rank=$NODE_RANK \\
    --master_addr=$MASTER_ADDR \\
    --master_port=$MASTER_PORT \\
    {script_path} {' '.join(args)}
"""
    
    return launch_cmd.strip()
```

================================================================================
SUMMARY AND KEY TAKEAWAYS
================================================================================

Distributed training and scaling are essential for modern ML at scale:

**Key Parallelism Strategies:**
- **Data Parallelism:** Most common, replicate model across devices
- **Model Parallelism:** Partition large models across devices  
- **Pipeline Parallelism:** Sequential processing through model stages
- **Hybrid Approaches:** Combine strategies for optimal resource utilization

**Communication Patterns:**
- **Synchronous:** All-reduce, broadcast, gather for coordinated updates
- **Asynchronous:** Parameter servers for flexible update timing
- **Hierarchical:** Multi-level communication for large-scale deployments
- **Compressed:** Reduce bandwidth requirements through compression

**Framework Implementation:**
- **PyTorch:** DistributedDataParallel, Pipeline Parallel, FairScale/DeepSpeed
- **TensorFlow:** Distribution strategies, TPU integration, multi-worker support
- **Specialized:** Horovod, Ray, specialized frameworks for specific use cases

**Performance Optimization:**
- **Communication:** Gradient compression, overlap with computation, efficient topologies
- **Memory:** Gradient checkpointing, mixed precision, ZeRO optimizer states
- **Computation:** Load balancing, optimal batch sizes, hardware-specific optimizations

**Best Practices:**
- Profile single-GPU performance before scaling
- Choose parallelism strategy based on model and hardware characteristics
- Implement proper synchronization and fault tolerance mechanisms
- Monitor scaling efficiency and communication overhead
- Design for the target deployment environment from the beginning

**Success Factors:**
- Clear understanding of model and data characteristics
- Appropriate hardware and network infrastructure
- Systematic optimization of communication and computation overlap
- Comprehensive monitoring and performance analysis
- Robust fault tolerance and recovery mechanisms

Effective distributed training enables training of larger models on larger datasets while maintaining reasonable training times and resource utilization. 