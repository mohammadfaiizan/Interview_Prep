COMPUTE OPTIMIZATION AND GPU CLUSTERS
=====================================

Table of Contents:
1. GPU Computing Fundamentals
2. GPU Cluster Architecture
3. Resource Management and Scheduling
4. Performance Optimization
5. Multi-GPU Training Optimization
6. Cost Optimization Strategies
7. Monitoring and Troubleshooting
8. Best Practices and Implementation

================================================================================
1. GPU COMPUTING FUNDAMENTALS
================================================================================

1.1 GPU Architecture for ML
---------------------------
**CUDA Cores vs Tensor Cores:**
- CUDA Cores: General parallel processing
- Tensor Cores: Optimized for AI/ML workloads (mixed precision)
- RT Cores: Ray tracing (less relevant for ML)

**Memory Hierarchy:**
- Global Memory: Large but high latency (16-80GB)
- Shared Memory: Fast but limited (48-164KB per SM)
- Registers: Fastest but very limited

**Key Performance Metrics:**
- FLOPS: Floating-point operations per second
- Memory Bandwidth: Data transfer rate
- Memory Size: Available VRAM
- Utilization: Percentage of GPU resources used

1.2 GPU Utilization Analysis
----------------------------
```python
import nvidia_ml_py3 as nvml
import psutil
import time
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class GPUMetrics:
    gpu_id: int
    name: str
    memory_used: int
    memory_total: int
    memory_utilization: float
    gpu_utilization: float
    temperature: int
    power_usage: int
    power_limit: int

class GPUMonitor:
    def __init__(self):
        nvml.nvmlInit()
        self.device_count = nvml.nvmlDeviceGetCount()
        
    def get_gpu_metrics(self) -> List[GPUMetrics]:
        """Get current GPU metrics for all devices"""
        
        metrics = []
        
        for i in range(self.device_count):
            handle = nvml.nvmlDeviceGetHandleByIndex(i)
            
            # Basic info
            name = nvml.nvmlDeviceGetName(handle).decode('utf-8')
            
            # Memory info
            mem_info = nvml.nvmlDeviceGetMemoryInfo(handle)
            memory_used = mem_info.used // 1024**2  # MB
            memory_total = mem_info.total // 1024**2  # MB
            memory_utilization = (mem_info.used / mem_info.total) * 100
            
            # Utilization
            util = nvml.nvmlDeviceGetUtilizationRates(handle)
            gpu_utilization = util.gpu
            
            # Temperature
            temperature = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)
            
            # Power
            power_usage = nvml.nvmlDeviceGetPowerUsage(handle) // 1000  # Watts
            power_limit = nvml.nvmlDeviceGetPowerManagementLimitConstraints(handle)[1] // 1000
            
            metrics.append(GPUMetrics(
                gpu_id=i,
                name=name,
                memory_used=memory_used,
                memory_total=memory_total,
                memory_utilization=memory_utilization,
                gpu_utilization=gpu_utilization,
                temperature=temperature,
                power_usage=power_usage,
                power_limit=power_limit
            ))
        
        return metrics
    
    def analyze_bottlenecks(self, metrics_history: List[List[GPUMetrics]]) -> Dict:
        """Analyze performance bottlenecks from metrics history"""
        
        analysis = {
            'memory_bottleneck': False,
            'compute_bottleneck': False,
            'thermal_throttling': False,
            'power_throttling': False,
            'recommendations': []
        }
        
        if not metrics_history:
            return analysis
        
        # Analyze patterns across time
        avg_memory_util = []
        avg_gpu_util = []
        max_temps = []
        avg_power = []
        
        for snapshot in metrics_history:
            avg_memory_util.append(sum(m.memory_utilization for m in snapshot) / len(snapshot))
            avg_gpu_util.append(sum(m.gpu_utilization for m in snapshot) / len(snapshot))
            max_temps.append(max(m.temperature for m in snapshot))
            avg_power.append(sum(m.power_usage for m in snapshot) / len(snapshot))
        
        # Memory bottleneck analysis
        if sum(avg_memory_util) / len(avg_memory_util) > 90:
            analysis['memory_bottleneck'] = True
            analysis['recommendations'].append('Consider reducing batch size or using gradient accumulation')
        
        # Compute bottleneck analysis
        if sum(avg_gpu_util) / len(avg_gpu_util) < 70:
            analysis['compute_bottleneck'] = True
            analysis['recommendations'].append('Optimize data loading or increase batch size')
        
        # Thermal analysis
        if max(max_temps) > 80:
            analysis['thermal_throttling'] = True
            analysis['recommendations'].append('Improve cooling or reduce workload intensity')
        
        return analysis

class GPUProfiler:
    def __init__(self):
        self.profiling_active = False
        
    def profile_training_step(self, model_forward_func, data_batch):
        """Profile a training step to identify bottlenecks"""
        
        import torch
        
        if not torch.cuda.is_available():
            return {'error': 'CUDA not available'}
        
        # Warm up
        for _ in range(3):
            _ = model_forward_func(data_batch)
        
        torch.cuda.synchronize()
        
        # Profile execution
        with torch.profiler.profile(
            activities=[
                torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ],
            record_shapes=True,
            profile_memory=True,
            with_stack=True
        ) as prof:
            output = model_forward_func(data_batch)
        
        # Analyze results
        profiling_results = {
            'cpu_time_total': 0,
            'cuda_time_total': 0,
            'memory_peak': torch.cuda.max_memory_allocated(),
            'top_operations': []
        }
        
        # Get top CUDA operations
        events = prof.key_averages()
        cuda_events = [e for e in events if e.device_type == torch.profiler.DeviceType.CUDA]
        cuda_events.sort(key=lambda x: x.cuda_time_total, reverse=True)
        
        profiling_results['top_operations'] = [
            {
                'name': event.key,
                'cuda_time': event.cuda_time_total,
                'cpu_time': event.cpu_time_total,
                'calls': event.count
            }
            for event in cuda_events[:10]
        ]
        
        return profiling_results
```

================================================================================
2. GPU CLUSTER ARCHITECTURE
================================================================================

2.1 Cluster Design Patterns
---------------------------
```python
class GPUClusterManager:
    def __init__(self, cluster_config):
        self.config = cluster_config
        self.nodes = {}
        self.resource_pool = ResourcePool()
        
    def design_cluster_topology(self, workload_requirements):
        """Design optimal cluster topology based on workload"""
        
        topology = {
            'node_types': [],
            'network_topology': '',
            'storage_architecture': '',
            'total_cost_estimate': 0
        }
        
        # Analyze workload requirements
        max_model_size = workload_requirements.get('max_model_size_gb', 10)
        concurrent_jobs = workload_requirements.get('concurrent_jobs', 4)
        communication_intensive = workload_requirements.get('distributed_training', False)
        
        # Determine node types
        if max_model_size <= 16:  # Fits in single GPU
            topology['node_types'].append({
                'type': 'single_gpu_node',
                'gpu_type': 'A100-40GB',
                'count': concurrent_jobs,
                'use_case': 'Single GPU training'
            })
        elif max_model_size <= 80:  # Needs multi-GPU
            topology['node_types'].append({
                'type': 'multi_gpu_node',
                'gpu_type': 'A100-80GB',
                'gpus_per_node': 8,
                'count': max(1, concurrent_jobs // 4),
                'use_case': 'Multi-GPU training'
            })
        else:  # Very large models
            topology['node_types'].append({
                'type': 'large_memory_node',
                'gpu_type': 'H100-80GB',
                'gpus_per_node': 8,
                'count': max(2, concurrent_jobs // 2),
                'use_case': 'Large model training'
            })
        
        # Network topology
        if communication_intensive:
            topology['network_topology'] = 'InfiniBand_HDR'  # High bandwidth
        else:
            topology['network_topology'] = 'Ethernet_100Gb'  # Standard
        
        # Storage architecture
        topology['storage_architecture'] = self._design_storage_architecture(workload_requirements)
        
        return topology
    
    def _design_storage_architecture(self, requirements):
        """Design storage architecture for cluster"""
        
        data_size = requirements.get('dataset_size_tb', 10)
        io_pattern = requirements.get('io_pattern', 'sequential')
        
        if data_size > 100:  # Large datasets
            return {
                'type': 'distributed_filesystem',
                'implementation': 'Lustre',
                'capacity_tb': data_size * 2,  # 2x for redundancy
                'performance_tier': 'nvme_ssd'
            }
        else:  # Smaller datasets
            return {
                'type': 'shared_storage',
                'implementation': 'NFS_over_RDMA',
                'capacity_tb': data_size * 1.5,
                'performance_tier': 'ssd'
            }

class NetworkOptimizer:
    def __init__(self, cluster_topology):
        self.topology = cluster_topology
        
    def optimize_network_configuration(self, communication_patterns):
        """Optimize network configuration for ML workloads"""
        
        optimizations = {
            'bandwidth_optimization': [],
            'latency_optimization': [],
            'topology_optimization': []
        }
        
        # Analyze communication patterns
        for pattern in communication_patterns:
            if pattern['type'] == 'all_reduce':
                optimizations['bandwidth_optimization'].extend([
                    'Enable NCCL tree algorithm for large messages',
                    'Configure optimal NCCL ring topology',
                    'Use hierarchical all-reduce for multi-node'
                ])
            
            elif pattern['type'] == 'parameter_server':
                optimizations['latency_optimization'].extend([
                    'Collocate parameter servers with compute nodes',
                    'Use RDMA for low-latency communication',
                    'Implement gradient compression'
                ])
            
            elif pattern['type'] == 'pipeline_parallel':
                optimizations['topology_optimization'].extend([
                    'Optimize pipeline stage placement',
                    'Minimize cross-switch communication',
                    'Use dedicated networks for pipeline communication'
                ])
        
        return optimizations
    
    def configure_nccl_settings(self, cluster_size, network_type):
        """Generate optimal NCCL configuration"""
        
        nccl_config = {
            'NCCL_DEBUG': 'INFO',
            'NCCL_IB_DISABLE': '0' if network_type == 'InfiniBand' else '1',
            'NCCL_NET_GDR_LEVEL': '5' if network_type == 'InfiniBand' else '0',
            'NCCL_TREE_THRESHOLD': '0',
            'NCCL_ALGO': 'Auto'
        }
        
        # Cluster-size specific optimizations
        if cluster_size > 16:
            nccl_config.update({
                'NCCL_MIN_NRINGS': '4',
                'NCCL_MAX_NRINGS': '16',
                'NCCL_BUFFSIZE': '8388608'  # 8MB
            })
        
        if network_type == 'InfiniBand':
            nccl_config.update({
                'NCCL_IB_QPS_PER_CONNECTION': '4',
                'NCCL_IB_GID_INDEX': '3',
                'NCCL_IB_HCA': 'mlx5'
            })
        
        return nccl_config

class ResourcePool:
    def __init__(self):
        self.available_resources = {}
        self.allocated_resources = {}
        self.resource_queue = []
        
    def add_node(self, node_id, node_specs):
        """Add node to resource pool"""
        
        self.available_resources[node_id] = {
            'gpus': node_specs['gpu_count'],
            'gpu_type': node_specs['gpu_type'],
            'memory_gb': node_specs['memory_gb'],
            'cpu_cores': node_specs['cpu_cores'],
            'network_bandwidth': node_specs['network_bandwidth'],
            'status': 'available'
        }
    
    def allocate_resources(self, job_requirements):
        """Allocate resources for job"""
        
        required_gpus = job_requirements['gpus']
        required_memory = job_requirements.get('memory_gb', 0)
        preferred_gpu_type = job_requirements.get('gpu_type', 'any')
        
        # Find suitable nodes
        suitable_nodes = []
        
        for node_id, resources in self.available_resources.items():
            if (resources['status'] == 'available' and
                resources['gpus'] >= required_gpus and
                resources['memory_gb'] >= required_memory):
                
                if preferred_gpu_type == 'any' or resources['gpu_type'] == preferred_gpu_type:
                    suitable_nodes.append((node_id, resources))
        
        if not suitable_nodes:
            return None  # No suitable resources
        
        # Select best node (could implement more sophisticated selection)
        selected_node_id, selected_resources = suitable_nodes[0]
        
        # Allocate resources
        allocation_id = f"job_{len(self.allocated_resources)}"
        
        self.allocated_resources[allocation_id] = {
            'node_id': selected_node_id,
            'gpus_allocated': required_gpus,
            'memory_allocated': required_memory,
            'job_requirements': job_requirements,
            'allocation_time': time.time()
        }
        
        # Update available resources
        self.available_resources[selected_node_id]['gpus'] -= required_gpus
        self.available_resources[selected_node_id]['memory_gb'] -= required_memory
        
        if self.available_resources[selected_node_id]['gpus'] == 0:
            self.available_resources[selected_node_id]['status'] = 'fully_allocated'
        
        return allocation_id
    
    def release_resources(self, allocation_id):
        """Release allocated resources"""
        
        if allocation_id not in self.allocated_resources:
            return False
        
        allocation = self.allocated_resources[allocation_id]
        node_id = allocation['node_id']
        
        # Return resources to pool
        self.available_resources[node_id]['gpus'] += allocation['gpus_allocated']
        self.available_resources[node_id]['memory_gb'] += allocation['memory_allocated']
        self.available_resources[node_id]['status'] = 'available'
        
        # Remove allocation
        del self.allocated_resources[allocation_id]
        
        return True
```

================================================================================
3. RESOURCE MANAGEMENT AND SCHEDULING
================================================================================

3.1 GPU Scheduling Strategies
-----------------------------
```python
from enum import Enum
from datetime import datetime, timedelta
import heapq

class SchedulingStrategy(Enum):
    FIFO = "first_in_first_out"
    PRIORITY = "priority_based"
    FAIR_SHARE = "fair_share"
    BACKFILL = "backfill"

class JobPriority(Enum):
    LOW = 1
    NORMAL = 2
    HIGH = 3
    URGENT = 4

class GPUScheduler:
    def __init__(self, strategy=SchedulingStrategy.PRIORITY):
        self.strategy = strategy
        self.job_queue = []
        self.running_jobs = {}
        self.completed_jobs = []
        self.resource_pool = ResourcePool()
        
    def submit_job(self, job_spec):
        """Submit job to scheduler"""
        
        job = {
            'job_id': f"job_{len(self.job_queue) + len(self.running_jobs)}",
            'user_id': job_spec['user_id'],
            'priority': job_spec.get('priority', JobPriority.NORMAL),
            'resource_requirements': job_spec['resources'],
            'estimated_runtime': job_spec.get('estimated_runtime', 3600),
            'submission_time': datetime.now(),
            'dependencies': job_spec.get('dependencies', []),
            'command': job_spec['command'],
            'status': 'queued'
        }
        
        if self.strategy == SchedulingStrategy.PRIORITY:
            # Use negative priority for max heap behavior
            heapq.heappush(self.job_queue, (-job['priority'].value, job['submission_time'], job))
        else:
            self.job_queue.append(job)
        
        return job['job_id']
    
    def schedule_jobs(self):
        """Main scheduling loop"""
        
        scheduled_jobs = []
        
        while self.job_queue:
            if self.strategy == SchedulingStrategy.PRIORITY:
                _, _, job = heapq.heappop(self.job_queue)
            else:
                job = self.job_queue.pop(0)
            
            # Check dependencies
            if not self._dependencies_satisfied(job):
                self.job_queue.append(job)
                break
            
            # Try to allocate resources
            allocation_id = self.resource_pool.allocate_resources(job['resource_requirements'])
            
            if allocation_id:
                # Start job
                job['status'] = 'running'
                job['start_time'] = datetime.now()
                job['allocation_id'] = allocation_id
                
                self.running_jobs[job['job_id']] = job
                scheduled_jobs.append(job)
            else:
                # Put job back in queue
                if self.strategy == SchedulingStrategy.PRIORITY:
                    heapq.heappush(self.job_queue, (-job['priority'].value, job['submission_time'], job))
                else:
                    self.job_queue.insert(0, job)
                break
        
        return scheduled_jobs
    
    def _dependencies_satisfied(self, job):
        """Check if job dependencies are satisfied"""
        
        for dep_job_id in job['dependencies']:
            if dep_job_id in self.running_jobs:
                return False
            
            # Check if dependency completed successfully
            dep_completed = any(
                completed_job['job_id'] == dep_job_id and completed_job['status'] == 'completed'
                for completed_job in self.completed_jobs
            )
            
            if not dep_completed:
                return False
        
        return True
    
    def complete_job(self, job_id, status='completed'):
        """Mark job as completed and release resources"""
        
        if job_id not in self.running_jobs:
            return False
        
        job = self.running_jobs[job_id]
        job['status'] = status
        job['end_time'] = datetime.now()
        job['runtime'] = (job['end_time'] - job['start_time']).total_seconds()
        
        # Release resources
        self.resource_pool.release_resources(job['allocation_id'])
        
        # Move to completed jobs
        self.completed_jobs.append(job)
        del self.running_jobs[job_id]
        
        return True
    
    def get_queue_status(self):
        """Get current queue status"""
        
        return {
            'queued_jobs': len(self.job_queue),
            'running_jobs': len(self.running_jobs),
            'completed_jobs': len(self.completed_jobs),
            'total_gpu_utilization': self._calculate_gpu_utilization()
        }
    
    def _calculate_gpu_utilization(self):
        """Calculate overall GPU utilization"""
        
        total_gpus = sum(
            node['gpus'] for node in self.resource_pool.available_resources.values()
        ) + sum(
            alloc['gpus_allocated'] for alloc in self.resource_pool.allocated_resources.values()
        )
        
        allocated_gpus = sum(
            alloc['gpus_allocated'] for alloc in self.resource_pool.allocated_resources.values()
        )
        
        return (allocated_gpus / total_gpus) * 100 if total_gpus > 0 else 0

class FairShareScheduler(GPUScheduler):
    def __init__(self):
        super().__init__(SchedulingStrategy.FAIR_SHARE)
        self.user_shares = {}
        self.user_usage = {}
        
    def set_user_share(self, user_id, share_percentage):
        """Set fair share percentage for user"""
        self.user_shares[user_id] = share_percentage
    
    def schedule_jobs(self):
        """Fair share scheduling implementation"""
        
        # Calculate current usage ratios
        self._update_usage_ratios()
        
        # Sort jobs by user priority (underutilized users first)
        sorted_jobs = sorted(
            self.job_queue,
            key=lambda job: self._calculate_user_priority(job['user_id'])
        )
        
        scheduled_jobs = []
        
        for job in sorted_jobs[:]:  # Copy to avoid modification during iteration
            if self._should_schedule_for_fairness(job):
                allocation_id = self.resource_pool.allocate_resources(job['resource_requirements'])
                
                if allocation_id:
                    job['status'] = 'running'
                    job['start_time'] = datetime.now()
                    job['allocation_id'] = allocation_id
                    
                    self.running_jobs[job['job_id']] = job
                    self.job_queue.remove(job)
                    scheduled_jobs.append(job)
        
        return scheduled_jobs
    
    def _update_usage_ratios(self):
        """Update current usage ratios for all users"""
        
        self.user_usage = {}
        
        for job in self.running_jobs.values():
            user_id = job['user_id']
            gpu_count = job['resource_requirements']['gpus']
            
            if user_id not in self.user_usage:
                self.user_usage[user_id] = 0
            self.user_usage[user_id] += gpu_count
    
    def _calculate_user_priority(self, user_id):
        """Calculate user priority based on fair share"""
        
        user_share = self.user_shares.get(user_id, 10.0)  # Default 10%
        user_usage = self.user_usage.get(user_id, 0)
        
        total_gpus = sum(
            node['gpus'] for node in self.resource_pool.available_resources.values()
        )
        
        expected_usage = (user_share / 100) * total_gpus
        actual_usage_ratio = user_usage / expected_usage if expected_usage > 0 else float('inf')
        
        # Lower ratio = higher priority
        return actual_usage_ratio
    
    def _should_schedule_for_fairness(self, job):
        """Determine if job should be scheduled based on fairness"""
        
        user_id = job['user_id']
        user_priority = self._calculate_user_priority(user_id)
        
        # Schedule if user is underutilized (priority < 1.0)
        return user_priority < 1.0

class PreemptiveScheduler(GPUScheduler):
    def __init__(self):
        super().__init__(SchedulingStrategy.PRIORITY)
        self.preemption_enabled = True
        
    def submit_urgent_job(self, job_spec):
        """Submit urgent job that can preempt running jobs"""
        
        job_spec['priority'] = JobPriority.URGENT
        job_id = self.submit_job(job_spec)
        
        if self.preemption_enabled:
            self._attempt_preemption(job_id)
        
        return job_id
    
    def _attempt_preemption(self, urgent_job_id):
        """Attempt to preempt lower priority jobs"""
        
        # Find the urgent job
        urgent_job = None
        for _, _, job in self.job_queue:
            if job['job_id'] == urgent_job_id:
                urgent_job = job
                break
        
        if not urgent_job:
            return
        
        # Find running jobs that can be preempted
        preemptible_jobs = [
            job for job in self.running_jobs.values()
            if job['priority'].value < urgent_job['priority'].value
        ]
        
        # Sort by priority (lowest first)
        preemptible_jobs.sort(key=lambda x: x['priority'].value)
        
        # Calculate resources needed
        resources_needed = urgent_job['resource_requirements']
        gpus_needed = resources_needed['gpus']
        
        # Preempt jobs until we have enough resources
        preempted_jobs = []
        gpus_freed = 0
        
        for job in preemptible_jobs:
            if gpus_freed >= gpus_needed:
                break
            
            # Preempt this job
            self._preempt_job(job['job_id'])
            preempted_jobs.append(job)
            gpus_freed += job['resource_requirements']['gpus']
        
        return preempted_jobs
    
    def _preempt_job(self, job_id):
        """Preempt a running job"""
        
        if job_id not in self.running_jobs:
            return False
        
        job = self.running_jobs[job_id]
        
        # Save checkpoint if supported
        if job.get('supports_checkpointing', False):
            self._save_checkpoint(job)
        
        # Release resources
        self.resource_pool.release_resources(job['allocation_id'])
        
        # Put job back in queue
        job['status'] = 'preempted'
        job['preemption_count'] = job.get('preemption_count', 0) + 1
        
        if self.strategy == SchedulingStrategy.PRIORITY:
            heapq.heappush(self.job_queue, (-job['priority'].value, job['submission_time'], job))
        else:
            self.job_queue.insert(0, job)
        
        del self.running_jobs[job_id]
        
        return True
    
    def _save_checkpoint(self, job):
        """Save job checkpoint for later resumption"""
        
        checkpoint_info = {
            'job_id': job['job_id'],
            'checkpoint_time': datetime.now(),
            'runtime_so_far': (datetime.now() - job['start_time']).total_seconds(),
            'checkpoint_path': f"/checkpoints/{job['job_id']}.pt"
        }
        
        # In practice, would integrate with ML framework checkpointing
        print(f"Saving checkpoint for job {job['job_id']}")
        
        return checkpoint_info
```

================================================================================
4. PERFORMANCE OPTIMIZATION
================================================================================

4.1 GPU Performance Tuning
--------------------------
```python
class GPUPerformanceOptimizer:
    def __init__(self):
        self.optimization_strategies = {
            'memory_optimization': self._optimize_memory_usage,
            'compute_optimization': self._optimize_compute_utilization,
            'communication_optimization': self._optimize_communication,
            'mixed_precision': self._enable_mixed_precision
        }
    
    def analyze_performance_profile(self, profiling_data):
        """Analyze performance profile and recommend optimizations"""
        
        recommendations = []
        
        # Memory analysis
        memory_util = profiling_data.get('memory_utilization', 0)
        if memory_util > 95:
            recommendations.append({
                'type': 'memory_optimization',
                'priority': 'high',
                'issue': 'High memory utilization',
                'recommendation': 'Reduce batch size or enable gradient checkpointing'
            })
        elif memory_util < 50:
            recommendations.append({
                'type': 'memory_optimization',
                'priority': 'medium',
                'issue': 'Low memory utilization',
                'recommendation': 'Increase batch size for better GPU utilization'
            })
        
        # Compute analysis
        gpu_util = profiling_data.get('gpu_utilization', 0)
        if gpu_util < 70:
            recommendations.append({
                'type': 'compute_optimization',
                'priority': 'high',
                'issue': 'Low GPU utilization',
                'recommendation': 'Optimize data loading or increase model complexity'
            })
        
        # Communication analysis
        comm_overhead = profiling_data.get('communication_overhead', 0)
        if comm_overhead > 30:
            recommendations.append({
                'type': 'communication_optimization',
                'priority': 'high',
                'issue': 'High communication overhead',
                'recommendation': 'Enable gradient compression or optimize network topology'
            })
        
        return recommendations
    
    def _optimize_memory_usage(self, model, config):
        """Optimize GPU memory usage"""
        
        optimizations = []
        
        # Gradient checkpointing
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
            optimizations.append('Enabled gradient checkpointing')
        
        # Mixed precision
        if config.get('enable_mixed_precision', True):
            optimizations.append('Enabled mixed precision training')
        
        # Memory cleanup
        import torch
        torch.cuda.empty_cache()
        optimizations.append('Cleared GPU memory cache')
        
        return optimizations
    
    def _optimize_compute_utilization(self, training_config):
        """Optimize compute utilization"""
        
        optimizations = []
        
        # Batch size optimization
        current_batch_size = training_config.get('batch_size', 32)
        optimal_batch_size = self._find_optimal_batch_size(training_config)
        
        if optimal_batch_size != current_batch_size:
            optimizations.append(f'Adjust batch size from {current_batch_size} to {optimal_batch_size}')
        
        # Data loading optimization
        num_workers = training_config.get('num_workers', 4)
        if num_workers < 8:
            optimizations.append('Increase data loading workers for better CPU-GPU overlap')
        
        return optimizations
    
    def _find_optimal_batch_size(self, config):
        """Find optimal batch size for given configuration"""
        
        # Start with current batch size and test larger sizes
        base_batch_size = config.get('batch_size', 32)
        max_batch_size = base_batch_size * 4
        
        # Binary search for optimal batch size
        low, high = base_batch_size, max_batch_size
        optimal_size = base_batch_size
        
        while low <= high:
            mid = (low + high) // 2
            
            try:
                # Test if batch size fits in memory
                memory_required = self._estimate_memory_requirement(config, mid)
                memory_available = torch.cuda.get_device_properties(0).total_memory * 0.9  # 90% safety margin
                
                if memory_required <= memory_available:
                    optimal_size = mid
                    low = mid + 1
                else:
                    high = mid - 1
                    
            except RuntimeError:  # OOM error
                high = mid - 1
        
        return optimal_size
    
    def _estimate_memory_requirement(self, config, batch_size):
        """Estimate memory requirement for given batch size"""
        
        # Simplified estimation - in practice would be more sophisticated
        model_params = config.get('model_parameters', 100_000_000)  # 100M parameters
        sequence_length = config.get('sequence_length', 512)
        hidden_size = config.get('hidden_size', 768)
        
        # Parameter memory
        param_memory = model_params * 4  # FP32
        
        # Activation memory (rough estimate)
        activation_memory = batch_size * sequence_length * hidden_size * 4
        
        # Gradient memory
        gradient_memory = param_memory
        
        # Optimizer state (Adam uses 2x param memory)
        optimizer_memory = param_memory * 2
        
        total_memory = param_memory + activation_memory + gradient_memory + optimizer_memory
        
        return total_memory

class MultiGPUOptimizer:
    def __init__(self, num_gpus):
        self.num_gpus = num_gpus
        
    def optimize_multi_gpu_training(self, model, training_config):
        """Optimize multi-GPU training configuration"""
        
        optimizations = {
            'data_parallel_config': {},
            'model_parallel_config': {},
            'communication_config': {},
            'memory_config': {}
        }
        
        model_size = self._estimate_model_size(model)
        
        # Choose parallelism strategy
        if model_size < 8_000_000_000:  # < 8GB model
            # Use data parallelism
            optimizations['data_parallel_config'] = {
                'strategy': 'DataParallel',
                'batch_size_per_gpu': training_config['batch_size'] // self.num_gpus,
                'gradient_accumulation_steps': 1
            }
        else:
            # Use model parallelism or pipeline parallelism
            optimizations['model_parallel_config'] = {
                'strategy': 'ModelParallel',
                'partition_method': 'layer_wise',
                'pipeline_stages': min(4, self.num_gpus)
            }
        
        # Communication optimization
        optimizations['communication_config'] = {
            'backend': 'nccl',
            'bucket_size_mb': 25,
            'gradient_compression': model_size > 1_000_000_000
        }
        
        return optimizations
    
    def _estimate_model_size(self, model):
        """Estimate model size in bytes"""
        
        total_params = sum(p.numel() for p in model.parameters())
        # Assume FP32 parameters
        model_size_bytes = total_params * 4
        
        return model_size_bytes
    
    def benchmark_scaling_efficiency(self, model, data_loader, gpu_counts):
        """Benchmark scaling efficiency across different GPU counts"""
        
        results = {}
        
        # Single GPU baseline
        single_gpu_time = self._benchmark_single_gpu(model, data_loader)
        
        for gpu_count in gpu_counts:
            if gpu_count == 1:
                results[gpu_count] = {
                    'time_per_epoch': single_gpu_time,
                    'speedup': 1.0,
                    'efficiency': 1.0
                }
            else:
                multi_gpu_time = self._benchmark_multi_gpu(model, data_loader, gpu_count)
                speedup = single_gpu_time / multi_gpu_time
                efficiency = speedup / gpu_count
                
                results[gpu_count] = {
                    'time_per_epoch': multi_gpu_time,
                    'speedup': speedup,
                    'efficiency': efficiency
                }
        
        return results
    
    def _benchmark_single_gpu(self, model, data_loader):
        """Benchmark single GPU training time"""
        
        import torch
        import time
        
        device = torch.device('cuda:0')
        model = model.to(device)
        
        start_time = time.time()
        
        # Run a few batches
        for i, (data, targets) in enumerate(data_loader):
            if i >= 10:  # Only run 10 batches for benchmark
                break
            
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            loss = torch.nn.functional.cross_entropy(outputs, targets)
            loss.backward()
        
        end_time = time.time()
        
        # Estimate time per epoch
        batches_per_epoch = len(data_loader)
        time_per_epoch = (end_time - start_time) * batches_per_epoch / 10
        
        return time_per_epoch
    
    def _benchmark_multi_gpu(self, model, data_loader, gpu_count):
        """Benchmark multi-GPU training time"""
        
        # This would implement actual multi-GPU benchmarking
        # For simplicity, using a scaling model
        
        single_gpu_time = self._benchmark_single_gpu(model, data_loader)
        
        # Assume some communication overhead
        communication_overhead = 0.1 * (gpu_count - 1)
        efficiency = 1.0 - communication_overhead
        
        multi_gpu_time = single_gpu_time / (gpu_count * efficiency)
        
        return multi_gpu_time
```

================================================================================
5. COST OPTIMIZATION STRATEGIES
================================================================================

5.1 Cost Analysis and Optimization
----------------------------------
```python
class GPUCostOptimizer:
    def __init__(self):
        self.pricing_models = {
            'on_demand': self._calculate_on_demand_cost,
            'reserved': self._calculate_reserved_cost,
            'spot': self._calculate_spot_cost,
            'preemptible': self._calculate_preemptible_cost
        }
        
        # GPU pricing (example prices in USD/hour)
        self.gpu_pricing = {
            'V100': {'on_demand': 3.06, 'reserved': 1.84, 'spot': 0.92},
            'A100': {'on_demand': 4.56, 'reserved': 2.74, 'spot': 1.37},
            'H100': {'on_demand': 8.00, 'reserved': 4.80, 'spot': 2.40}
        }
    
    def analyze_workload_cost(self, workload_profile):
        """Analyze cost for different pricing models"""
        
        cost_analysis = {}
        
        gpu_type = workload_profile['gpu_type']
        gpu_count = workload_profile['gpu_count']
        runtime_hours = workload_profile['runtime_hours']
        fault_tolerance = workload_profile.get('fault_tolerance', 'high')
        
        for pricing_model in self.pricing_models:
            if pricing_model == 'spot' and fault_tolerance == 'low':
                continue  # Skip spot pricing for fault-intolerant workloads
            
            cost = self.pricing_models[pricing_model](
                gpu_type, gpu_count, runtime_hours, workload_profile
            )
            
            cost_analysis[pricing_model] = {
                'total_cost': cost,
                'cost_per_hour': cost / runtime_hours if runtime_hours > 0 else 0,
                'cost_per_gpu_hour': cost / (gpu_count * runtime_hours) if gpu_count > 0 and runtime_hours > 0 else 0
            }
        
        # Add recommendations
        cost_analysis['recommendations'] = self._generate_cost_recommendations(
            cost_analysis, workload_profile
        )
        
        return cost_analysis
    
    def _calculate_on_demand_cost(self, gpu_type, gpu_count, runtime_hours, workload_profile):
        """Calculate on-demand pricing cost"""
        
        hourly_rate = self.gpu_pricing[gpu_type]['on_demand']
        return gpu_count * runtime_hours * hourly_rate
    
    def _calculate_reserved_cost(self, gpu_type, gpu_count, runtime_hours, workload_profile):
        """Calculate reserved instance cost"""
        
        hourly_rate = self.gpu_pricing[gpu_type]['reserved']
        
        # Reserved instances require commitment
        commitment_hours = workload_profile.get('commitment_hours', 8760)  # 1 year
        
        if runtime_hours < commitment_hours * 0.7:  # Less than 70% utilization
            # Pro-rate the reserved cost
            effective_rate = hourly_rate * (commitment_hours / runtime_hours)
        else:
            effective_rate = hourly_rate
        
        return gpu_count * runtime_hours * effective_rate
    
    def _calculate_spot_cost(self, gpu_type, gpu_count, runtime_hours, workload_profile):
        """Calculate spot/preemptible instance cost"""
        
        base_hourly_rate = self.gpu_pricing[gpu_type]['spot']
        
        # Account for interruption overhead
        interruption_rate = workload_profile.get('interruption_rate', 0.1)  # 10% chance per hour
        overhead_factor = 1 + (interruption_rate * 0.2)  # 20% overhead per interruption
        
        effective_rate = base_hourly_rate * overhead_factor
        
        return gpu_count * runtime_hours * effective_rate
    
    def _calculate_preemptible_cost(self, gpu_type, gpu_count, runtime_hours, workload_profile):
        """Calculate preemptible instance cost (Google Cloud)"""
        
        # Similar to spot but with different pricing model
        return self._calculate_spot_cost(gpu_type, gpu_count, runtime_hours, workload_profile)
    
    def _generate_cost_recommendations(self, cost_analysis, workload_profile):
        """Generate cost optimization recommendations"""
        
        recommendations = []
        
        # Find cheapest option
        costs = {k: v['total_cost'] for k, v in cost_analysis.items() if k != 'recommendations'}
        cheapest_option = min(costs.items(), key=lambda x: x[1])
        
        recommendations.append(f"Use {cheapest_option[0]} pricing for lowest cost: ${cheapest_option[1]:.2f}")
        
        # Runtime optimization
        runtime_hours = workload_profile['runtime_hours']
        if runtime_hours > 168:  # > 1 week
            recommendations.append("Consider reserved instances for long-running workloads")
        
        # Fault tolerance optimization
        fault_tolerance = workload_profile.get('fault_tolerance', 'high')
        if fault_tolerance == 'medium':
            recommendations.append("Use spot instances with checkpointing for cost savings")
        
        return recommendations
    
    def optimize_cluster_scaling(self, workload_schedule, cluster_config):
        """Optimize cluster scaling to minimize costs"""
        
        optimization_plan = {
            'scaling_schedule': [],
            'cost_savings': 0,
            'recommendations': []
        }
        
        # Analyze workload patterns
        peak_hours = self._identify_peak_hours(workload_schedule)
        off_peak_hours = self._identify_off_peak_hours(workload_schedule)
        
        # Generate scaling plan
        for hour, workload in workload_schedule.items():
            required_gpus = workload['gpu_demand']
            
            if hour in peak_hours:
                # Scale up with on-demand instances
                scaling_plan = {
                    'hour': hour,
                    'action': 'scale_up',
                    'gpu_count': required_gpus,
                    'instance_type': 'on_demand'
                }
            elif hour in off_peak_hours:
                # Scale down or use spot instances
                scaling_plan = {
                    'hour': hour,
                    'action': 'scale_down' if required_gpus < cluster_config['min_gpus'] else 'maintain',
                    'gpu_count': max(required_gpus, cluster_config['min_gpus']),
                    'instance_type': 'spot'
                }
            else:
                # Maintain baseline
                scaling_plan = {
                    'hour': hour,
                    'action': 'maintain',
                    'gpu_count': required_gpus,
                    'instance_type': 'reserved'
                }
            
            optimization_plan['scaling_schedule'].append(scaling_plan)
        
        # Calculate cost savings
        baseline_cost = self._calculate_baseline_cost(workload_schedule, cluster_config)
        optimized_cost = self._calculate_optimized_cost(optimization_plan, cluster_config)
        optimization_plan['cost_savings'] = baseline_cost - optimized_cost
        
        return optimization_plan
    
    def _identify_peak_hours(self, workload_schedule):
        """Identify peak demand hours"""
        
        demands = [workload['gpu_demand'] for workload in workload_schedule.values()]
        threshold = np.percentile(demands, 80)  # Top 20% as peak
        
        peak_hours = [
            hour for hour, workload in workload_schedule.items()
            if workload['gpu_demand'] >= threshold
        ]
        
        return peak_hours
    
    def _identify_off_peak_hours(self, workload_schedule):
        """Identify off-peak demand hours"""
        
        demands = [workload['gpu_demand'] for workload in workload_schedule.values()]
        threshold = np.percentile(demands, 20)  # Bottom 20% as off-peak
        
        off_peak_hours = [
            hour for hour, workload in workload_schedule.items()
            if workload['gpu_demand'] <= threshold
        ]
        
        return off_peak_hours

class ResourceRightSizer:
    def __init__(self):
        pass
    
    def analyze_resource_utilization(self, usage_history):
        """Analyze historical resource utilization"""
        
        analysis = {
            'cpu_utilization': {},
            'memory_utilization': {},
            'gpu_utilization': {},
            'recommendations': []
        }
        
        # Analyze each metric
        for metric_type in ['cpu', 'memory', 'gpu']:
            metric_key = f'{metric_type}_utilization'
            
            if metric_key in usage_history:
                utilization_data = usage_history[metric_key]
                
                analysis[metric_key] = {
                    'mean': np.mean(utilization_data),
                    'median': np.median(utilization_data),
                    'p95': np.percentile(utilization_data, 95),
                    'p99': np.percentile(utilization_data, 99),
                    'std': np.std(utilization_data)
                }
                
                # Generate recommendations
                mean_util = analysis[metric_key]['mean']
                p95_util = analysis[metric_key]['p95']
                
                if mean_util < 30:
                    analysis['recommendations'].append(
                        f"Consider downsizing {metric_type} resources (mean utilization: {mean_util:.1f}%)"
                    )
                elif p95_util > 90:
                    analysis['recommendations'].append(
                        f"Consider upsizing {metric_type} resources (95th percentile: {p95_util:.1f}%)"
                    )
        
        return analysis
    
    def recommend_instance_types(self, workload_characteristics):
        """Recommend optimal instance types based on workload"""
        
        recommendations = []
        
        compute_intensity = workload_characteristics.get('compute_intensity', 'medium')
        memory_requirements = workload_characteristics.get('memory_gb', 32)
        gpu_requirements = workload_characteristics.get('gpu_memory_gb', 16)
        network_requirements = workload_characteristics.get('network_intensive', False)
        
        # GPU recommendations
        if gpu_requirements <= 16:
            recommendations.append({
                'resource_type': 'gpu',
                'instance_type': 'V100-16GB',
                'reason': 'Sufficient for most ML workloads'
            })
        elif gpu_requirements <= 40:
            recommendations.append({
                'resource_type': 'gpu',
                'instance_type': 'A100-40GB',
                'reason': 'Better performance for larger models'
            })
        else:
            recommendations.append({
                'resource_type': 'gpu',
                'instance_type': 'A100-80GB or H100',
                'reason': 'Required for very large models'
            })
        
        # CPU recommendations
        if compute_intensity == 'high':
            recommendations.append({
                'resource_type': 'cpu',
                'instance_type': 'compute_optimized',
                'reason': 'High compute intensity workload'
            })
        elif network_requirements:
            recommendations.append({
                'resource_type': 'cpu',
                'instance_type': 'network_optimized',
                'reason': 'Network intensive workload'
            })
        else:
            recommendations.append({
                'resource_type': 'cpu',
                'instance_type': 'general_purpose',
                'reason': 'Balanced workload requirements'
            })
        
        return recommendations
```

================================================================================
6. BEST PRACTICES AND IMPLEMENTATION
================================================================================

6.1 Implementation Guidelines
-----------------------------
```python
class GPUClusterBestPractices:
    @staticmethod
    def get_implementation_checklist():
        return {
            'infrastructure_setup': [
                'Design network topology for high bandwidth',
                'Configure InfiniBand or high-speed Ethernet',
                'Set up shared storage with adequate IOPS',
                'Implement proper cooling and power management',
                'Configure monitoring and alerting systems'
            ],
            'software_configuration': [
                'Install CUDA drivers and libraries',
                'Configure NCCL for optimal communication',
                'Set up container runtime (Docker/Podman)',
                'Install orchestration system (Kubernetes/Slurm)',
                'Configure resource scheduling policies'
            ],
            'optimization': [
                'Profile applications for bottlenecks',
                'Optimize data loading pipelines',
                'Implement efficient memory management',
                'Configure optimal batch sizes',
                'Enable mixed precision training'
            ],
            'monitoring': [
                'Track GPU utilization and memory usage',
                'Monitor network bandwidth and latency',
                'Set up performance alerting',
                'Log resource allocation and job completion',
                'Implement cost tracking and reporting'
            ]
        }
    
    @staticmethod
    def get_performance_optimization_guide():
        return {
            'memory_optimization': [
                'Use gradient checkpointing for large models',
                'Enable mixed precision (FP16/BF16) training',
                'Optimize batch size for memory efficiency',
                'Clear unused variables and call torch.cuda.empty_cache()',
                'Use memory mapping for large datasets'
            ],
            'compute_optimization': [
                'Overlap data loading with computation',
                'Use efficient data formats (TensorFlow Records, WebDataset)',
                'Optimize model architecture for GPU utilization',
                'Use fused operations where possible',
                'Profile and optimize hot code paths'
            ],
            'communication_optimization': [
                'Use NCCL for multi-GPU communication',
                'Enable gradient compression for large models',
                'Optimize all-reduce algorithms (ring, tree, hierarchical)',
                'Use async communication where possible',
                'Minimize data movement between GPUs'
            ],
            'scaling_optimization': [
                'Start with strong single-GPU baseline',
                'Choose appropriate parallelism strategy',
                'Monitor scaling efficiency (aim for >70%)',
                'Optimize batch size for distributed training',
                'Implement proper synchronization patterns'
            ]
        }
    
    @staticmethod
    def get_common_pitfalls():
        return {
            'resource_management': [
                'Overcommitting GPU memory',
                'Not accounting for communication overhead',
                'Inefficient job scheduling policies',
                'Poor load balancing across nodes'
            ],
            'performance': [
                'Suboptimal batch sizes',
                'CPU bottlenecks in data loading',
                'Inefficient memory access patterns',
                'Not utilizing tensor cores effectively'
            ],
            'cost_management': [
                'Using on-demand pricing for long workloads',
                'Not right-sizing instances for workloads',
                'Leaving idle resources running',
                'Not monitoring cost metrics'
            ],
            'operations': [
                'Insufficient monitoring and alerting',
                'Poor fault tolerance and recovery',
                'Manual processes that should be automated',
                'Inadequate capacity planning'
            ]
        }

# Success metrics for GPU cluster implementation
SUCCESS_METRICS = {
    'performance': {
        'gpu_utilization': 'Target: >80% average utilization',
        'training_throughput': 'Target: Linear scaling up to 32 GPUs',
        'job_completion_rate': 'Target: >95% successful completion',
        'queue_wait_time': 'Target: <30 minutes median wait time'
    },
    'cost_efficiency': {
        'cost_per_model': 'Target: <50% of baseline cost',
        'resource_utilization': 'Target: >70% overall utilization',
        'spot_instance_usage': 'Target: >60% of workloads on spot instances',
        'cost_predictability': 'Target: <10% variance from budget'
    },
    'reliability': {
        'system_availability': 'Target: >99.5% uptime',
        'fault_recovery_time': 'Target: <5 minutes automatic recovery',
        'data_loss_incidents': 'Target: Zero data loss events',
        'security_incidents': 'Target: Zero security breaches'
    },
    'operational': {
        'deployment_time': 'Target: <4 hours for new workloads',
        'scaling_response_time': 'Target: <10 minutes for auto-scaling',
        'maintenance_window': 'Target: <2 hours monthly maintenance',
        'user_satisfaction': 'Target: >4.5/5 user satisfaction score'
    }
}

# Production deployment configuration
CLUSTER_DEPLOYMENT_CONFIG = {
    'small_cluster': {
        'description': '8-16 GPUs for small teams',
        'hardware': {
            'nodes': 2,
            'gpus_per_node': 8,
            'gpu_type': 'A100-40GB',
            'network': '100Gb Ethernet',
            'storage': 'NFS 100TB'
        },
        'software': {
            'scheduler': 'Slurm',
            'container_runtime': 'Docker',
            'monitoring': 'Prometheus + Grafana'
        }
    },
    'medium_cluster': {
        'description': '64-128 GPUs for medium teams',
        'hardware': {
            'nodes': 16,
            'gpus_per_node': 8,
            'gpu_type': 'A100-80GB',
            'network': 'InfiniBand HDR',
            'storage': 'Lustre 500TB'
        },
        'software': {
            'scheduler': 'Slurm + Kubernetes',
            'container_runtime': 'Singularity',
            'monitoring': 'Prometheus + Grafana + ELK'
        }
    },
    'large_cluster': {
        'description': '512+ GPUs for large organizations',
        'hardware': {
            'nodes': 64,
            'gpus_per_node': 8,
            'gpu_type': 'H100-80GB',
            'network': 'InfiniBand NDR',
            'storage': 'Distributed filesystem 2PB'
        },
        'software': {
            'scheduler': 'Advanced scheduling with preemption',
            'container_runtime': 'Kubernetes + Operator',
            'monitoring': 'Full observability stack'
        }
    }
}
```

================================================================================
SUMMARY AND KEY TAKEAWAYS
================================================================================

GPU cluster optimization is essential for efficient ML at scale:

**Key Architecture Components:**
- **GPU Hardware:** Choose appropriate GPU types (V100, A100, H100) based on workload requirements
- **Network Topology:** High-bandwidth interconnects (InfiniBand, 100Gb+ Ethernet) for multi-GPU communication
- **Storage Systems:** High-performance shared storage for data access across cluster
- **Cooling and Power:** Adequate infrastructure for high-density GPU deployments

**Resource Management:**
- **Scheduling Strategies:** FIFO, priority-based, fair-share, and preemptive scheduling
- **Resource Allocation:** Dynamic allocation based on job requirements and priorities
- **Load Balancing:** Efficient distribution of workloads across available resources
- **Queue Management:** Intelligent queuing with dependency handling and backfill

**Performance Optimization:**
- **Memory Optimization:** Gradient checkpointing, mixed precision, optimal batch sizing
- **Compute Utilization:** Overlap data loading with computation, optimize model architecture
- **Communication Efficiency:** NCCL optimization, gradient compression, topology-aware placement
- **Scaling Efficiency:** Monitor and optimize scaling beyond single GPU performance

**Cost Optimization:**
- **Pricing Models:** Strategic use of on-demand, reserved, and spot instances
- **Right-sizing:** Match instance types to workload characteristics
- **Auto-scaling:** Dynamic scaling based on demand patterns
- **Resource Utilization:** Maximize utilization through efficient scheduling

**Monitoring and Troubleshooting:**
- **Performance Metrics:** GPU utilization, memory usage, network bandwidth, job completion rates
- **Cost Tracking:** Monitor spend across different pricing models and instance types
- **Bottleneck Analysis:** Identify and resolve memory, compute, and communication bottlenecks
- **Predictive Analytics:** Anticipate failures and capacity needs

**Best Practices:**
- Design for scalability from the beginning
- Implement comprehensive monitoring and alerting
- Optimize for both performance and cost efficiency
- Plan for fault tolerance and recovery
- Regular performance profiling and optimization

**Success Factors:**
- Clear understanding of workload characteristics and requirements
- Appropriate hardware selection and network design
- Efficient resource management and scheduling policies
- Comprehensive optimization across the entire stack
- Continuous monitoring and improvement processes

Effective GPU cluster management enables organizations to maximize the value of their GPU investments while maintaining high performance and cost efficiency for ML workloads. 