COST OPTIMIZATION AND RESOURCE MANAGEMENT
=========================================

Table of Contents:
1. Cost Analysis Fundamentals
2. Cloud Cost Optimization
3. Resource Allocation Strategies
4. Auto-scaling and Dynamic Sizing
5. Cost Monitoring and Reporting
6. Resource Governance
7. Performance vs Cost Trade-offs
8. Best Practices and Implementation

================================================================================
1. COST ANALYSIS FUNDAMENTALS
================================================================================

1.1 ML Cost Components
----------------------
**Compute Costs:**
- Training: GPU/CPU hours for model training
- Inference: Serving infrastructure costs
- Development: Experimentation and prototyping
- Storage: Data storage and model artifacts

**Data Costs:**
- Data acquisition and licensing
- Data processing and transformation
- Data transfer and bandwidth
- Storage tiers and lifecycle management

**Personnel Costs:**
- Data scientist and ML engineer time
- Infrastructure and DevOps support
- Model development and iteration
- Operational maintenance

**Infrastructure Costs:**
- Cloud services and licensing
- Monitoring and observability tools
- Security and compliance systems
- Disaster recovery and backup

1.2 Cost Modeling Framework
---------------------------
```python
from dataclasses import dataclass
from typing import Dict, List, Optional
import numpy as np
from datetime import datetime, timedelta

@dataclass
class ResourceCost:
    resource_type: str
    unit_cost: float
    usage_hours: float
    region: str
    pricing_model: str  # on-demand, reserved, spot

@dataclass
class WorkloadCostProfile:
    workload_id: str
    compute_costs: List[ResourceCost]
    storage_costs: List[ResourceCost]
    data_transfer_costs: float
    total_cost: float
    cost_per_prediction: Optional[float] = None

class CostAnalyzer:
    def __init__(self):
        self.cost_history = []
        self.cost_models = {
            'aws': self._aws_pricing_model,
            'gcp': self._gcp_pricing_model,
            'azure': self._azure_pricing_model,
            'on_premise': self._on_premise_pricing_model
        }
    
    def analyze_workload_cost(self, workload_config, cloud_provider='aws'):
        """Analyze total cost of ownership for ML workload"""
        
        cost_breakdown = {
            'training_cost': 0,
            'inference_cost': 0,
            'storage_cost': 0,
            'data_transfer_cost': 0,
            'total_cost': 0
        }
        
        # Training cost calculation
        training_config = workload_config.get('training', {})
        if training_config:
            cost_breakdown['training_cost'] = self._calculate_training_cost(
                training_config, cloud_provider
            )
        
        # Inference cost calculation
        inference_config = workload_config.get('inference', {})
        if inference_config:
            cost_breakdown['inference_cost'] = self._calculate_inference_cost(
                inference_config, cloud_provider
            )
        
        # Storage cost calculation
        storage_config = workload_config.get('storage', {})
        if storage_config:
            cost_breakdown['storage_cost'] = self._calculate_storage_cost(
                storage_config, cloud_provider
            )
        
        # Data transfer cost
        data_transfer = workload_config.get('data_transfer_gb', 0)
        cost_breakdown['data_transfer_cost'] = self._calculate_data_transfer_cost(
            data_transfer, cloud_provider
        )
        
        # Total cost
        cost_breakdown['total_cost'] = sum(cost_breakdown.values())
        
        return cost_breakdown
    
    def _calculate_training_cost(self, training_config, provider):
        """Calculate training cost based on configuration"""
        
        gpu_type = training_config.get('gpu_type', 'V100')
        gpu_count = training_config.get('gpu_count', 1)
        training_hours = training_config.get('training_hours', 24)
        pricing_model = training_config.get('pricing_model', 'on_demand')
        
        # Get pricing for GPU type
        pricing = self.cost_models[provider](gpu_type, pricing_model)
        
        total_cost = pricing * gpu_count * training_hours
        
        # Add storage cost for checkpoints
        checkpoint_storage_gb = training_config.get('checkpoint_storage_gb', 10)
        storage_cost = checkpoint_storage_gb * 0.023 * (training_hours / 24 / 30)  # S3 pricing
        
        return total_cost + storage_cost
    
    def _calculate_inference_cost(self, inference_config, provider):
        """Calculate inference serving cost"""
        
        instance_type = inference_config.get('instance_type', 'ml.m5.large')
        requests_per_day = inference_config.get('requests_per_day', 10000)
        days_per_month = inference_config.get('days_per_month', 30)
        
        # Calculate required capacity
        avg_latency_ms = inference_config.get('avg_latency_ms', 100)
        requests_per_second = requests_per_day / (24 * 3600)
        
        # Estimate instance hours needed
        utilization_factor = 0.7  # 70% utilization target
        instance_capacity = 1000 / avg_latency_ms  # requests per second
        instances_needed = max(1, requests_per_second / (instance_capacity * utilization_factor))
        
        hours_per_month = days_per_month * 24
        total_instance_hours = instances_needed * hours_per_month
        
        # Get pricing
        pricing = self.cost_models[provider](instance_type, 'on_demand')
        
        return total_instance_hours * pricing
    
    def _aws_pricing_model(self, resource_type, pricing_model):
        """AWS pricing model (simplified)"""
        
        aws_pricing = {
            'V100': {'on_demand': 3.06, 'reserved': 1.84, 'spot': 0.92},
            'A100': {'on_demand': 4.56, 'reserved': 2.74, 'spot': 1.37},
            'ml.m5.large': {'on_demand': 0.096, 'reserved': 0.058},
            'ml.g4dn.xlarge': {'on_demand': 0.736, 'reserved': 0.442}
        }
        
        return aws_pricing.get(resource_type, {}).get(pricing_model, 0)
    
    def compare_cloud_providers(self, workload_config):
        """Compare costs across cloud providers"""
        
        comparison = {}
        
        for provider in ['aws', 'gcp', 'azure']:
            cost_breakdown = self.analyze_workload_cost(workload_config, provider)
            comparison[provider] = cost_breakdown
        
        # Find cheapest provider
        cheapest_provider = min(
            comparison.items(), 
            key=lambda x: x[1]['total_cost']
        )
        
        return {
            'comparison': comparison,
            'recommended_provider': cheapest_provider[0],
            'potential_savings': self._calculate_savings(comparison)
        }
    
    def _calculate_savings(self, comparison):
        """Calculate potential savings between providers"""
        
        costs = [details['total_cost'] for details in comparison.values()]
        max_cost = max(costs)
        min_cost = min(costs)
        
        return {
            'absolute_savings': max_cost - min_cost,
            'percentage_savings': ((max_cost - min_cost) / max_cost) * 100 if max_cost > 0 else 0
        }

class CostForecastingEngine:
    def __init__(self):
        self.historical_data = []
        self.forecast_models = {
            'linear_trend': self._linear_trend_forecast,
            'seasonal': self._seasonal_forecast,
            'ml_forecast': self._ml_based_forecast
        }
    
    def forecast_costs(self, historical_costs, forecast_horizon_months=12, model='linear_trend'):
        """Forecast future costs based on historical data"""
        
        if len(historical_costs) < 3:
            return {'error': 'Insufficient historical data for forecasting'}
        
        forecast_func = self.forecast_models.get(model, self._linear_trend_forecast)
        forecast = forecast_func(historical_costs, forecast_horizon_months)
        
        return {
            'forecast_values': forecast,
            'confidence_interval': self._calculate_confidence_interval(historical_costs, forecast),
            'total_forecasted_cost': sum(forecast),
            'growth_rate': self._calculate_growth_rate(historical_costs)
        }
    
    def _linear_trend_forecast(self, historical_costs, horizon):
        """Simple linear trend forecasting"""
        
        # Fit linear trend
        x = np.arange(len(historical_costs))
        coeffs = np.polyfit(x, historical_costs, 1)
        
        # Generate forecast
        future_x = np.arange(len(historical_costs), len(historical_costs) + horizon)
        forecast = np.polyval(coeffs, future_x)
        
        return forecast.tolist()
    
    def _calculate_growth_rate(self, historical_costs):
        """Calculate cost growth rate"""
        
        if len(historical_costs) < 2:
            return 0
        
        total_growth = (historical_costs[-1] - historical_costs[0]) / historical_costs[0]
        periods = len(historical_costs) - 1
        monthly_growth_rate = (1 + total_growth) ** (1/periods) - 1
        
        return monthly_growth_rate * 100  # Percentage
```

================================================================================
2. CLOUD COST OPTIMIZATION
================================================================================

2.1 Multi-Cloud Cost Strategy
-----------------------------
```python
class MultiCloudCostOptimizer:
    def __init__(self):
        self.provider_configs = {}
        self.cost_policies = []
        
    def optimize_workload_placement(self, workloads, constraints):
        """Optimize workload placement across cloud providers"""
        
        optimization_results = {
            'placement_recommendations': [],
            'cost_savings': 0,
            'performance_impact': {},
            'risk_assessment': {}
        }
        
        for workload in workloads:
            best_placement = self._find_optimal_placement(workload, constraints)
            optimization_results['placement_recommendations'].append(best_placement)
        
        return optimization_results
    
    def _find_optimal_placement(self, workload, constraints):
        """Find optimal cloud provider for workload"""
        
        placement_options = []
        
        for provider in ['aws', 'gcp', 'azure']:
            if self._meets_constraints(provider, workload, constraints):
                cost = self._calculate_workload_cost(workload, provider)
                performance_score = self._calculate_performance_score(workload, provider)
                
                placement_options.append({
                    'provider': provider,
                    'cost': cost,
                    'performance_score': performance_score,
                    'composite_score': self._calculate_composite_score(cost, performance_score)
                })
        
        # Sort by composite score (lower is better)
        placement_options.sort(key=lambda x: x['composite_score'])
        
        return placement_options[0] if placement_options else None
    
    def implement_reserved_instance_strategy(self, usage_forecast, commitment_level='moderate'):
        """Implement reserved instance purchasing strategy"""
        
        strategy = {
            'reserved_instance_purchases': [],
            'coverage_percentage': 0,
            'estimated_savings': 0
        }
        
        # Analyze usage patterns
        baseline_usage = self._calculate_baseline_usage(usage_forecast)
        
        # Determine RI coverage based on commitment level
        coverage_targets = {
            'conservative': 0.5,  # 50% coverage
            'moderate': 0.7,      # 70% coverage
            'aggressive': 0.85    # 85% coverage
        }
        
        target_coverage = coverage_targets.get(commitment_level, 0.7)
        
        # Generate RI recommendations
        for resource_type, usage in baseline_usage.items():
            ri_quantity = int(usage * target_coverage)
            
            if ri_quantity > 0:
                on_demand_cost = usage * self._get_on_demand_price(resource_type) * 8760  # Annual
                ri_cost = ri_quantity * self._get_reserved_price(resource_type) * 8760
                savings = on_demand_cost - ri_cost
                
                strategy['reserved_instance_purchases'].append({
                    'resource_type': resource_type,
                    'quantity': ri_quantity,
                    'term': '1_year',
                    'estimated_savings': savings
                })
        
        strategy['estimated_savings'] = sum(
            ri['estimated_savings'] for ri in strategy['reserved_instance_purchases']
        )
        
        return strategy
    
    def optimize_spot_instance_usage(self, workload_characteristics):
        """Optimize spot instance usage for cost savings"""
        
        spot_strategy = {
            'eligible_workloads': [],
            'cost_savings_potential': 0,
            'risk_mitigation': []
        }
        
        for workload in workload_characteristics:
            fault_tolerance = workload.get('fault_tolerance', 'low')
            runtime_hours = workload.get('runtime_hours', 1)
            checkpointing_support = workload.get('supports_checkpointing', False)
            
            # Determine spot eligibility
            if fault_tolerance in ['medium', 'high'] or checkpointing_support:
                savings_potential = self._calculate_spot_savings(workload)
                
                spot_strategy['eligible_workloads'].append({
                    'workload_id': workload['id'],
                    'savings_potential': savings_potential,
                    'recommended_strategy': self._recommend_spot_strategy(workload)
                })
        
        return spot_strategy
    
    def _recommend_spot_strategy(self, workload):
        """Recommend spot instance strategy for workload"""
        
        if workload.get('supports_checkpointing', False):
            return {
                'strategy': 'checkpointing',
                'checkpoint_frequency': '15_minutes',
                'max_interruption_tolerance': '5_minutes'
            }
        elif workload.get('fault_tolerance') == 'high':
            return {
                'strategy': 'multi_az_diversification',
                'instance_types': ['mixed'],
                'max_price': '50_percent_on_demand'
            }
        else:
            return {
                'strategy': 'batch_processing',
                'queue_management': 'priority_based',
                'retry_policy': 'exponential_backoff'
            }

class CostGovernanceEngine:
    def __init__(self):
        self.budgets = {}
        self.policies = []
        self.alerts = []
        
    def set_budget(self, budget_name, budget_config):
        """Set cost budget with alerts and controls"""
        
        self.budgets[budget_name] = {
            'total_budget': budget_config['amount'],
            'time_period': budget_config['period'],
            'alert_thresholds': budget_config.get('alert_thresholds', [50, 80, 100]),
            'auto_actions': budget_config.get('auto_actions', []),
            'current_spend': 0,
            'forecasted_spend': 0
        }
    
    def enforce_cost_policies(self, resource_request):
        """Enforce cost governance policies on resource requests"""
        
        enforcement_result = {
            'approved': True,
            'policy_violations': [],
            'required_approvals': [],
            'cost_estimate': 0
        }
        
        # Calculate cost estimate
        cost_estimate = self._estimate_request_cost(resource_request)
        enforcement_result['cost_estimate'] = cost_estimate
        
        # Check against policies
        for policy in self.policies:
            violation = self._check_policy_compliance(resource_request, policy, cost_estimate)
            if violation:
                enforcement_result['policy_violations'].append(violation)
                
                if policy.get('blocking', False):
                    enforcement_result['approved'] = False
        
        return enforcement_result
    
    def _check_policy_compliance(self, request, policy, cost_estimate):
        """Check if request complies with cost policy"""
        
        if policy['type'] == 'max_cost_per_request':
            if cost_estimate > policy['limit']:
                return {
                    'policy': policy['name'],
                    'violation': f"Cost estimate ${cost_estimate} exceeds limit ${policy['limit']}"
                }
        
        elif policy['type'] == 'instance_type_restriction':
            allowed_types = policy['allowed_types']
            requested_type = request.get('instance_type')
            if requested_type not in allowed_types:
                return {
                    'policy': policy['name'],
                    'violation': f"Instance type {requested_type} not in allowed list"
                }
        
        elif policy['type'] == 'region_restriction':
            allowed_regions = policy['allowed_regions']
            requested_region = request.get('region')
            if requested_region not in allowed_regions:
                return {
                    'policy': policy['name'],
                    'violation': f"Region {requested_region} not in allowed list"
                }
        
        return None
    
    def generate_cost_alerts(self, current_spend, forecast_spend):
        """Generate cost alerts based on thresholds"""
        
        alerts = []
        
        for budget_name, budget in self.budgets.items():
            total_budget = budget['total_budget']
            alert_thresholds = budget['alert_thresholds']
            
            # Check current spend alerts
            for threshold in alert_thresholds:
                threshold_amount = (threshold / 100) * total_budget
                
                if current_spend >= threshold_amount and threshold not in budget.get('triggered_alerts', []):
                    alerts.append({
                        'type': 'budget_threshold',
                        'budget': budget_name,
                        'threshold': threshold,
                        'current_spend': current_spend,
                        'budget_amount': total_budget,
                        'severity': 'critical' if threshold >= 100 else 'warning'
                    })
            
            # Check forecast alerts
            if forecast_spend > total_budget * 1.1:  # 10% over budget
                alerts.append({
                    'type': 'forecast_overrun',
                    'budget': budget_name,
                    'forecasted_spend': forecast_spend,
                    'budget_amount': total_budget,
                    'projected_overrun': forecast_spend - total_budget,
                    'severity': 'warning'
                })
        
        return alerts
```

================================================================================
3. RESOURCE ALLOCATION STRATEGIES
================================================================================

3.1 Dynamic Resource Allocation
-------------------------------
```python
class DynamicResourceAllocator:
    def __init__(self):
        self.allocation_policies = {}
        self.resource_pools = {}
        self.allocation_history = []
        
    def allocate_resources(self, workload_requirements, allocation_strategy='cost_optimized'):
        """Allocate resources based on strategy and requirements"""
        
        allocation_strategies = {
            'cost_optimized': self._cost_optimized_allocation,
            'performance_optimized': self._performance_optimized_allocation,
            'balanced': self._balanced_allocation,
            'green_computing': self._green_optimized_allocation
        }
        
        strategy_func = allocation_strategies.get(allocation_strategy, self._balanced_allocation)
        allocation = strategy_func(workload_requirements)
        
        # Record allocation
        self.allocation_history.append({
            'timestamp': datetime.now(),
            'workload_id': workload_requirements.get('workload_id'),
            'allocation': allocation,
            'strategy': allocation_strategy
        })
        
        return allocation
    
    def _cost_optimized_allocation(self, requirements):
        """Allocate resources optimizing for minimum cost"""
        
        # Prefer spot instances and lower-cost regions
        allocation = {
            'compute_resources': [],
            'storage_resources': [],
            'estimated_cost': 0,
            'estimated_completion_time': 0
        }
        
        # Compute allocation
        gpu_count = requirements.get('gpu_count', 1)
        memory_gb = requirements.get('memory_gb', 32)
        
        # Choose cost-effective instance types
        instance_options = self._get_instance_options(gpu_count, memory_gb)
        cheapest_option = min(instance_options, key=lambda x: x['hourly_cost'])
        
        allocation['compute_resources'].append({
            'instance_type': cheapest_option['type'],
            'count': cheapest_option['count'],
            'pricing_model': 'spot',
            'hourly_cost': cheapest_option['hourly_cost']
        })
        
        # Storage allocation - prefer cheaper storage tiers
        storage_gb = requirements.get('storage_gb', 100)
        allocation['storage_resources'].append({
            'storage_type': 'standard',
            'size_gb': storage_gb,
            'monthly_cost': storage_gb * 0.023  # S3 standard pricing
        })
        
        return allocation
    
    def _performance_optimized_allocation(self, requirements):
        """Allocate resources optimizing for maximum performance"""
        
        allocation = {
            'compute_resources': [],
            'storage_resources': [],
            'estimated_cost': 0,
            'estimated_completion_time': 0
        }
        
        # Use high-performance instances
        gpu_count = requirements.get('gpu_count', 1)
        
        allocation['compute_resources'].append({
            'instance_type': 'p4d.24xlarge',  # High-performance GPU instance
            'count': max(1, gpu_count // 8),
            'pricing_model': 'on_demand',
            'network_optimized': True
        })
        
        # Use high-performance storage
        storage_gb = requirements.get('storage_gb', 100)
        allocation['storage_resources'].append({
            'storage_type': 'nvme_ssd',
            'size_gb': storage_gb,
            'iops': 50000
        })
        
        return allocation
    
    def _green_optimized_allocation(self, requirements):
        """Allocate resources optimizing for energy efficiency"""
        
        allocation = {
            'compute_resources': [],
            'storage_resources': [],
            'carbon_footprint': 0,
            'renewable_energy_percentage': 0
        }
        
        # Prefer regions with renewable energy
        green_regions = ['us-west-2', 'eu-north-1', 'ca-central-1']
        selected_region = green_regions[0]  # Choose based on actual green energy data
        
        # Choose energy-efficient instance types
        gpu_count = requirements.get('gpu_count', 1)
        
        allocation['compute_resources'].append({
            'instance_type': 'g4dn.xlarge',  # More energy efficient
            'count': gpu_count,
            'region': selected_region,
            'carbon_footprint_kg': self._calculate_carbon_footprint(gpu_count, selected_region)
        })
        
        return allocation
    
    def optimize_resource_utilization(self, current_allocations):
        """Optimize current resource utilization"""
        
        optimization_recommendations = []
        
        for allocation in current_allocations:
            utilization_metrics = self._get_utilization_metrics(allocation['allocation_id'])
            
            # Check for underutilized resources
            if utilization_metrics['cpu_utilization'] < 30:
                optimization_recommendations.append({
                    'allocation_id': allocation['allocation_id'],
                    'type': 'downsize',
                    'resource': 'cpu',
                    'current_size': allocation['cpu_count'],
                    'recommended_size': max(1, allocation['cpu_count'] // 2),
                    'estimated_savings': allocation['hourly_cost'] * 0.3
                })
            
            if utilization_metrics['memory_utilization'] < 40:
                optimization_recommendations.append({
                    'allocation_id': allocation['allocation_id'],
                    'type': 'downsize',
                    'resource': 'memory',
                    'current_size': allocation['memory_gb'],
                    'recommended_size': max(8, allocation['memory_gb'] * 0.7),
                    'estimated_savings': allocation['hourly_cost'] * 0.2
                })
            
            # Check for overutilized resources
            if utilization_metrics['cpu_utilization'] > 90:
                optimization_recommendations.append({
                    'allocation_id': allocation['allocation_id'],
                    'type': 'upsize',
                    'resource': 'cpu',
                    'reason': 'High CPU utilization may impact performance'
                })
        
        return optimization_recommendations
    
    def implement_resource_pooling(self, workloads):
        """Implement resource pooling for better utilization"""
        
        pooling_strategy = {
            'shared_pools': [],
            'dedicated_pools': [],
            'utilization_improvement': 0
        }
        
        # Group similar workloads
        workload_groups = self._group_similar_workloads(workloads)
        
        for group_name, group_workloads in workload_groups.items():
            if len(group_workloads) >= 3:  # Worth pooling
                pool_config = self._design_shared_pool(group_workloads)
                pooling_strategy['shared_pools'].append({
                    'pool_name': group_name,
                    'workloads': [w['workload_id'] for w in group_workloads],
                    'pool_config': pool_config,
                    'estimated_utilization_improvement': pool_config['utilization_improvement']
                })
        
        return pooling_strategy
    
    def _group_similar_workloads(self, workloads):
        """Group workloads with similar resource requirements"""
        
        groups = {
            'cpu_intensive': [],
            'gpu_training': [],
            'gpu_inference': [],
            'memory_intensive': [],
            'io_intensive': []
        }
        
        for workload in workloads:
            if workload.get('gpu_count', 0) > 0:
                if workload.get('workload_type') == 'training':
                    groups['gpu_training'].append(workload)
                else:
                    groups['gpu_inference'].append(workload)
            elif workload.get('memory_gb', 0) > 64:
                groups['memory_intensive'].append(workload)
            elif workload.get('cpu_count', 0) > 16:
                groups['cpu_intensive'].append(workload)
            else:
                groups['io_intensive'].append(workload)
        
        return {k: v for k, v in groups.items() if v}  # Remove empty groups

class ResourceScheduler:
    def __init__(self):
        self.scheduling_policies = []
        self.resource_queue = []
        self.active_allocations = {}
        
    def schedule_workload(self, workload, scheduling_policy='priority'):
        """Schedule workload based on policy"""
        
        scheduling_policies = {
            'priority': self._priority_scheduling,
            'fair_share': self._fair_share_scheduling,
            'cost_aware': self._cost_aware_scheduling,
            'deadline_aware': self._deadline_aware_scheduling
        }
        
        policy_func = scheduling_policies.get(scheduling_policy, self._priority_scheduling)
        schedule_result = policy_func(workload)
        
        return schedule_result
    
    def _cost_aware_scheduling(self, workload):
        """Schedule workload considering cost constraints"""
        
        cost_budget = workload.get('cost_budget', float('inf'))
        deadline = workload.get('deadline')
        
        # Find cost-optimal resource allocation
        allocation_options = self._generate_allocation_options(workload)
        
        # Filter by cost budget
        feasible_options = [
            option for option in allocation_options
            if option['estimated_cost'] <= cost_budget
        ]
        
        if not feasible_options:
            return {
                'status': 'rejected',
                'reason': 'No feasible allocation within cost budget'
            }
        
        # Sort by completion time if deadline exists
        if deadline:
            feasible_options.sort(key=lambda x: x['estimated_completion_time'])
        else:
            feasible_options.sort(key=lambda x: x['estimated_cost'])
        
        selected_option = feasible_options[0]
        
        return {
            'status': 'scheduled',
            'allocation': selected_option,
            'estimated_start_time': self._estimate_start_time(selected_option),
            'estimated_completion_time': selected_option['estimated_completion_time']
        }
    
    def _deadline_aware_scheduling(self, workload):
        """Schedule workload to meet deadlines"""
        
        deadline = workload.get('deadline')
        if not deadline:
            return self._priority_scheduling(workload)
        
        current_time = datetime.now()
        time_to_deadline = (deadline - current_time).total_seconds() / 3600  # hours
        
        # Find allocation options that can meet deadline
        allocation_options = self._generate_allocation_options(workload)
        
        feasible_options = [
            option for option in allocation_options
            if option['estimated_completion_time'] <= time_to_deadline
        ]
        
        if not feasible_options:
            return {
                'status': 'deadline_missed',
                'reason': 'No allocation can meet the specified deadline'
            }
        
        # Choose cheapest option that meets deadline
        feasible_options.sort(key=lambda x: x['estimated_cost'])
        selected_option = feasible_options[0]
        
        return {
            'status': 'scheduled',
            'allocation': selected_option,
            'deadline_compliance': True
        }
```

================================================================================
4. AUTO-SCALING AND DYNAMIC SIZING
================================================================================

4.1 Intelligent Auto-scaling
----------------------------
```python
class IntelligentAutoScaler:
    def __init__(self):
        self.scaling_policies = []
        self.metrics_history = []
        self.scaling_actions = []
        
    def create_scaling_policy(self, policy_config):
        """Create auto-scaling policy"""
        
        policy = {
            'name': policy_config['name'],
            'target_metric': policy_config['target_metric'],
            'scale_up_threshold': policy_config['scale_up_threshold'],
            'scale_down_threshold': policy_config['scale_down_threshold'],
            'scale_up_action': policy_config['scale_up_action'],
            'scale_down_action': policy_config['scale_down_action'],
            'cooldown_period': policy_config.get('cooldown_period', 300),  # 5 minutes
            'min_capacity': policy_config.get('min_capacity', 1),
            'max_capacity': policy_config.get('max_capacity', 10)
        }
        
        self.scaling_policies.append(policy)
        return policy
    
    def evaluate_scaling_decision(self, current_metrics):
        """Evaluate if scaling action is needed"""
        
        scaling_decisions = []
        
        for policy in self.scaling_policies:
            decision = self._evaluate_policy(policy, current_metrics)
            if decision['action'] != 'no_action':
                scaling_decisions.append(decision)
        
        return scaling_decisions
    
    def _evaluate_policy(self, policy, metrics):
        """Evaluate individual scaling policy"""
        
        target_metric = policy['target_metric']
        current_value = metrics.get(target_metric, 0)
        
        # Check if in cooldown period
        if self._is_in_cooldown(policy):
            return {'action': 'no_action', 'reason': 'In cooldown period'}
        
        # Scale up decision
        if current_value > policy['scale_up_threshold']:
            return {
                'action': 'scale_up',
                'policy': policy['name'],
                'current_value': current_value,
                'threshold': policy['scale_up_threshold'],
                'scale_action': policy['scale_up_action']
            }
        
        # Scale down decision
        elif current_value < policy['scale_down_threshold']:
            return {
                'action': 'scale_down',
                'policy': policy['name'],
                'current_value': current_value,
                'threshold': policy['scale_down_threshold'],
                'scale_action': policy['scale_down_action']
            }
        
        return {'action': 'no_action', 'reason': 'Within thresholds'}
    
    def predictive_scaling(self, metrics_history, forecast_horizon_minutes=60):
        """Implement predictive scaling based on historical patterns"""
        
        if len(metrics_history) < 10:
            return {'error': 'Insufficient historical data for prediction'}
        
        # Extract time series data
        timestamps = [m['timestamp'] for m in metrics_history]
        values = [m['cpu_utilization'] for m in metrics_history]
        
        # Simple linear prediction (in practice, use more sophisticated models)
        forecast = self._forecast_metric(values, forecast_horizon_minutes)
        
        # Determine if preemptive scaling is needed
        scaling_recommendations = []
        
        for policy in self.scaling_policies:
            if any(v > policy['scale_up_threshold'] for v in forecast):
                scaling_recommendations.append({
                    'action': 'preemptive_scale_up',
                    'policy': policy['name'],
                    'predicted_max': max(forecast),
                    'confidence': 0.7  # Placeholder confidence score
                })
        
        return {
            'forecast': forecast,
            'recommendations': scaling_recommendations
        }
    
    def _forecast_metric(self, values, horizon_minutes):
        """Simple metric forecasting"""
        
        # Use linear regression for forecasting
        x = np.arange(len(values))
        coeffs = np.polyfit(x, values, 1)
        
        # Generate forecast points
        forecast_points = horizon_minutes // 5  # 5-minute intervals
        future_x = np.arange(len(values), len(values) + forecast_points)
        forecast = np.polyval(coeffs, future_x)
        
        return forecast.tolist()
    
    def implement_queue_based_scaling(self, queue_metrics):
        """Scale based on queue length and processing time"""
        
        queue_length = queue_metrics.get('queue_length', 0)
        avg_processing_time = queue_metrics.get('avg_processing_time_minutes', 5)
        target_wait_time = queue_metrics.get('target_wait_time_minutes', 10)
        
        # Calculate required capacity
        current_throughput = queue_metrics.get('current_throughput_per_minute', 1)
        required_throughput = queue_length / target_wait_time
        
        if required_throughput > current_throughput * 1.2:  # 20% buffer
            scale_factor = required_throughput / current_throughput
            
            return {
                'action': 'scale_up',
                'scale_factor': scale_factor,
                'reason': f'Queue length {queue_length} exceeds target wait time'
            }
        elif required_throughput < current_throughput * 0.5:  # Underutilized
            return {
                'action': 'scale_down',
                'scale_factor': required_throughput / current_throughput,
                'reason': 'Queue processing capacity is underutilized'
            }
        
        return {'action': 'no_action', 'reason': 'Queue processing within targets'}

class CostAwareScaler:
    def __init__(self):
        self.cost_thresholds = {}
        self.budget_tracking = {}
        
    def scale_with_cost_constraints(self, scaling_decision, cost_budget):
        """Implement scaling while respecting cost constraints"""
        
        if scaling_decision['action'] == 'scale_up':
            # Calculate cost impact
            additional_cost = self._calculate_scaling_cost(scaling_decision)
            current_spend = self.budget_tracking.get('current_spend', 0)
            
            if current_spend + additional_cost > cost_budget:
                # Try alternative scaling strategies
                alternative = self._find_cost_efficient_alternative(scaling_decision, cost_budget)
                
                if alternative:
                    return alternative
                else:
                    return {
                        'action': 'scale_denied',
                        'reason': 'Scaling would exceed cost budget',
                        'budget_impact': additional_cost
                    }
        
        return scaling_decision
    
    def _find_cost_efficient_alternative(self, original_decision, budget):
        """Find cost-efficient alternative to scaling decision"""
        
        alternatives = []
        
        # Try spot instances
        spot_cost = self._calculate_spot_instance_cost(original_decision)
        if spot_cost < budget:
            alternatives.append({
                'action': 'scale_up_spot',
                'cost_impact': spot_cost,
                'risk_level': 'medium'
            })
        
        # Try smaller scale-up
        reduced_scale = {
            **original_decision,
            'scale_factor': original_decision.get('scale_factor', 2) * 0.5
        }
        reduced_cost = self._calculate_scaling_cost(reduced_scale)
        
        if reduced_cost < budget:
            alternatives.append({
                'action': 'scale_up_reduced',
                'scale_factor': reduced_scale['scale_factor'],
                'cost_impact': reduced_cost,
                'risk_level': 'low'
            })
        
        # Return cheapest alternative
        if alternatives:
            return min(alternatives, key=lambda x: x['cost_impact'])
        
        return None
    
    def implement_scheduled_scaling(self, scaling_schedule, cost_optimization=True):
        """Implement scheduled scaling with cost optimization"""
        
        optimized_schedule = []
        
        for schedule_item in scaling_schedule:
            if cost_optimization:
                # Optimize timing for cost
                optimized_item = self._optimize_schedule_timing(schedule_item)
                optimized_schedule.append(optimized_item)
            else:
                optimized_schedule.append(schedule_item)
        
        return optimized_schedule
    
    def _optimize_schedule_timing(self, schedule_item):
        """Optimize scaling schedule timing for cost"""
        
        # Shift scaling to cheaper time periods if possible
        scheduled_time = schedule_item['scheduled_time']
        
        # Check if can delay scaling to cheaper hours
        cheaper_time = self._find_cheaper_time_slot(scheduled_time)
        
        if cheaper_time:
            return {
                **schedule_item,
                'scheduled_time': cheaper_time,
                'optimization_applied': 'time_shifted_for_cost',
                'estimated_savings': self._calculate_time_shift_savings(scheduled_time, cheaper_time)
            }
        
        return schedule_item
```

================================================================================
5. COST MONITORING AND REPORTING
================================================================================

5.1 Real-time Cost Monitoring
-----------------------------
```python
class CostMonitoringSystem:
    def __init__(self):
        self.cost_collectors = {}
        self.alerting_rules = []
        self.cost_dashboard = CostDashboard()
        
    def setup_cost_collection(self, provider_configs):
        """Setup cost data collection from cloud providers"""
        
        for provider, config in provider_configs.items():
            if provider == 'aws':
                self.cost_collectors[provider] = AWSCostCollector(config)
            elif provider == 'gcp':
                self.cost_collectors[provider] = GCPCostCollector(config)
            elif provider == 'azure':
                self.cost_collectors[provider] = AzureCostCollector(config)
    
    def collect_real_time_costs(self):
        """Collect real-time cost data from all providers"""
        
        aggregated_costs = {
            'total_cost': 0,
            'provider_breakdown': {},
            'service_breakdown': {},
            'resource_breakdown': {},
            'timestamp': datetime.now()
        }
        
        for provider, collector in self.cost_collectors.items():
            try:
                provider_costs = collector.get_current_costs()
                aggregated_costs['provider_breakdown'][provider] = provider_costs
                aggregated_costs['total_cost'] += provider_costs.get('total', 0)
            except Exception as e:
                print(f"Error collecting costs from {provider}: {e}")
        
        return aggregated_costs
    
    def analyze_cost_trends(self, time_window_hours=24):
        """Analyze cost trends over specified time window"""
        
        trend_analysis = {
            'hourly_costs': [],
            'growth_rate': 0,
            'cost_volatility': 0,
            'peak_hours': [],
            'recommendations': []
        }
        
        # Collect historical data
        historical_costs = self._get_historical_costs(time_window_hours)
        
        if len(historical_costs) > 1:
            # Calculate growth rate
            start_cost = historical_costs[0]['total_cost']
            end_cost = historical_costs[-1]['total_cost']
            growth_rate = ((end_cost - start_cost) / start_cost) * 100 if start_cost > 0 else 0
            
            trend_analysis['growth_rate'] = growth_rate
            
            # Calculate volatility
            costs = [entry['total_cost'] for entry in historical_costs]
            trend_analysis['cost_volatility'] = np.std(costs) / np.mean(costs) if costs else 0
            
            # Identify peak cost hours
            avg_cost = np.mean(costs)
            peak_threshold = avg_cost * 1.5
            
            trend_analysis['peak_hours'] = [
                entry['timestamp'].hour for entry in historical_costs
                if entry['total_cost'] > peak_threshold
            ]
        
        return trend_analysis
    
    def generate_cost_attribution(self, attribution_dimensions):
        """Generate cost attribution across different dimensions"""
        
        attribution = {}
        
        for dimension in attribution_dimensions:
            if dimension == 'team':
                attribution[dimension] = self._attribute_costs_by_team()
            elif dimension == 'project':
                attribution[dimension] = self._attribute_costs_by_project()
            elif dimension == 'environment':
                attribution[dimension] = self._attribute_costs_by_environment()
            elif dimension == 'resource_type':
                attribution[dimension] = self._attribute_costs_by_resource_type()
        
        return attribution
    
    def _attribute_costs_by_team(self):
        """Attribute costs to teams based on resource tags"""
        
        team_costs = {}
        
        for provider, collector in self.cost_collectors.items():
            tagged_costs = collector.get_costs_by_tag('team')
            
            for team, cost in tagged_costs.items():
                if team not in team_costs:
                    team_costs[team] = 0
                team_costs[team] += cost
        
        return team_costs
    
    def setup_cost_alerts(self, alert_configs):
        """Setup cost alerting rules"""
        
        for config in alert_configs:
            alert_rule = {
                'name': config['name'],
                'condition': config['condition'],
                'threshold': config['threshold'],
                'notification_channels': config['notification_channels'],
                'frequency': config.get('frequency', 'immediate')
            }
            
            self.alerting_rules.append(alert_rule)
    
    def check_cost_alerts(self, current_costs):
        """Check if any cost alerts should be triggered"""
        
        triggered_alerts = []
        
        for rule in self.alerting_rules:
            if self._evaluate_alert_condition(rule, current_costs):
                alert = {
                    'rule_name': rule['name'],
                    'condition': rule['condition'],
                    'current_value': current_costs.get(rule['condition']['metric'], 0),
                    'threshold': rule['threshold'],
                    'severity': rule.get('severity', 'warning'),
                    'timestamp': datetime.now()
                }
                
                triggered_alerts.append(alert)
                
                # Send notifications
                self._send_alert_notifications(alert, rule['notification_channels'])
        
        return triggered_alerts
    
    def _evaluate_alert_condition(self, rule, current_costs):
        """Evaluate if alert condition is met"""
        
        condition = rule['condition']
        metric = condition['metric']
        operator = condition['operator']
        threshold = rule['threshold']
        
        current_value = current_costs.get(metric, 0)
        
        if operator == 'greater_than':
            return current_value > threshold
        elif operator == 'less_than':
            return current_value < threshold
        elif operator == 'percentage_increase':
            baseline = condition.get('baseline', 0)
            increase = ((current_value - baseline) / baseline) * 100 if baseline > 0 else 0
            return increase > threshold
        
        return False

class CostDashboard:
    def __init__(self):
        self.widgets = []
        self.refresh_interval = 300  # 5 minutes
        
    def create_cost_overview_widget(self, cost_data):
        """Create cost overview widget"""
        
        widget = {
            'type': 'cost_overview',
            'data': {
                'total_cost': cost_data['total_cost'],
                'daily_average': cost_data.get('daily_average', 0),
                'monthly_projection': cost_data.get('monthly_projection', 0),
                'budget_utilization': cost_data.get('budget_utilization', 0)
            },
            'layout': {
                'position': {'x': 0, 'y': 0},
                'size': {'width': 4, 'height': 2}
            }
        }
        
        return widget
    
    def create_cost_trend_widget(self, trend_data):
        """Create cost trend chart widget"""
        
        widget = {
            'type': 'cost_trend_chart',
            'data': {
                'time_series': trend_data['hourly_costs'],
                'growth_rate': trend_data['growth_rate'],
                'forecast': trend_data.get('forecast', [])
            },
            'layout': {
                'position': {'x': 4, 'y': 0},
                'size': {'width': 8, 'height': 4}
            }
        }
        
        return widget
    
    def create_cost_breakdown_widget(self, breakdown_data):
        """Create cost breakdown pie chart widget"""
        
        widget = {
            'type': 'cost_breakdown_pie',
            'data': {
                'categories': breakdown_data,
                'total': sum(breakdown_data.values())
            },
            'layout': {
                'position': {'x': 0, 'y': 2},
                'size': {'width': 4, 'height': 4}
            }
        }
        
        return widget
    
    def generate_dashboard_config(self, cost_data, trend_data, breakdown_data):
        """Generate complete dashboard configuration"""
        
        dashboard_config = {
            'title': 'ML Cost Monitoring Dashboard',
            'refresh_interval': self.refresh_interval,
            'widgets': [
                self.create_cost_overview_widget(cost_data),
                self.create_cost_trend_widget(trend_data),
                self.create_cost_breakdown_widget(breakdown_data)
            ],
            'alerts': {
                'enabled': True,
                'notification_channels': ['email', 'slack']
            }
        }
        
        return dashboard_config

class CostReportGenerator:
    def __init__(self):
        self.report_templates = {}
        
    def generate_monthly_cost_report(self, cost_data, month, year):
        """Generate comprehensive monthly cost report"""
        
        report = {
            'report_metadata': {
                'title': f'Monthly Cost Report - {month}/{year}',
                'generated_at': datetime.now(),
                'period': f'{month}/{year}'
            },
            'executive_summary': self._generate_executive_summary(cost_data),
            'cost_breakdown': self._generate_cost_breakdown(cost_data),
            'trend_analysis': self._generate_trend_analysis(cost_data),
            'optimization_recommendations': self._generate_optimization_recommendations(cost_data),
            'budget_analysis': self._generate_budget_analysis(cost_data)
        }
        
        return report
    
    def _generate_executive_summary(self, cost_data):
        """Generate executive summary section"""
        
        total_cost = cost_data.get('total_cost', 0)
        previous_month_cost = cost_data.get('previous_month_cost', 0)
        
        month_over_month_change = ((total_cost - previous_month_cost) / previous_month_cost) * 100 if previous_month_cost > 0 else 0
        
        summary = {
            'total_monthly_cost': total_cost,
            'month_over_month_change': month_over_month_change,
            'largest_cost_component': max(cost_data.get('service_breakdown', {}).items(), key=lambda x: x[1], default=('N/A', 0)),
            'key_insights': [
                f"Total monthly cost: ${total_cost:,.2f}",
                f"Month-over-month change: {month_over_month_change:+.1f}%",
                "Primary cost drivers identified and analyzed"
            ]
        }
        
        return summary
```

================================================================================
6. BEST PRACTICES AND IMPLEMENTATION
================================================================================

6.1 Implementation Guidelines
-----------------------------
```python
class CostOptimizationBestPractices:
    @staticmethod
    def get_implementation_roadmap():
        return {
            'phase_1_foundation': {
                'duration': '2-4 weeks',
                'objectives': [
                    'Implement cost tracking and monitoring',
                    'Set up budget controls and alerts',
                    'Establish cost attribution framework',
                    'Create baseline cost metrics'
                ],
                'deliverables': [
                    'Cost monitoring dashboard',
                    'Budget tracking system',
                    'Initial cost analysis report',
                    'Cost governance policies'
                ]
            },
            'phase_2_optimization': {
                'duration': '4-6 weeks',
                'objectives': [
                    'Implement resource right-sizing',
                    'Optimize pricing models (Reserved Instances, Spot)',
                    'Set up auto-scaling policies',
                    'Implement cost allocation tags'
                ],
                'deliverables': [
                    'Auto-scaling implementation',
                    'Reserved instance strategy',
                    'Resource optimization recommendations',
                    'Cost allocation framework'
                ]
            },
            'phase_3_advanced': {
                'duration': '6-8 weeks',
                'objectives': [
                    'Implement predictive cost management',
                    'Set up cross-cloud cost optimization',
                    'Develop custom cost optimization models',
                    'Implement advanced governance'
                ],
                'deliverables': [
                    'Predictive cost models',
                    'Multi-cloud cost optimization',
                    'Advanced governance policies',
                    'Continuous optimization pipeline'
                ]
            }
        }
    
    @staticmethod
    def get_cost_optimization_checklist():
        return {
            'monitoring_and_visibility': [
                'Real-time cost monitoring enabled',
                'Cost attribution by team/project implemented',
                'Budget tracking and alerts configured',
                'Cost anomaly detection active',
                'Regular cost reviews scheduled'
            ],
            'resource_optimization': [
                'Instance right-sizing analysis performed',
                'Auto-scaling policies implemented',
                'Unused resources identified and removed',
                'Storage tiering and lifecycle policies active',
                'Network optimization implemented'
            ],
            'pricing_optimization': [
                'Reserved instance analysis completed',
                'Spot instance strategy implemented',
                'Savings plans evaluated and purchased',
                'Multi-cloud pricing comparison done',
                'Contract negotiations optimized'
            ],
            'governance_and_control': [
                'Cost governance policies defined',
                'Approval workflows for high-cost resources',
                'Resource tagging standards enforced',
                'Cost allocation models implemented',
                'Regular cost optimization reviews'
            ]
        }
    
    @staticmethod
    def get_common_cost_optimization_pitfalls():
        return {
            'monitoring_gaps': [
                'Insufficient granularity in cost tracking',
                'Missing cost attribution to teams/projects',
                'Lack of real-time cost visibility',
                'No alerting for cost anomalies'
            ],
            'resource_waste': [
                'Oversized instances for workload requirements',
                'Unused or idle resources left running',
                'Inefficient storage usage and tiers',
                'Poor auto-scaling configuration'
            ],
            'pricing_inefficiencies': [
                'Over-reliance on on-demand pricing',
                'Insufficient use of reserved instances',
                'Missing spot instance opportunities',
                'Lack of multi-cloud price comparison'
            ],
            'governance_issues': [
                'No approval process for expensive resources',
                'Inconsistent resource tagging',
                'Missing cost allocation to business units',
                'Lack of cost optimization culture'
            ]
        }

# Success metrics for cost optimization implementation
COST_OPTIMIZATION_SUCCESS_METRICS = {
    'cost_efficiency': {
        'cost_reduction': 'Target: 20-30% reduction in first year',
        'cost_per_prediction': 'Target: 50% reduction in unit costs',
        'resource_utilization': 'Target: >80% average utilization',
        'waste_elimination': 'Target: <5% idle resource costs'
    },
    'operational_efficiency': {
        'cost_visibility': 'Target: 100% cost attribution coverage',
        'budget_accuracy': 'Target: <10% variance from budget',
        'optimization_automation': 'Target: >90% automated optimization',
        'cost_governance': 'Target: 100% compliance with policies'
    },
    'business_impact': {
        'roi_on_optimization': 'Target: >300% ROI on optimization efforts',
        'cost_predictability': 'Target: <15% monthly cost variance',
        'budget_planning_accuracy': 'Target: <5% variance in annual planning',
        'team_productivity': 'Target: 25% improvement in cost per project'
    }
}

# Production deployment configuration
COST_OPTIMIZATION_DEPLOYMENT_CONFIG = {
    'basic_setup': {
        'description': 'Basic cost monitoring and optimization',
        'components': [
            'Cost tracking dashboard',
            'Basic budget alerts',
            'Resource tagging framework',
            'Monthly cost reports'
        ],
        'estimated_setup_time': '2-3 weeks',
        'expected_savings': '10-15%'
    },
    'advanced_setup': {
        'description': 'Advanced cost optimization with automation',
        'components': [
            'Real-time cost monitoring',
            'Automated right-sizing',
            'Predictive cost management',
            'Multi-cloud optimization',
            'Advanced governance'
        ],
        'estimated_setup_time': '6-8 weeks',
        'expected_savings': '25-35%'
    },
    'enterprise_setup': {
        'description': 'Enterprise-grade cost optimization platform',
        'components': [
            'Comprehensive cost analytics',
            'AI-powered optimization',
            'Advanced governance and compliance',
            'Custom optimization algorithms',
            'Integration with business systems'
        ],
        'estimated_setup_time': '12-16 weeks',
        'expected_savings': '35-50%'
    }
}
```

================================================================================
SUMMARY AND KEY TAKEAWAYS
================================================================================

Cost optimization and resource management are critical for sustainable ML operations:

**Key Cost Components:**
- **Compute Costs:** Training, inference, and development workloads
- **Storage Costs:** Data storage, model artifacts, and backup systems
- **Data Costs:** Transfer, processing, and licensing expenses
- **Operational Costs:** Monitoring, security, and personnel expenses

**Optimization Strategies:**
- **Resource Right-sizing:** Match instance types and sizes to workload requirements
- **Pricing Models:** Strategic use of reserved instances, spot instances, and savings plans
- **Auto-scaling:** Dynamic resource allocation based on demand patterns
- **Multi-cloud:** Leverage pricing differences and avoid vendor lock-in

**Cost Monitoring and Control:**
- **Real-time Visibility:** Continuous cost tracking and attribution
- **Budget Management:** Proactive budget controls and alerts
- **Anomaly Detection:** Automated identification of cost spikes
- **Governance:** Policies and approvals for resource requests

**Resource Allocation:**
- **Dynamic Allocation:** Intelligent resource assignment based on priorities
- **Resource Pooling:** Shared resources for improved utilization
- **Scheduling:** Cost-aware and deadline-aware scheduling
- **Performance vs Cost:** Balanced optimization for business requirements

**Advanced Techniques:**
- **Predictive Scaling:** Anticipate demand changes for proactive scaling
- **Cost Forecasting:** Predict future costs for better budget planning
- **Optimization Automation:** Automated recommendations and actions
- **Green Computing:** Energy-efficient resource allocation

**Best Practices:**
- Implement comprehensive cost monitoring from day one
- Establish clear cost governance and approval processes
- Regular review and optimization of resource utilization
- Balance cost optimization with performance requirements
- Automate optimization decisions where possible

**Success Factors:**
- Clear understanding of cost drivers and optimization opportunities
- Strong governance framework with appropriate controls
- Regular monitoring and optimization processes
- Cultural emphasis on cost awareness and efficiency
- Continuous improvement based on metrics and feedback

**Key Metrics:**
- Cost per prediction/model/project
- Resource utilization rates
- Budget variance and forecast accuracy
- ROI on optimization efforts
- Time to implement optimizations

Effective cost optimization enables organizations to maximize the value of their ML investments while maintaining performance and reliability requirements. The key is balancing cost efficiency with business objectives through intelligent automation and governance. 