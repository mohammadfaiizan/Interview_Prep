ML SYSTEM DESIGN PRINCIPLES
===========================

Table of Contents:
1. Introduction to ML System Design
2. System Architecture Fundamentals
3. Scalability and Performance Principles
4. Reliability and Fault Tolerance
5. Data Architecture and Management
6. Model Architecture and Serving
7. Monitoring and Observability
8. Security and Privacy Considerations
9. Cost Optimization and Resource Management
10. System Integration and APIs
11. Testing and Validation Strategies
12. Case Studies and Best Practices

================================================================================
1. INTRODUCTION TO ML SYSTEM DESIGN
================================================================================

1.1 Definition and Scope
------------------------
ML System Design encompasses the architecture, infrastructure, and operational considerations for building production-ready machine learning systems that are scalable, reliable, and maintainable.

**Key Components:**
- Data ingestion and processing pipelines
- Model training and serving infrastructure
- Monitoring and alerting systems
- Version control and deployment mechanisms
- Security and compliance frameworks

**Design Principles:**
- **Modularity:** Loosely coupled, independently deployable components
- **Scalability:** Handle increasing loads gracefully
- **Reliability:** High availability and fault tolerance
- **Maintainability:** Easy to update, debug, and extend
- **Security:** Protect data and models throughout lifecycle

1.2 ML System Lifecycle
-----------------------
**Development Phase:**
1. Problem definition and requirements gathering
2. Data collection and exploration
3. Model development and experimentation
4. Model validation and testing

**Deployment Phase:**
1. Infrastructure setup and configuration
2. Model packaging and containerization
3. Deployment automation and rollout
4. Production testing and validation

**Operations Phase:**
1. Monitoring and alerting
2. Performance optimization
3. Model updates and retraining
4. Incident response and debugging

1.3 Stakeholders and Requirements
--------------------------------
**Data Scientists:**
- Experimentation environments
- Model development tools
- Reproducible research workflows

**ML Engineers:**
- Scalable training infrastructure
- Model deployment pipelines
- Performance monitoring tools

**Software Engineers:**
- API integration capabilities
- Microservices architecture
- Development best practices

**DevOps Engineers:**
- Infrastructure automation
- Container orchestration
- CI/CD pipeline management

**Business Stakeholders:**
- Cost-effective solutions
- Reliable service delivery
- Compliance and governance

================================================================================
2. SYSTEM ARCHITECTURE FUNDAMENTALS
================================================================================

2.1 Architectural Patterns
--------------------------
**Batch Processing Architecture:**
```
Data Sources → Data Lake → Batch Processing → Model Training → Model Store
```

**Stream Processing Architecture:**
```
Event Streams → Stream Processor → Feature Store → Real-time Inference → Response
```

**Lambda Architecture:**
```
Data Sources → [Batch Layer + Speed Layer] → Serving Layer → Applications
```

**Kappa Architecture:**
```
Data Sources → Stream Processing → Serving Layer → Applications
```

2.2 Microservices vs Monolithic Design
--------------------------------------
**Microservices Architecture:**

**Advantages:**
- Independent deployment and scaling
- Technology diversity and team autonomy
- Fault isolation and resilience
- Easier testing and maintenance

**Challenges:**
- Increased complexity in service coordination
- Network latency and communication overhead
- Distributed system debugging difficulties
- Data consistency across services

**Service Decomposition Strategies:**
```
# By Business Function
- User Service
- Recommendation Service  
- Analytics Service

# By Data Domain
- Customer Data Service
- Product Data Service
- Transaction Service

# By ML Pipeline Stage
- Feature Engineering Service
- Model Training Service
- Inference Service
```

**Monolithic Architecture:**

**When to Use:**
- Simple applications with limited scope
- Small teams with shared technology stack
- Tight coupling between components is beneficial
- Rapid prototyping and development

2.3 Event-Driven Architecture
----------------------------
**Event Sourcing Pattern:**
```
Event Store → [Events] → Event Handlers → State Updates
```

**Command Query Responsibility Segregation (CQRS):**
```
Commands → Write Model → Event Store
Queries → Read Model ← Event Projections
```

**Benefits for ML Systems:**
- Auditability and reproducibility
- Temporal data access for feature engineering
- Decoupled model training and serving
- Real-time and batch processing integration

================================================================================
3. SCALABILITY AND PERFORMANCE PRINCIPLES
================================================================================

3.1 Horizontal vs Vertical Scaling
----------------------------------
**Horizontal Scaling (Scale Out):**
- Add more instances/nodes to handle load
- Better fault tolerance and cost efficiency
- Requires stateless design and load balancing
- Suitable for distributed ML workloads

**Vertical Scaling (Scale Up):**
- Increase resources (CPU, memory, GPU) per instance
- Simpler architecture and state management
- Limited by hardware constraints
- Suitable for memory-intensive ML tasks

**Scaling Strategies:**
```
# Auto Scaling Policy
if avg_cpu_utilization > 70%:
    scale_out(target_instances = current + 2)
elif avg_cpu_utilization < 30%:
    scale_in(target_instances = max(current - 1, min_instances))
```

3.2 Load Balancing Strategies
----------------------------
**Round Robin:**
```
next_server = (current_server + 1) % total_servers
```

**Weighted Round Robin:**
```
for server in servers:
    for _ in range(server.weight):
        assign_request(server)
```

**Least Connections:**
```
selected_server = min(servers, key=lambda s: s.active_connections)
```

**Consistent Hashing:**
```
def get_server(key):
    hash_value = hash(key) % RING_SIZE
    return find_next_server_on_ring(hash_value)
```

3.3 Caching Strategies
---------------------
**Cache-Aside Pattern:**
```
def get_prediction(features):
    cache_key = hash(features)
    result = cache.get(cache_key)
    if result is None:
        result = model.predict(features)
        cache.set(cache_key, result, ttl=3600)
    return result
```

**Write-Through Cache:**
```
def update_model(model_id, model_data):
    database.save(model_id, model_data)
    cache.set(model_id, model_data)
```

**Cache Invalidation Strategies:**
- Time-based expiration (TTL)
- Event-driven invalidation
- Version-based invalidation
- Probabilistic early expiration

3.4 Performance Optimization
----------------------------
**Model Serving Optimization:**
- **Model Quantization:** Reduce precision for faster inference
- **Batching:** Process multiple requests together
- **Model Compilation:** Optimize computational graphs
- **Hardware Acceleration:** GPU, TPU, specialized chips

**Database Optimization:**
- Indexing strategies for feature lookups
- Partitioning and sharding for large datasets
- Read replicas for query distribution
- Connection pooling and query optimization

**Network Optimization:**
- Content delivery networks (CDN)
- Request/response compression
- Keep-alive connections
- Protocol optimization (HTTP/2, gRPC)

================================================================================
4. RELIABILITY AND FAULT TOLERANCE
================================================================================

4.1 Failure Modes in ML Systems
-------------------------------
**Infrastructure Failures:**
- Server crashes and network partitions
- Storage failures and data corruption
- Cloud provider outages
- Resource exhaustion (OOM, disk space)

**Model Failures:**
- Model performance degradation
- Distribution shift and concept drift
- Adversarial inputs and edge cases
- Model serving errors and timeouts

**Data Failures:**
- Missing or corrupted data
- Schema changes and format errors
- Late-arriving data and ordering issues
- Data quality degradation

4.2 Redundancy and Replication
------------------------------
**Active-Active Configuration:**
```
Load Balancer → [Server 1, Server 2, Server 3] → Database Cluster
```

**Active-Passive Configuration:**
```
Load Balancer → Primary Server → Standby Server
                ↓
           Shared Storage
```

**Database Replication:**
```
Master DB → [Replica 1, Replica 2, Replica 3]
         → [Read Queries] [Write Queries to Master]
```

4.3 Circuit Breaker Pattern
---------------------------
```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
    
    def call(self, func, *args, **kwargs):
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "HALF_OPEN"
            else:
                raise CircuitBreakerOpenException()
        
        try:
            result = func(*args, **kwargs)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e
    
    def on_success(self):
        self.failure_count = 0
        self.state = "CLOSED"
    
    def on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()
        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"
```

4.4 Graceful Degradation
------------------------
**Fallback Strategies:**
```python
def get_recommendation(user_id):
    try:
        # Primary ML model
        return personalized_model.predict(user_id)
    except ModelUnavailableException:
        try:
            # Simpler backup model
            return popularity_model.predict()
        except Exception:
            # Static fallback
            return default_recommendations
```

**Quality Levels:**
- **Level 1:** Full ML-powered experience
- **Level 2:** Simplified ML with cached results
- **Level 3:** Rule-based heuristics
- **Level 4:** Static default responses

================================================================================
5. DATA ARCHITECTURE AND MANAGEMENT
================================================================================

5.1 Data Storage Patterns
-------------------------
**Data Lake Architecture:**
```
Raw Data → Data Lake (S3/HDFS) → Data Processing → Feature Store → ML Models
```

**Data Warehouse Pattern:**
```
OLTP Systems → ETL → Data Warehouse → OLAP → Analytics/ML
```

**Lakehouse Architecture:**
```
Raw Data → Delta Lake → [Analytics + ML Workloads]
```

5.2 Feature Store Design
-----------------------
**Components:**
- **Offline Store:** Historical features for training
- **Online Store:** Low-latency features for serving
- **Feature Registry:** Metadata and lineage tracking
- **Transformation Engine:** Feature computation logic

**Architecture:**
```
Data Sources → Feature Engineering → [Offline Store | Online Store]
                                   ↓              ↓
                              Training Pipeline  Serving Pipeline
```

**Feature Serving Pattern:**
```python
class FeatureStore:
    def get_offline_features(self, entity_ids, feature_names, timestamp_range):
        # Batch feature retrieval for training
        return historical_features
    
    def get_online_features(self, entity_id, feature_names):
        # Real-time feature retrieval for serving
        return current_features
    
    def push_features(self, features, timestamp):
        # Update both offline and online stores
        self.offline_store.write(features, timestamp)
        self.online_store.update(features)
```

5.3 Data Versioning and Lineage
-------------------------------
**Data Versioning Strategies:**
```
# Git-like versioning for datasets
dataset_v1.0 → feature_engineering_v2.1 → model_v3.2

# Content-based versioning
dataset_hash_abc123 → features_hash_def456 → model_hash_ghi789
```

**Lineage Tracking:**
```python
@track_lineage
def create_features(raw_data_path, transformation_config):
    # Automatically tracks inputs, outputs, and transformations
    features = transform(raw_data_path, transformation_config)
    return features
```

5.4 Stream Processing Architecture
---------------------------------
**Event Streaming Patterns:**
```
Producers → Message Broker (Kafka) → Stream Processors → Consumers
```

**Windowing Operations:**
```python
# Tumbling Window
def tumbling_window(stream, window_size):
    return stream.window(Time.seconds(window_size))

# Sliding Window  
def sliding_window(stream, window_size, slide_interval):
    return stream.window(Time.seconds(window_size), 
                        Time.seconds(slide_interval))

# Session Window
def session_window(stream, gap_duration):
    return stream.window(ProcessingTimeSessionWindows.withGap(gap_duration))
```

================================================================================
6. MODEL ARCHITECTURE AND SERVING
================================================================================

6.1 Model Serving Patterns
--------------------------
**Synchronous Serving:**
```
Client Request → Load Balancer → Model Server → Response
```

**Asynchronous Serving:**
```
Client Request → Message Queue → Model Worker → Result Store → Client Poll
```

**Batch Inference:**
```
Batch Job → Data Processing → Model Inference → Result Storage
```

6.2 Model Packaging and Containerization
----------------------------------------
**Docker Container Structure:**
```dockerfile
FROM python:3.8-slim

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy model artifacts
COPY model/ /opt/model/
COPY src/ /opt/src/

# Set up serving
EXPOSE 8080
CMD ["python", "/opt/src/serve.py"]
```

**Model Artifact Structure:**
```
model_package/
├── model.pkl              # Serialized model
├── metadata.json          # Model metadata
├── schema.json           # Input/output schema
├── requirements.txt      # Dependencies
├── preprocessing.py      # Feature preprocessing
└── postprocessing.py     # Output postprocessing
```

6.3 Multi-Model Serving
-----------------------
**Model Routing:**
```python
class ModelRouter:
    def __init__(self):
        self.models = {}
        self.routing_rules = {}
    
    def route_request(self, request):
        model_id = self.determine_model(request)
        model = self.models[model_id]
        return model.predict(request.features)
    
    def determine_model(self, request):
        # A/B testing, canary deployment, user segmentation
        if request.user_segment == "premium":
            return "advanced_model_v2"
        elif random.random() < 0.1:  # 10% traffic to new model
            return "experimental_model_v3"
        else:
            return "stable_model_v1"
```

6.4 Model Caching and Optimization
----------------------------------
**Prediction Caching:**
```python
class PredictionCache:
    def __init__(self, cache_backend, ttl=3600):
        self.cache = cache_backend
        self.ttl = ttl
    
    def get_or_predict(self, model, features):
        cache_key = self.compute_key(features)
        cached_result = self.cache.get(cache_key)
        
        if cached_result is not None:
            return cached_result
        
        prediction = model.predict(features)
        self.cache.set(cache_key, prediction, self.ttl)
        return prediction
    
    def compute_key(self, features):
        # Create deterministic key from features
        return hashlib.md5(str(sorted(features.items())).encode()).hexdigest()
```

**Model Warm-up:**
```python
def warm_up_model(model, sample_requests):
    """Pre-load model and warm up caches"""
    for request in sample_requests:
        _ = model.predict(request)
    
    # JIT compilation warm-up for TensorFlow/PyTorch
    if hasattr(model, 'trace'):
        model.trace(sample_requests[0])
```

================================================================================
7. MONITORING AND OBSERVABILITY
================================================================================

7.1 Monitoring Stack Architecture
---------------------------------
**Three Pillars of Observability:**
- **Metrics:** Quantitative measurements over time
- **Logs:** Event records with context
- **Traces:** Request flow through distributed systems

**Monitoring Architecture:**
```
Applications → [Metrics Collector, Log Aggregator, Trace Collector] 
            → Time Series DB → Visualization & Alerting
```

7.2 ML-Specific Metrics
-----------------------
**Model Performance Metrics:**
```python
class ModelMetrics:
    def __init__(self):
        self.accuracy_gauge = Gauge('model_accuracy', 'Model accuracy')
        self.latency_histogram = Histogram('prediction_latency', 'Prediction latency')
        self.throughput_counter = Counter('predictions_total', 'Total predictions')
        self.error_counter = Counter('prediction_errors', 'Prediction errors')
    
    def record_prediction(self, start_time, prediction, ground_truth=None):
        latency = time.time() - start_time
        self.latency_histogram.observe(latency)
        self.throughput_counter.inc()
        
        if ground_truth is not None:
            accuracy = self.compute_accuracy(prediction, ground_truth)
            self.accuracy_gauge.set(accuracy)
```

**Data Quality Metrics:**
```python
def monitor_data_quality(incoming_data, reference_data):
    metrics = {}
    
    # Distribution drift detection
    metrics['ks_statistic'] = ks_2samp(incoming_data, reference_data).statistic
    
    # Missing value rate
    metrics['missing_rate'] = incoming_data.isnull().sum() / len(incoming_data)
    
    # Feature correlation changes
    metrics['correlation_drift'] = compute_correlation_drift(
        incoming_data, reference_data
    )
    
    return metrics
```

7.3 Alerting and Anomaly Detection
----------------------------------
**Threshold-based Alerting:**
```python
class AlertManager:
    def __init__(self):
        self.thresholds = {
            'error_rate': 0.05,      # 5% error rate
            'latency_p99': 1000,     # 1 second 99th percentile
            'throughput': 100,       # minimum requests per minute
        }
    
    def check_alerts(self, metrics):
        alerts = []
        
        if metrics['error_rate'] > self.thresholds['error_rate']:
            alerts.append(Alert('HIGH_ERROR_RATE', metrics['error_rate']))
        
        if metrics['latency_p99'] > self.thresholds['latency_p99']:
            alerts.append(Alert('HIGH_LATENCY', metrics['latency_p99']))
        
        return alerts
```

**Statistical Anomaly Detection:**
```python
def detect_anomalies(time_series, window_size=24, threshold=3):
    """Detect anomalies using statistical methods"""
    
    # Rolling statistics
    rolling_mean = time_series.rolling(window_size).mean()
    rolling_std = time_series.rolling(window_size).std()
    
    # Z-score based detection
    z_scores = (time_series - rolling_mean) / rolling_std
    anomalies = abs(z_scores) > threshold
    
    return anomalies
```

7.4 Distributed Tracing
-----------------------
**Trace Context Propagation:**
```python
@trace_method
def predict(self, features):
    with tracer.start_span("feature_preprocessing") as span:
        processed_features = self.preprocess(features)
        span.set_attribute("feature_count", len(processed_features))
    
    with tracer.start_span("model_inference") as span:
        prediction = self.model.predict(processed_features)
        span.set_attribute("model_version", self.model.version)
    
    return prediction
```

================================================================================
8. SECURITY AND PRIVACY CONSIDERATIONS
================================================================================

8.1 Authentication and Authorization
------------------------------------
**OAuth 2.0 / JWT Implementation:**
```python
def verify_token(token):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])
        user_id = payload['user_id']
        permissions = payload['permissions']
        return user_id, permissions
    except jwt.ExpiredSignatureError:
        raise AuthenticationError("Token expired")
    except jwt.InvalidTokenError:
        raise AuthenticationError("Invalid token")

@require_auth(['model:predict'])
def predict_endpoint(request):
    features = request.json['features']
    prediction = model.predict(features)
    return {'prediction': prediction}
```

**Role-Based Access Control (RBAC):**
```python
class Permission:
    MODEL_READ = "model:read"
    MODEL_WRITE = "model:write"
    DATA_READ = "data:read"
    DATA_WRITE = "data:write"

def check_permission(user_role, required_permission):
    role_permissions = {
        'data_scientist': [Permission.MODEL_READ, Permission.DATA_READ],
        'ml_engineer': [Permission.MODEL_READ, Permission.MODEL_WRITE],
        'admin': [Permission.MODEL_READ, Permission.MODEL_WRITE, 
                 Permission.DATA_READ, Permission.DATA_WRITE]
    }
    return required_permission in role_permissions.get(user_role, [])
```

8.2 Data Encryption and Protection
----------------------------------
**Encryption at Rest:**
```python
from cryptography.fernet import Fernet

class EncryptedStorage:
    def __init__(self, encryption_key):
        self.cipher = Fernet(encryption_key)
    
    def store_model(self, model_data, path):
        encrypted_data = self.cipher.encrypt(model_data)
        with open(path, 'wb') as f:
            f.write(encrypted_data)
    
    def load_model(self, path):
        with open(path, 'rb') as f:
            encrypted_data = f.read()
        return self.cipher.decrypt(encrypted_data)
```

**Encryption in Transit:**
```python
# TLS configuration for API endpoints
app.config.update(
    SSL_CERTIFICATE='/path/to/cert.pem',
    SSL_PRIVATE_KEY='/path/to/key.pem',
    SSL_PROTOCOL='TLSv1.2'
)

# Certificate pinning for client connections
requests.get('https://api.example.com', verify='/path/to/ca-bundle.crt')
```

8.3 Privacy-Preserving ML
-------------------------
**Differential Privacy:**
```python
def add_noise(value, epsilon, sensitivity):
    """Add Laplace noise for differential privacy"""
    scale = sensitivity / epsilon
    noise = numpy.random.laplace(0, scale)
    return value + noise

def private_gradient_descent(gradients, epsilon, clip_norm):
    """Differentially private gradient descent"""
    # Clip gradients
    clipped_gradients = [torch.clamp(g, -clip_norm, clip_norm) for g in gradients]
    
    # Add noise
    noisy_gradients = []
    for grad in clipped_gradients:
        noise = torch.normal(0, (clip_norm * 2 / epsilon) ** 2, grad.shape)
        noisy_gradients.append(grad + noise)
    
    return noisy_gradients
```

**Federated Learning:**
```python
class FederatedLearningServer:
    def __init__(self, global_model):
        self.global_model = global_model
        self.client_updates = []
    
    def aggregate_updates(self, client_updates):
        """Federated averaging"""
        averaged_weights = {}
        
        for layer_name in self.global_model.state_dict():
            layer_updates = [update[layer_name] for update in client_updates]
            averaged_weights[layer_name] = torch.mean(torch.stack(layer_updates), dim=0)
        
        self.global_model.load_state_dict(averaged_weights)
        return self.global_model
```

8.4 Model Security
------------------
**Adversarial Attack Detection:**
```python
def detect_adversarial_input(input_data, model, threshold=0.1):
    """Detect potential adversarial examples"""
    
    # Input validation
    if not validate_input_format(input_data):
        return True, "Invalid input format"
    
    # Statistical analysis
    if detect_statistical_anomaly(input_data):
        return True, "Statistical anomaly detected"
    
    # Model confidence check
    prediction_confidence = model.predict_proba(input_data).max()
    if prediction_confidence < threshold:
        return True, "Low prediction confidence"
    
    return False, "Input appears benign"
```

================================================================================
9. COST OPTIMIZATION AND RESOURCE MANAGEMENT
================================================================================

9.1 Resource Allocation Strategies
----------------------------------
**Dynamic Resource Allocation:**
```python
class ResourceManager:
    def __init__(self):
        self.resource_pools = {
            'training': {'cpu': 0, 'memory': 0, 'gpu': 0},
            'inference': {'cpu': 0, 'memory': 0, 'gpu': 0},
            'data_processing': {'cpu': 0, 'memory': 0, 'gpu': 0}
        }
    
    def allocate_resources(self, workload_type, requirements):
        pool = self.resource_pools[workload_type]
        
        if self.has_available_resources(pool, requirements):
            self.reserve_resources(pool, requirements)
            return True
        else:
            return self.scale_up_or_queue(workload_type, requirements)
    
    def optimize_allocation(self):
        """Optimize resource allocation based on usage patterns"""
        for pool_name, pool in self.resource_pools.items():
            utilization = self.calculate_utilization(pool)
            
            if utilization < 0.3:  # Under-utilized
                self.scale_down(pool_name)
            elif utilization > 0.8:  # Over-utilized
                self.scale_up(pool_name)
```

9.2 Cost Monitoring and Optimization
------------------------------------
**Cost Tracking:**
```python
class CostTracker:
    def __init__(self):
        self.cost_metrics = {}
    
    def track_compute_cost(self, instance_type, duration, rate):
        cost = duration * rate
        self.cost_metrics['compute'] = self.cost_metrics.get('compute', 0) + cost
        
        # Alert on cost thresholds
        if cost > self.get_budget_threshold('compute'):
            self.send_cost_alert('compute', cost)
    
    def track_storage_cost(self, storage_gb, rate_per_gb):
        cost = storage_gb * rate_per_gb
        self.cost_metrics['storage'] = self.cost_metrics.get('storage', 0) + cost
    
    def get_cost_breakdown(self):
        return {
            'compute': self.cost_metrics.get('compute', 0),
            'storage': self.cost_metrics.get('storage', 0),
            'data_transfer': self.cost_metrics.get('data_transfer', 0),
            'total': sum(self.cost_metrics.values())
        }
```

**Spot Instance Management:**
```python
def manage_spot_instances(training_job):
    """Handle spot instance interruptions gracefully"""
    
    try:
        # Start training on spot instance
        result = run_training_job(training_job, instance_type='spot')
        return result
    
    except SpotInstanceInterruption:
        # Save checkpoint and migrate to on-demand
        checkpoint_path = save_training_checkpoint(training_job)
        
        # Resume on on-demand instance
        training_job.resume_from_checkpoint(checkpoint_path)
        return run_training_job(training_job, instance_type='on-demand')
```

9.3 Storage Optimization
------------------------
**Data Lifecycle Management:**
```python
class DataLifecycleManager:
    def __init__(self):
        self.storage_tiers = {
            'hot': {'cost_per_gb': 0.023, 'access_time': 'immediate'},
            'warm': {'cost_per_gb': 0.0125, 'access_time': 'minutes'},
            'cold': {'cost_per_gb': 0.004, 'access_time': 'hours'},
            'archive': {'cost_per_gb': 0.001, 'access_time': 'hours'}
        }
    
    def optimize_storage(self, dataset_metadata):
        """Move data to appropriate storage tier based on access patterns"""
        
        for dataset in dataset_metadata:
            access_frequency = dataset['access_frequency']
            size_gb = dataset['size_gb']
            
            if access_frequency > 10:  # Daily access
                target_tier = 'hot'
            elif access_frequency > 1:  # Weekly access
                target_tier = 'warm'
            elif access_frequency > 0.1:  # Monthly access
                target_tier = 'cold'
            else:  # Rarely accessed
                target_tier = 'archive'
            
            if dataset['current_tier'] != target_tier:
                self.migrate_data(dataset['path'], target_tier)
```

================================================================================
10. SYSTEM INTEGRATION AND APIS
================================================================================

10.1 API Design Principles
--------------------------
**RESTful API Design:**
```python
# Resource-based URLs
GET /api/v1/models/{model_id}           # Get model metadata
POST /api/v1/models/{model_id}/predict  # Make prediction
PUT /api/v1/models/{model_id}           # Update model
DELETE /api/v1/models/{model_id}        # Delete model

# Consistent response format
{
    "status": "success|error",
    "data": {...},
    "message": "Human readable message",
    "timestamp": "2023-12-01T10:00:00Z",
    "request_id": "uuid"
}
```

**GraphQL API:**
```graphql
type Model {
    id: ID!
    name: String!
    version: String!
    accuracy: Float
    createdAt: DateTime!
}

type Query {
    model(id: ID!): Model
    models(limit: Int, offset: Int): [Model]
}

type Mutation {
    predict(modelId: ID!, features: JSON!): PredictionResult
    deployModel(modelId: ID!, environment: String!): DeploymentResult
}
```

10.2 Event-Driven Integration
-----------------------------
**Event Schema Design:**
```json
{
    "event_type": "model.training.completed",
    "event_id": "uuid",
    "timestamp": "2023-12-01T10:00:00Z",
    "source": "training-service",
    "data": {
        "model_id": "model-123",
        "version": "v1.2.3",
        "accuracy": 0.95,
        "training_duration": 3600
    }
}
```

**Event Handler Pattern:**
```python
class EventHandler:
    def __init__(self):
        self.handlers = {}
    
    def register_handler(self, event_type, handler_func):
        if event_type not in self.handlers:
            self.handlers[event_type] = []
        self.handlers[event_type].append(handler_func)
    
    def handle_event(self, event):
        event_type = event['event_type']
        if event_type in self.handlers:
            for handler in self.handlers[event_type]:
                try:
                    handler(event)
                except Exception as e:
                    logging.error(f"Handler failed for {event_type}: {e}")

# Usage
event_handler = EventHandler()
event_handler.register_handler('model.training.completed', deploy_model)
event_handler.register_handler('model.training.completed', update_model_registry)
```

10.3 Workflow Orchestration
---------------------------
**DAG-based Workflow:**
```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

def extract_data():
    # Data extraction logic
    pass

def train_model():
    # Model training logic
    pass

def validate_model():
    # Model validation logic
    pass

def deploy_model():
    # Model deployment logic
    pass

dag = DAG(
    'ml_pipeline',
    default_args={
        'owner': 'ml-team',
        'depends_on_past': False,
        'start_date': datetime(2023, 1, 1),
        'email_on_failure': True,
        'retries': 2,
        'retry_delay': timedelta(minutes=5)
    },
    schedule_interval='@daily'
)

extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag
)

train_task = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    dag=dag
)

validate_task = PythonOperator(
    task_id='validate_model',
    python_callable=validate_model,
    dag=dag
)

deploy_task = PythonOperator(
    task_id='deploy_model',
    python_callable=deploy_model,
    dag=dag
)

extract_task >> train_task >> validate_task >> deploy_task
```

================================================================================
11. TESTING AND VALIDATION STRATEGIES
================================================================================

11.1 Testing Pyramid for ML Systems
-----------------------------------
**Unit Tests:**
```python
import unittest
import numpy as np

class TestFeatureEngineering(unittest.TestCase):
    def setUp(self):
        self.feature_processor = FeatureProcessor()
    
    def test_normalize_features(self):
        input_data = np.array([1, 2, 3, 4, 5])
        expected_output = np.array([0, 0.25, 0.5, 0.75, 1.0])
        actual_output = self.feature_processor.normalize(input_data)
        np.testing.assert_array_almost_equal(actual_output, expected_output)
    
    def test_handle_missing_values(self):
        input_data = np.array([1, np.nan, 3, np.nan, 5])
        result = self.feature_processor.handle_missing(input_data)
        self.assertFalse(np.isnan(result).any())
```

**Integration Tests:**
```python
class TestMLPipeline(unittest.TestCase):
    def test_end_to_end_pipeline(self):
        # Test complete pipeline from raw data to prediction
        raw_data = load_test_data()
        
        # Data processing
        processed_data = data_processor.process(raw_data)
        
        # Feature engineering
        features = feature_engineer.transform(processed_data)
        
        # Model prediction
        prediction = model.predict(features)
        
        # Validation
        self.assertIsNotNone(prediction)
        self.assertTrue(0 <= prediction <= 1)  # For probability predictions
```

**Property-Based Testing:**
```python
from hypothesis import given, strategies as st

class TestModelRobustness(unittest.TestCase):
    @given(st.lists(st.floats(min_value=0, max_value=100), min_size=5, max_size=10))
    def test_model_handles_valid_inputs(self, input_features):
        """Test that model handles any valid input without crashing"""
        try:
            prediction = self.model.predict(input_features)
            self.assertIsInstance(prediction, (int, float))
        except Exception as e:
            self.fail(f"Model failed on valid input: {e}")
    
    @given(st.lists(st.floats(allow_nan=True), min_size=1, max_size=10))
    def test_model_handles_invalid_inputs(self, input_features):
        """Test that model gracefully handles invalid inputs"""
        if any(math.isnan(x) for x in input_features):
            with self.assertRaises((ValueError, TypeError)):
                self.model.predict(input_features)
```

11.2 Model Validation Strategies
-------------------------------
**Cross-Validation:**
```python
def validate_model_performance(model, X, y, cv_folds=5):
    """Perform k-fold cross-validation"""
    kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
    scores = []
    
    for train_idx, val_idx in kfold.split(X):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # Train model
        model.fit(X_train, y_train)
        
        # Validate
        y_pred = model.predict(X_val)
        score = accuracy_score(y_val, y_pred)
        scores.append(score)
    
    return {
        'mean_score': np.mean(scores),
        'std_score': np.std(scores),
        'scores': scores
    }
```

**Shadow Testing:**
```python
class ShadowTester:
    def __init__(self, production_model, candidate_model):
        self.production_model = production_model
        self.candidate_model = candidate_model
        self.comparison_results = []
    
    def predict(self, features):
        # Get production prediction
        prod_prediction = self.production_model.predict(features)
        
        # Get candidate prediction (shadow)
        try:
            candidate_prediction = self.candidate_model.predict(features)
            
            # Log comparison for analysis
            self.comparison_results.append({
                'features': features,
                'production_pred': prod_prediction,
                'candidate_pred': candidate_prediction,
                'timestamp': time.time()
            })
        except Exception as e:
            logging.error(f"Shadow model failed: {e}")
        
        # Always return production prediction
        return prod_prediction
```

11.3 A/B Testing Framework
--------------------------
```python
class ABTestingFramework:
    def __init__(self):
        self.experiments = {}
        self.user_assignments = {}
    
    def create_experiment(self, experiment_id, variants, traffic_split):
        self.experiments[experiment_id] = {
            'variants': variants,
            'traffic_split': traffic_split,
            'results': {variant: [] for variant in variants}
        }
    
    def assign_variant(self, experiment_id, user_id):
        if user_id in self.user_assignments:
            return self.user_assignments[user_id]
        
        experiment = self.experiments[experiment_id]
        random_value = hash(f"{experiment_id}_{user_id}") % 100
        
        cumulative_split = 0
        for variant, split in experiment['traffic_split'].items():
            cumulative_split += split
            if random_value < cumulative_split:
                self.user_assignments[user_id] = variant
                return variant
        
        return list(experiment['variants'])[0]  # Default variant
    
    def record_outcome(self, experiment_id, user_id, outcome):
        variant = self.user_assignments.get(user_id)
        if variant:
            self.experiments[experiment_id]['results'][variant].append(outcome)
```

================================================================================
12. CASE STUDIES AND BEST PRACTICES
================================================================================

12.1 Netflix Recommendation System
----------------------------------
**Architecture Overview:**
- Microservices architecture with 500+ services
- Real-time and batch processing pipelines
- A/B testing platform for algorithm evaluation
- Multi-armed bandit for exploration/exploitation

**Key Design Decisions:**
```
Data Pipeline: Kafka → Spark → Cassandra → Recommendation API
Model Serving: Multiple models for different contexts (homepage, search, etc.)
Caching Strategy: Multi-layer caching (CDN, application, database)
```

**Lessons Learned:**
- Importance of feature stores for consistency
- Need for robust experimentation frameworks
- Critical role of monitoring and alerting
- Value of gradual model rollouts

12.2 Uber's Michelangelo Platform
---------------------------------
**Platform Components:**
- Unified ML workflow management
- Feature store with online/offline serving
- Model training and hyperparameter tuning
- Model deployment and monitoring

**Architecture Pattern:**
```
Data Sources → Feature Store → Model Training → Model Registry → Serving Infrastructure
```

**Best Practices:**
- Standardized ML workflows across teams
- Self-service capabilities for data scientists
- Automated model validation and deployment
- Comprehensive monitoring and alerting

12.3 Google's TFX (TensorFlow Extended)
--------------------------------------
**Pipeline Components:**
```
ExampleGen → StatisticsGen → SchemaGen → ExampleValidator → Transform
         → Trainer → Evaluator → Pusher → BulkInferrer
```

**Key Principles:**
- Configuration-driven pipelines
- Metadata tracking and lineage
- Scalable and portable across environments
- Integration with ML metadata store

12.4 Best Practices Summary
---------------------------
**Design Principles:**
1. **Start Simple:** Begin with minimal viable system, iterate
2. **Design for Scale:** Plan for 10x growth from day one
3. **Automate Everything:** Reduce manual operations and errors
4. **Monitor Constantly:** Comprehensive observability stack
5. **Test Thoroughly:** Multiple layers of testing and validation

**Operational Excellence:**
1. **Documentation:** Keep architecture and processes well-documented
2. **Security:** Implement security by design, not as an afterthought
3. **Cost Management:** Monitor and optimize costs continuously
4. **Team Structure:** Align team responsibilities with system architecture
5. **Continuous Learning:** Regular post-mortems and knowledge sharing

**Technical Debt Management:**
1. **Regular Refactoring:** Allocate time for technical debt reduction
2. **Code Quality:** Enforce coding standards and review processes
3. **Dependency Management:** Keep dependencies up-to-date and secure
4. **Performance Optimization:** Regular performance reviews and optimization
5. **Legacy System Migration:** Plan and execute gradual migration strategies

================================================================================
SUMMARY AND KEY TAKEAWAYS
================================================================================

ML System Design requires careful consideration of multiple interconnected aspects to build production-ready, scalable, and reliable systems. Key principles include:

**Architectural Foundations:**
- Microservices vs monolithic trade-offs
- Event-driven architecture for decoupling
- Scalability through horizontal scaling and load balancing
- Reliability through redundancy and fault tolerance

**Data and Model Management:**
- Feature stores for consistent data serving
- Model versioning and artifact management
- Multi-model serving and routing strategies
- Comprehensive monitoring and observability

**Operational Excellence:**
- Security and privacy by design
- Cost optimization and resource management
- Robust testing and validation strategies
- Effective system integration and APIs

**Best Practices:**
- Start simple and iterate based on requirements
- Design for scale and reliability from the beginning
- Implement comprehensive monitoring and alerting
- Maintain focus on cost optimization and security

Success in ML system design comes from balancing technical requirements with business objectives, while maintaining flexibility for future growth and adaptation. 