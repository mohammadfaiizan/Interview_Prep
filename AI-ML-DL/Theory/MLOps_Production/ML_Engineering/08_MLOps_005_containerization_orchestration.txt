CONTAINERIZATION AND ORCHESTRATION FOR ML
=========================================

Table of Contents:
1. Containerization Fundamentals for ML
2. Docker for ML Workloads
3. Kubernetes for ML Orchestration
4. Microservices Architecture for ML Systems
5. Resource Management and Autoscaling
6. Service Mesh and Communication
7. Monitoring and Observability
8. Production Deployment Patterns

================================================================================
1. CONTAINERIZATION FUNDAMENTALS FOR ML
================================================================================

1.1 Why Containerization for ML?
--------------------------------
**Traditional Deployment Challenges:**
- Environment inconsistencies ("works on my machine")
- Complex dependency management
- Resource isolation issues
- Scaling difficulties
- Deployment complexity

**Container Benefits for ML:**
- Reproducible environments
- Dependency isolation
- Easy scaling and deployment
- Version control for entire stack
- Platform independence

1.2 ML-Specific Container Considerations
---------------------------------------
**Large Model Files:**
```dockerfile
# Multi-stage build to reduce image size
FROM python:3.8-slim as builder
COPY requirements.txt .
RUN pip install --user -r requirements.txt

FROM python:3.8-slim
# Copy only installed packages
COPY --from=builder /root/.local /root/.local
# Copy application code
COPY src/ /app/src/
# Model files handled separately (volume mounts or init containers)
ENV PATH=/root/.local/bin:$PATH
WORKDIR /app
CMD ["python", "src/serve.py"]
```

**GPU Support:**
```dockerfile
FROM nvidia/cuda:11.2-runtime-ubuntu20.04

# Install Python and ML dependencies
RUN apt-get update && apt-get install -y python3 python3-pip
COPY requirements-gpu.txt .
RUN pip3 install -r requirements-gpu.txt

# Configure for GPU usage
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility
```

**Resource Optimization:**
```dockerfile
# Use Alpine Linux for smaller images
FROM python:3.8-alpine

# Install only necessary packages
RUN apk add --no-cache gcc musl-dev linux-headers

# Use virtual environments
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Install dependencies with no cache
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Remove build dependencies
RUN apk del gcc musl-dev linux-headers
```

================================================================================
2. DOCKER FOR ML WORKLOADS
================================================================================

2.1 ML Application Dockerization
--------------------------------
**Training Container:**
```dockerfile
FROM tensorflow/tensorflow:2.8.0-gpu

# Set working directory
WORKDIR /app

# Install additional dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy training code
COPY src/ ./src/
COPY config/ ./config/

# Create directories for data and models
RUN mkdir -p /data /models /logs

# Set environment variables
ENV PYTHONPATH=/app/src
ENV TF_CPP_MIN_LOG_LEVEL=2

# Default command
CMD ["python", "src/train.py", "--config", "config/train_config.yaml"]
```

**Serving Container:**
```dockerfile
FROM python:3.8-slim

# Install serving dependencies
COPY requirements-serve.txt .
RUN pip install --no-cache-dir -r requirements-serve.txt

# Copy serving code
COPY src/serve.py /app/
COPY src/utils/ /app/utils/

# Create model directory
RUN mkdir -p /models

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8080/health || exit 1

# Expose port
EXPOSE 8080

# Run server
CMD ["python", "/app/serve.py"]
```

2.2 Container Composition with Docker Compose
---------------------------------------------
```yaml
# docker-compose.yml for ML pipeline
version: '3.8'

services:
  data-processor:
    build: ./data-processing
    volumes:
      - ./data:/data
      - ./processed:/processed
    environment:
      - DATA_SOURCE=/data/raw
      - OUTPUT_PATH=/processed
    depends_on:
      - database

  model-trainer:
    build: ./training
    volumes:
      - ./processed:/data
      - ./models:/models
      - ./logs:/logs
    environment:
      - TRAINING_DATA=/data
      - MODEL_OUTPUT=/models
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      - data-processor
      - mlflow
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  model-server:
    build: ./serving
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models:ro
    environment:
      - MODEL_PATH=/models/latest
      - LOG_LEVEL=INFO
    depends_on:
      - model-trainer
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 2G
          cpus: '1.0'

  mlflow:
    image: mlflow/mlflow:1.26.1
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlruns
    command: mlflow server --host 0.0.0.0 --default-artifact-root /mlruns

  database:
    image: postgres:13
    environment:
      POSTGRES_DB: mldb
      POSTGRES_USER: mluser
      POSTGRES_PASSWORD: mlpass
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

2.3 Image Optimization Strategies
---------------------------------
```dockerfile
# Multi-stage build for production optimization
FROM python:3.8 as base
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

# Development stage
FROM base as development
COPY requirements-dev.txt .
RUN pip install -r requirements-dev.txt
COPY . .
CMD ["python", "-m", "pytest"]

# Production stage
FROM base as production
COPY src/ ./src/
# Only copy what's needed for production
RUN adduser --disabled-password --gecos '' appuser
USER appuser
CMD ["python", "src/serve.py"]

# Build with: docker build --target production -t ml-app:prod .
```

**Image Layer Optimization:**
```dockerfile
# Bad: Creates multiple layers
RUN apt-get update
RUN apt-get install -y curl
RUN apt-get install -y wget
RUN rm -rf /var/lib/apt/lists/*

# Good: Single layer with cleanup
RUN apt-get update && \
    apt-get install -y curl wget && \
    rm -rf /var/lib/apt/lists/*

# Use .dockerignore to exclude unnecessary files
# .dockerignore
*.pyc
__pycache__
.git
.gitignore
README.md
Dockerfile
.dockerignore
tests/
.pytest_cache
```

================================================================================
3. KUBERNETES FOR ML ORCHESTRATION
================================================================================

3.1 Kubernetes ML Deployment Patterns
-------------------------------------
**Model Serving Deployment:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-server
  labels:
    app: ml-model
    component: server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-model
      component: server
  template:
    metadata:
      labels:
        app: ml-model
        component: server
    spec:
      containers:
      - name: model-server
        image: ml-model-server:v1.0.0
        ports:
        - containerPort: 8080
          name: http
        env:
        - name: MODEL_PATH
          value: "/models"
        - name: WORKERS
          value: "4"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        volumeMounts:
        - name: model-storage
          mountPath: /models
          readOnly: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-pvc
```

**Training Job:**
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: model-training-job
spec:
  template:
    spec:
      containers:
      - name: trainer
        image: ml-trainer:v1.0.0
        env:
        - name: TRAINING_DATA_PATH
          value: "/data/train"
        - name: MODEL_OUTPUT_PATH
          value: "/models/output"
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow-service:5000"
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "8Gi"
            cpu: "4000m"
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
            cpu: "8000m"
        volumeMounts:
        - name: training-data
          mountPath: /data
        - name: model-output
          mountPath: /models
      restartPolicy: Never
      volumes:
      - name: training-data
        persistentVolumeClaim:
          claimName: training-data-pvc
      - name: model-output
        persistentVolumeClaim:
          claimName: model-output-pvc
  backoffLimit: 2
```

3.2 Custom Resource Definitions (CRDs)
--------------------------------------
```yaml
# MLModel CRD definition
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: mlmodels.ml.company.com
spec:
  group: ml.company.com
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              modelName:
                type: string
              version:
                type: string
              framework:
                type: string
                enum: ["tensorflow", "pytorch", "sklearn"]
              resourceRequirements:
                type: object
                properties:
                  cpu:
                    type: string
                  memory:
                    type: string
                  gpu:
                    type: integer
              replicas:
                type: integer
                minimum: 1
                maximum: 10
          status:
            type: object
            properties:
              phase:
                type: string
                enum: ["Pending", "Running", "Failed", "Succeeded"]
              deploymentStatus:
                type: string
  scope: Namespaced
  names:
    plural: mlmodels
    singular: mlmodel
    kind: MLModel
```

**MLModel Resource Usage:**
```yaml
apiVersion: ml.company.com/v1
kind: MLModel
metadata:
  name: recommendation-model
  namespace: production
spec:
  modelName: "recommendation-engine"
  version: "v2.1.0"
  framework: "tensorflow"
  resourceRequirements:
    cpu: "1000m"
    memory: "2Gi"
    gpu: 0
  replicas: 5
```

3.3 Operators for ML Workloads
------------------------------
```python
# Custom controller for MLModel CRD
import kopf
import kubernetes
from kubernetes import client, config

@kopf.on.create('ml.company.com', 'v1', 'mlmodels')
def create_mlmodel(spec, name, namespace, **kwargs):
    """Handle MLModel creation"""
    
    # Create deployment
    deployment = create_model_deployment(spec, name, namespace)
    api = client.AppsV1Api()
    api.create_namespaced_deployment(namespace=namespace, body=deployment)
    
    # Create service
    service = create_model_service(spec, name, namespace)
    api = client.CoreV1Api()
    api.create_namespaced_service(namespace=namespace, body=service)
    
    return {'message': f'MLModel {name} created successfully'}

def create_model_deployment(spec, name, namespace):
    """Create Kubernetes deployment for ML model"""
    
    container = client.V1Container(
        name=f"{name}-container",
        image=f"ml-models/{spec['modelName']}:{spec['version']}",
        ports=[client.V1ContainerPort(container_port=8080)],
        resources=client.V1ResourceRequirements(
            requests={
                'cpu': spec['resourceRequirements']['cpu'],
                'memory': spec['resourceRequirements']['memory']
            },
            limits={
                'cpu': spec['resourceRequirements']['cpu'],
                'memory': spec['resourceRequirements']['memory']
            }
        )
    )
    
    if spec['resourceRequirements'].get('gpu', 0) > 0:
        container.resources.requests['nvidia.com/gpu'] = spec['resourceRequirements']['gpu']
        container.resources.limits['nvidia.com/gpu'] = spec['resourceRequirements']['gpu']
    
    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={'app': name}),
        spec=client.V1PodSpec(containers=[container])
    )
    
    spec_obj = client.V1DeploymentSpec(
        replicas=spec.get('replicas', 1),
        selector=client.V1LabelSelector(match_labels={'app': name}),
        template=template
    )
    
    deployment = client.V1Deployment(
        api_version="apps/v1",
        kind="Deployment",
        metadata=client.V1ObjectMeta(name=name, namespace=namespace),
        spec=spec_obj
    )
    
    return deployment
```

================================================================================
4. MICROSERVICES ARCHITECTURE FOR ML SYSTEMS
================================================================================

4.1 ML Microservices Design Patterns
------------------------------------
**Service Decomposition:**
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  Data Ingestion │    │ Feature Service │    │ Model Service   │
│     Service     │───▶│                 │───▶│                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                              │                       │
                              ▼                       ▼
                    ┌─────────────────┐    ┌─────────────────┐
                    │ Feature Store   │    │ Prediction API  │
                    │                 │    │                 │
                    └─────────────────┘    └─────────────────┘
```

**API Gateway Pattern:**
```yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: ml-api-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - ml-api.company.com
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ml-api-routes
spec:
  hosts:
  - ml-api.company.com
  gateways:
  - ml-api-gateway
  http:
  - match:
    - uri:
        prefix: /predict
    route:
    - destination:
        host: model-service
        port:
          number: 8080
  - match:
    - uri:
        prefix: /features
    route:
    - destination:
        host: feature-service
        port:
          number: 8080
```

4.2 Inter-Service Communication
------------------------------
**Synchronous Communication (gRPC):**
```python
# prediction_service.proto
syntax = "proto3";

service PredictionService {
    rpc Predict(PredictRequest) returns (PredictResponse);
    rpc GetModelInfo(ModelInfoRequest) returns (ModelInfoResponse);
}

message PredictRequest {
    string model_id = 1;
    repeated Feature features = 2;
}

message Feature {
    string name = 1;
    oneof value {
        double numeric_value = 2;
        string categorical_value = 3;
    }
}

message PredictResponse {
    double prediction = 1;
    double confidence = 2;
    string model_version = 3;
}
```

**Python gRPC Server:**
```python
import grpc
from concurrent import futures
import prediction_pb2_grpc
import prediction_pb2

class PredictionServicer(prediction_pb2_grpc.PredictionServiceServicer):
    def __init__(self, model):
        self.model = model
    
    def Predict(self, request, context):
        try:
            # Convert protobuf features to model input format
            features = self._extract_features(request.features)
            
            # Make prediction
            prediction = self.model.predict(features)
            confidence = self.model.predict_proba(features).max()
            
            return prediction_pb2.PredictResponse(
                prediction=float(prediction[0]),
                confidence=float(confidence),
                model_version=self.model.version
            )
        except Exception as e:
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(f"Prediction failed: {str(e)}")
            return prediction_pb2.PredictResponse()

def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    prediction_pb2_grpc.add_PredictionServiceServicer_to_server(
        PredictionServicer(model), server
    )
    listen_addr = '[::]:50051'
    server.add_insecure_port(listen_addr)
    server.start()
    server.wait_for_termination()
```

**Asynchronous Communication (Message Queue):**
```python
import asyncio
import aio_pika
import json

class AsyncMLProcessor:
    def __init__(self, rabbitmq_url):
        self.connection = None
        self.channel = None
        self.rabbitmq_url = rabbitmq_url
    
    async def connect(self):
        self.connection = await aio_pika.connect_robust(self.rabbitmq_url)
        self.channel = await self.connection.channel()
        
        # Declare queues
        self.training_queue = await self.channel.declare_queue('training_requests')
        self.prediction_queue = await self.channel.declare_queue('prediction_requests')
    
    async def publish_training_request(self, training_config):
        """Publish training request to queue"""
        message = aio_pika.Message(
            json.dumps(training_config).encode(),
            delivery_mode=aio_pika.DeliveryMode.PERSISTENT
        )
        await self.channel.default_exchange.publish(
            message, routing_key='training_requests'
        )
    
    async def process_training_requests(self):
        """Process training requests from queue"""
        async with self.training_queue.iterator() as queue_iter:
            async for message in queue_iter:
                async with message.process():
                    training_config = json.loads(message.body.decode())
                    
                    # Process training request
                    await self._handle_training_request(training_config)
    
    async def _handle_training_request(self, config):
        """Handle individual training request"""
        # Start training job
        training_job = await self._start_training_job(config)
        
        # Monitor training progress
        while not training_job.completed:
            await asyncio.sleep(30)
            status = await training_job.get_status()
            
            # Publish status updates
            await self._publish_training_status(config['request_id'], status)
```

================================================================================
5. RESOURCE MANAGEMENT AND AUTOSCALING
================================================================================

5.1 Horizontal Pod Autoscaler (HPA)
-----------------------------------
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-model-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-model-server
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
```

5.2 Vertical Pod Autoscaler (VPA)
---------------------------------
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: ml-model-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-model-server
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: model-server
      minAllowed:
        cpu: 100m
        memory: 512Mi
      maxAllowed:
        cpu: 2000m
        memory: 4Gi
      controlledResources: ["cpu", "memory"]
```

5.3 Custom Metrics for ML Workloads
-----------------------------------
```python
from prometheus_client import Gauge, Counter, Histogram, start_http_server
import time

# Define ML-specific metrics
model_inference_time = Histogram(
    'model_inference_duration_seconds',
    'Time spent on model inference',
    ['model_name', 'model_version']
)

model_accuracy = Gauge(
    'model_accuracy_score',
    'Current model accuracy',
    ['model_name', 'model_version']
)

prediction_requests = Counter(
    'prediction_requests_total',
    'Total prediction requests',
    ['model_name', 'status']
)

queue_length = Gauge(
    'prediction_queue_length',
    'Number of items in prediction queue'
)

class MLMetricsCollector:
    def __init__(self, model_name, model_version):
        self.model_name = model_name
        self.model_version = model_version
        
        # Start metrics server
        start_http_server(8000)
    
    def record_prediction(self, inference_time, success=True):
        """Record prediction metrics"""
        # Record inference time
        model_inference_time.labels(
            model_name=self.model_name,
            model_version=self.model_version
        ).observe(inference_time)
        
        # Record request count
        status = 'success' if success else 'error'
        prediction_requests.labels(
            model_name=self.model_name,
            status=status
        ).inc()
    
    def update_accuracy(self, accuracy_score):
        """Update model accuracy metric"""
        model_accuracy.labels(
            model_name=self.model_name,
            model_version=self.model_version
        ).set(accuracy_score)
    
    def update_queue_length(self, length):
        """Update queue length metric"""
        queue_length.set(length)
```

**Custom HPA with ML Metrics:**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-model-custom-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-model-server
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: External
    external:
      metric:
        name: prediction_queue_length
      target:
        type: AverageValue
        averageValue: "30"
  - type: External
    external:
      metric:
        name: model_inference_duration_seconds
        selector:
          matchLabels:
            model_name: "recommendation_model"
      target:
        type: AverageValue
        averageValue: "200m"  # 200ms average inference time
```

================================================================================
6. SERVICE MESH AND COMMUNICATION
================================================================================

6.1 Istio Service Mesh for ML
-----------------------------
**Traffic Management:**
```yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: ml-model-destination
spec:
  host: ml-model-service
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 10
        maxRequestsPerConnection: 2
    circuitBreaker:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ml-model-routing
spec:
  hosts:
  - ml-model-service
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: ml-model-service
        subset: v2
      weight: 100
  - route:
    - destination:
        host: ml-model-service
        subset: v1
      weight: 90
    - destination:
        host: ml-model-service
        subset: v2
      weight: 10
```

6.2 Security Policies
---------------------
```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: ml-api-access
spec:
  selector:
    matchLabels:
      app: ml-model-server
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/api-gateway"]
  - to:
    - operation:
        methods: ["POST"]
        paths: ["/predict"]
  - when:
    - key: request.headers[authorization]
      values: ["Bearer *"]
```

================================================================================
7. MONITORING AND OBSERVABILITY
================================================================================

7.1 Distributed Tracing
-----------------------
```python
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Configure tracing
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="jaeger-agent",
    agent_port=6831
)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

class TracedMLPipeline:
    def __init__(self):
        self.tracer = trace.get_tracer(__name__)
    
    def predict(self, features):
        with self.tracer.start_as_current_span("ml_prediction") as span:
            span.set_attribute("model.name", "recommendation_model")
            span.set_attribute("model.version", "v1.2.0")
            span.set_attribute("input.feature_count", len(features))
            
            # Preprocessing
            with self.tracer.start_as_current_span("preprocessing"):
                processed_features = self._preprocess(features)
            
            # Model inference
            with self.tracer.start_as_current_span("model_inference") as inference_span:
                prediction = self._model_predict(processed_features)
                inference_span.set_attribute("prediction.value", prediction)
            
            # Postprocessing
            with self.tracer.start_as_current_span("postprocessing"):
                result = self._postprocess(prediction)
            
            span.set_attribute("prediction.confidence", result.get('confidence', 0))
            return result
```

7.2 Logging Strategy
-------------------
```python
import structlog
import json
from pythonjsonlogger import jsonlogger

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

class MLLogger:
    def __init__(self, service_name, version):
        self.logger = logger.bind(
            service=service_name,
            version=version
        )
    
    def log_prediction(self, request_id, model_version, features, prediction, latency):
        """Log prediction with structured data"""
        self.logger.info(
            "prediction_completed",
            request_id=request_id,
            model_version=model_version,
            feature_count=len(features),
            prediction_value=prediction,
            latency_ms=latency * 1000,
            event_type="prediction"
        )
    
    def log_model_load(self, model_path, model_size_mb, load_time):
        """Log model loading event"""
        self.logger.info(
            "model_loaded",
            model_path=model_path,
            model_size_mb=model_size_mb,
            load_time_seconds=load_time,
            event_type="model_lifecycle"
        )
    
    def log_error(self, error_type, error_message, context=None):
        """Log errors with context"""
        self.logger.error(
            "error_occurred",
            error_type=error_type,
            error_message=error_message,
            context=context or {},
            event_type="error"
        )
```

================================================================================
8. PRODUCTION DEPLOYMENT PATTERNS
================================================================================

8.1 Multi-Environment Deployment
--------------------------------
```yaml
# Base configuration (kustomization.yaml)
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- deployment.yaml
- service.yaml
- configmap.yaml

# Production overlay
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: production

resources:
- ../../base

patchesStrategicMerge:
- deployment-patch.yaml

configMapGenerator:
- name: ml-config
  literals:
  - MODEL_REPLICAS=5
  - LOG_LEVEL=WARN
  - CACHE_SIZE=1000

# Production deployment patch
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-server
spec:
  replicas: 5
  template:
    spec:
      containers:
      - name: model-server
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
```

8.2 GitOps Deployment Pipeline
------------------------------
```yaml
# ArgoCD Application
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ml-model-production
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/company/ml-deployments
    targetRevision: HEAD
    path: production/ml-model
    kustomize:
      images:
      - ml-model-server:v1.2.0
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
```

**Helm Chart for ML Model:**
```yaml
# Chart.yaml
apiVersion: v2
name: ml-model
description: ML Model Deployment Chart
version: 0.1.0
appVersion: 1.0.0

# values.yaml
replicaCount: 3

image:
  repository: ml-model-server
  tag: latest
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8080

resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70

model:
  name: "recommendation-engine"
  version: "v1.0.0"
  path: "/models"
```

================================================================================
SUMMARY AND KEY TAKEAWAYS
================================================================================

Containerization and orchestration are essential for scalable ML systems:

**Containerization Benefits:**
- Environment consistency and reproducibility
- Simplified dependency management
- Easy scaling and deployment
- Resource isolation and optimization

**Kubernetes for ML:**
- Orchestrates complex ML workflows
- Provides autoscaling capabilities
- Enables microservices architecture
- Supports GPU workloads and specialized hardware

**Best Practices:**
- Use multi-stage builds for optimization
- Implement proper resource management
- Design for observability and monitoring
- Follow microservices patterns for complex systems
- Automate deployment with GitOps workflows

**Key Considerations:**
- Model artifact management and versioning
- Resource requirements for ML workloads
- Network communication patterns
- Security and access control
- Monitoring and debugging in distributed systems

Success requires balancing complexity with maintainability, ensuring that containerization and orchestration enhance rather than complicate the ML development lifecycle. 