CONTINUOUS INTEGRATION AND DEPLOYMENT FOR ML
============================================

Table of Contents:
1. CI/CD Fundamentals for ML Systems
2. Testing Strategies for ML Pipelines
3. Automated Model Validation
4. Deployment Automation and Strategies
5. GitOps for ML Workflows
6. Integration with MLOps Tools
7. Monitoring and Rollback Procedures
8. Best Practices and Implementation

================================================================================
1. CI/CD FUNDAMENTALS FOR ML SYSTEMS
================================================================================

1.1 ML-Specific CI/CD Challenges
--------------------------------
**Traditional Software vs ML Systems:**
```
Traditional CI/CD:
Code → Build → Test → Deploy

ML CI/CD:
[Code + Data + Model] → Build → [Test + Validate] → Deploy → Monitor
```

**Key Differences:**
- Data as a dependency (versioning, quality)
- Model artifacts and their dependencies
- Performance validation beyond functional testing
- Gradual rollouts and A/B testing requirements
- Continuous monitoring and retraining needs

1.2 ML Pipeline Architecture
----------------------------
```
Data Pipeline CI/CD:
Data Sources → Validation → Processing → Feature Store → Trigger Model Pipeline

Model Pipeline CI/CD:
Feature Store → Training → Validation → Registry → Deployment → Monitoring
```

**Pipeline Triggers:**
- Code changes (training scripts, preprocessing)
- Data changes (new data availability, schema changes)
- Schedule-based (periodic retraining)
- Performance degradation (automatic retraining)

1.3 Multi-Environment Strategy
-----------------------------
```python
class MLEnvironmentConfig:
    def __init__(self, env_name):
        self.env_name = env_name
        self.config = self._load_config(env_name)
    
    def _load_config(self, env_name):
        configs = {
            'dev': {
                'data_source': 'sample_data',
                'model_validation_threshold': 0.7,
                'deployment_strategy': 'blue_green',
                'monitoring_level': 'basic'
            },
            'staging': {
                'data_source': 'production_subset',
                'model_validation_threshold': 0.85,
                'deployment_strategy': 'canary',
                'monitoring_level': 'detailed'
            },
            'production': {
                'data_source': 'production_full',
                'model_validation_threshold': 0.9,
                'deployment_strategy': 'rolling',
                'monitoring_level': 'comprehensive'
            }
        }
        return configs[env_name]
```

================================================================================
2. TESTING STRATEGIES FOR ML PIPELINES
================================================================================

2.1 Testing Pyramid for ML
--------------------------
**Unit Tests:**
```python
import pytest
import pandas as pd
import numpy as np

class TestFeatureEngineering:
    def test_handle_missing_values(self):
        """Test missing value handling"""
        data = pd.DataFrame({'feature1': [1, np.nan, 3, np.nan, 5]})
        processed = handle_missing_values(data)
        assert not processed.isnull().any().any()
    
    def test_feature_scaling(self):
        """Test feature scaling maintains relationships"""
        data = np.array([[1, 2], [3, 4], [5, 6]])
        scaled = scale_features(data)
        
        # Test that relative relationships are maintained
        assert np.allclose(scaled[1] / scaled[0], data[1] / data[0])
    
    def test_categorical_encoding(self):
        """Test categorical encoding"""
        categories = ['A', 'B', 'C', 'A', 'B']
        encoded = encode_categorical(categories)
        
        assert len(encoded.shape) == 2  # One-hot encoded
        assert encoded.shape[1] == 3   # Three categories
```

**Integration Tests:**
```python
class TestMLPipeline:
    def test_end_to_end_pipeline(self):
        """Test complete ML pipeline"""
        # Load test data
        test_data = load_test_dataset()
        
        # Run pipeline
        pipeline = MLPipeline()
        result = pipeline.run(test_data)
        
        # Validate outputs
        assert 'predictions' in result
        assert 'model_metrics' in result
        assert result['model_metrics']['accuracy'] > 0.7
    
    def test_data_pipeline_integration(self):
        """Test data pipeline integration"""
        raw_data = simulate_raw_data()
        processed_data = data_pipeline.process(raw_data)
        
        # Validate schema
        expected_columns = ['feature1', 'feature2', 'target']
        assert all(col in processed_data.columns for col in expected_columns)
        
        # Validate data quality
        assert processed_data.isnull().sum().sum() == 0
```

2.2 Property-Based Testing
--------------------------
```python
from hypothesis import given, strategies as st

class TestModelRobustness:
    @given(st.lists(st.floats(min_value=0, max_value=100), min_size=5, max_size=10))
    def test_model_prediction_bounds(self, features):
        """Test model predictions are within expected bounds"""
        prediction = model.predict([features])
        assert 0 <= prediction[0] <= 1  # For probability predictions
    
    @given(st.data())
    def test_feature_engineering_invariants(self, data):
        """Test feature engineering invariants"""
        original_data = data.draw(generate_dataframe_strategy())
        processed_data = feature_engineer.transform(original_data)
        
        # Invariant: number of rows should be preserved
        assert len(processed_data) == len(original_data)
        
        # Invariant: no infinite values should be created
        assert not np.isinf(processed_data.select_dtypes(include=[np.number])).any().any()
```

2.3 Model Validation Tests
--------------------------
```python
class TestModelValidation:
    def test_model_performance_threshold(self):
        """Test model meets minimum performance requirements"""
        test_data, test_labels = load_test_data()
        predictions = model.predict(test_data)
        
        accuracy = accuracy_score(test_labels, predictions)
        precision = precision_score(test_labels, predictions, average='weighted')
        recall = recall_score(test_labels, predictions, average='weighted')
        
        assert accuracy >= 0.85, f"Accuracy {accuracy} below threshold"
        assert precision >= 0.8, f"Precision {precision} below threshold"
        assert recall >= 0.8, f"Recall {recall} below threshold"
    
    def test_model_fairness(self):
        """Test model fairness across demographic groups"""
        test_data, test_labels, demographics = load_test_data_with_demographics()
        predictions = model.predict(test_data)
        
        # Calculate performance by demographic group
        for group in demographics['group'].unique():
            group_mask = demographics['group'] == group
            group_accuracy = accuracy_score(
                test_labels[group_mask], 
                predictions[group_mask]
            )
            assert group_accuracy >= 0.75, f"Accuracy for {group} below threshold"
    
    def test_model_inference_time(self):
        """Test model inference performance"""
        test_sample = generate_test_sample()
        
        start_time = time.time()
        prediction = model.predict(test_sample)
        inference_time = time.time() - start_time
        
        assert inference_time < 0.1, f"Inference time {inference_time}s too slow"
```

================================================================================
3. AUTOMATED MODEL VALIDATION
================================================================================

3.1 Model Validation Pipeline
-----------------------------
```python
class ModelValidator:
    def __init__(self, validation_config):
        self.config = validation_config
        self.validators = self._setup_validators()
    
    def validate_model(self, model, test_data, baseline_model=None):
        """Run comprehensive model validation"""
        validation_results = {}
        
        for validator_name, validator in self.validators.items():
            try:
                result = validator.validate(model, test_data, baseline_model)
                validation_results[validator_name] = result
            except Exception as e:
                validation_results[validator_name] = {
                    'status': 'failed',
                    'error': str(e)
                }
        
        # Aggregate results
        overall_status = self._aggregate_validation_results(validation_results)
        
        return {
            'overall_status': overall_status,
            'detailed_results': validation_results,
            'timestamp': datetime.now()
        }
    
    def _setup_validators(self):
        return {
            'performance': PerformanceValidator(self.config['performance']),
            'bias': BiasValidator(self.config['bias']),
            'robustness': RobustnessValidator(self.config['robustness']),
            'drift': DriftValidator(self.config['drift'])
        }

class PerformanceValidator:
    def __init__(self, config):
        self.thresholds = config['thresholds']
    
    def validate(self, model, test_data, baseline_model=None):
        X_test, y_test = test_data
        predictions = model.predict(X_test)
        
        metrics = {
            'accuracy': accuracy_score(y_test, predictions),
            'precision': precision_score(y_test, predictions, average='weighted'),
            'recall': recall_score(y_test, predictions, average='weighted')
        }
        
        validation_results = {}
        for metric, value in metrics.items():
            threshold = self.thresholds.get(metric, 0)
            validation_results[metric] = {
                'value': value,
                'threshold': threshold,
                'passed': value >= threshold
            }
        
        # Compare with baseline if provided
        if baseline_model:
            baseline_predictions = baseline_model.predict(X_test)
            baseline_accuracy = accuracy_score(y_test, baseline_predictions)
            
            validation_results['baseline_comparison'] = {
                'improvement': metrics['accuracy'] - baseline_accuracy,
                'passed': metrics['accuracy'] >= baseline_accuracy
            }
        
        return validation_results
```

3.2 Drift Detection
-------------------
```python
class DriftValidator:
    def __init__(self, config):
        self.drift_threshold = config.get('drift_threshold', 0.05)
        self.reference_data = None
    
    def set_reference_data(self, reference_data):
        """Set reference data for drift detection"""
        self.reference_data = reference_data
    
    def validate(self, model, test_data, baseline_model=None):
        """Detect data drift between reference and test data"""
        if self.reference_data is None:
            return {'status': 'skipped', 'reason': 'No reference data'}
        
        X_test, _ = test_data
        drift_results = {}
        
        # Statistical drift detection
        for column in X_test.columns:
            if column in self.reference_data.columns:
                drift_score = self._calculate_drift_score(
                    self.reference_data[column], 
                    X_test[column]
                )
                
                drift_results[column] = {
                    'drift_score': drift_score,
                    'threshold': self.drift_threshold,
                    'drift_detected': drift_score > self.drift_threshold
                }
        
        overall_drift = any(result['drift_detected'] for result in drift_results.values())
        
        return {
            'overall_drift_detected': overall_drift,
            'column_results': drift_results
        }
    
    def _calculate_drift_score(self, reference_col, test_col):
        """Calculate drift score using KS test"""
        from scipy.stats import ks_2samp
        
        if reference_col.dtype in ['int64', 'float64']:
            # Numerical feature - use KS test
            statistic, p_value = ks_2samp(reference_col, test_col)
            return statistic
        else:
            # Categorical feature - use chi-square test
            return self._categorical_drift_score(reference_col, test_col)
    
    def _categorical_drift_score(self, reference_col, test_col):
        """Calculate categorical drift using distribution comparison"""
        ref_dist = reference_col.value_counts(normalize=True)
        test_dist = test_col.value_counts(normalize=True)
        
        # Align distributions
        all_categories = set(ref_dist.index) | set(test_dist.index)
        ref_aligned = ref_dist.reindex(all_categories, fill_value=0)
        test_aligned = test_dist.reindex(all_categories, fill_value=0)
        
        # Calculate Jensen-Shannon divergence
        return self._jensen_shannon_divergence(ref_aligned, test_aligned)
```

================================================================================
4. DEPLOYMENT AUTOMATION AND STRATEGIES
================================================================================

4.1 Deployment Strategies
-------------------------
**Blue-Green Deployment:**
```python
class BlueGreenDeployment:
    def __init__(self, load_balancer, blue_env, green_env):
        self.load_balancer = load_balancer
        self.blue_env = blue_env
        self.green_env = green_env
        self.current_env = 'blue'
    
    def deploy(self, new_model_version):
        """Deploy new model using blue-green strategy"""
        
        # Determine target environment
        target_env = 'green' if self.current_env == 'blue' else 'blue'
        target = self.green_env if target_env == 'green' else self.blue_env
        
        try:
            # Deploy to inactive environment
            target.deploy_model(new_model_version)
            
            # Health check
            if not self._health_check(target):
                raise DeploymentError("Health check failed")
            
            # Validation tests
            if not self._run_validation_tests(target):
                raise DeploymentError("Validation tests failed")
            
            # Switch traffic
            self.load_balancer.switch_traffic(target_env)
            self.current_env = target_env
            
            return {'status': 'success', 'new_environment': target_env}
            
        except Exception as e:
            # Rollback on failure
            self._rollback(e)
            raise e
    
    def _health_check(self, environment):
        """Perform health check on environment"""
        try:
            response = environment.health_check()
            return response.status_code == 200
        except:
            return False
    
    def _run_validation_tests(self, environment):
        """Run validation tests against new deployment"""
        test_cases = [
            {'input': [1, 2, 3], 'expected_type': float},
            {'input': [4, 5, 6], 'expected_type': float}
        ]
        
        for test_case in test_cases:
            try:
                prediction = environment.predict(test_case['input'])
                if not isinstance(prediction, test_case['expected_type']):
                    return False
            except:
                return False
        
        return True
```

**Canary Deployment:**
```python
class CanaryDeployment:
    def __init__(self, traffic_manager):
        self.traffic_manager = traffic_manager
        self.canary_percentage = 5  # Start with 5% traffic
    
    def deploy(self, new_model_version):
        """Deploy using canary strategy"""
        
        # Deploy canary version
        canary_endpoint = self._deploy_canary(new_model_version)
        
        # Gradual traffic increase
        for percentage in [5, 10, 25, 50, 100]:
            self.traffic_manager.set_canary_traffic(percentage)
            
            # Monitor for specified duration
            monitoring_result = self._monitor_canary(duration_minutes=30)
            
            if not monitoring_result['healthy']:
                self._rollback_canary()
                raise DeploymentError(f"Canary failed at {percentage}% traffic")
        
        # Full deployment successful
        self.traffic_manager.promote_canary_to_production()
        return {'status': 'success', 'deployment_type': 'canary'}
    
    def _monitor_canary(self, duration_minutes):
        """Monitor canary deployment health"""
        start_time = datetime.now()
        end_time = start_time + timedelta(minutes=duration_minutes)
        
        while datetime.now() < end_time:
            metrics = self.traffic_manager.get_canary_metrics()
            
            # Check error rate
            if metrics['error_rate'] > 0.05:  # 5% error threshold
                return {'healthy': False, 'reason': 'high_error_rate'}
            
            # Check latency
            if metrics['p99_latency'] > 1000:  # 1 second threshold
                return {'healthy': False, 'reason': 'high_latency'}
            
            time.sleep(60)  # Check every minute
        
        return {'healthy': True}
```

4.2 GitOps Workflow
-------------------
```yaml
# .github/workflows/ml-pipeline.yml
name: ML Pipeline CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.8
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run unit tests
      run: pytest tests/unit/ -v
    
    - name: Run integration tests
      run: pytest tests/integration/ -v
    
    - name: Data validation
      run: python scripts/validate_data.py
    
  train-and-validate:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Train model
      run: |
        python src/train.py --config config/training.yaml
    
    - name: Validate model
      run: |
        python src/validate_model.py --model-path models/latest/
    
    - name: Register model
      if: success()
      run: |
        python src/register_model.py --model-path models/latest/
  
  deploy-staging:
    needs: train-and-validate
    runs-on: ubuntu-latest
    steps:
    - name: Deploy to staging
      run: |
        kubectl apply -f k8s/staging/
        kubectl rollout status deployment/ml-model-staging
    
    - name: Run staging tests
      run: |
        python tests/staging/test_deployment.py
  
  deploy-production:
    needs: deploy-staging
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment: production
    steps:
    - name: Deploy to production
      run: |
        python scripts/deploy_production.py --strategy canary
```

================================================================================
5. GITOPS FOR ML WORKFLOWS
================================================================================

5.1 Infrastructure as Code
--------------------------
```yaml
# k8s/model-serving.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-serving
  labels:
    app: ml-model
    version: v1.0.0
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-model
  template:
    metadata:
      labels:
        app: ml-model
    spec:
      containers:
      - name: model-server
        image: ml-model:v1.0.0
        ports:
        - containerPort: 8080
        env:
        - name: MODEL_PATH
          value: "/models/current"
        - name: LOG_LEVEL
          value: "INFO"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
```

**Config Management:**
```python
# config/deployment_config.py
from dataclasses import dataclass
from typing import Dict, Any

@dataclass
class DeploymentConfig:
    environment: str
    model_version: str
    replica_count: int
    resource_limits: Dict[str, str]
    monitoring_config: Dict[str, Any]
    
    @classmethod
    def from_env(cls, env_name: str):
        configs = {
            'dev': cls(
                environment='dev',
                model_version='latest',
                replica_count=1,
                resource_limits={'memory': '512Mi', 'cpu': '250m'},
                monitoring_config={'metrics_enabled': True, 'logging_level': 'DEBUG'}
            ),
            'staging': cls(
                environment='staging',
                model_version='candidate',
                replica_count=2,
                resource_limits={'memory': '1Gi', 'cpu': '500m'},
                monitoring_config={'metrics_enabled': True, 'logging_level': 'INFO'}
            ),
            'production': cls(
                environment='production',
                model_version='stable',
                replica_count=5,
                resource_limits={'memory': '2Gi', 'cpu': '1000m'},
                monitoring_config={'metrics_enabled': True, 'logging_level': 'WARN'}
            )
        }
        return configs[env_name]
```

================================================================================
6. INTEGRATION WITH MLOPS TOOLS
================================================================================

6.1 MLflow Integration
----------------------
```python
# src/training_pipeline.py
import mlflow
import mlflow.sklearn
from mlflow.models.signature import infer_signature

class TrainingPipeline:
    def __init__(self, config):
        self.config = config
        mlflow.set_tracking_uri(config['mlflow_tracking_uri'])
        mlflow.set_experiment(config['experiment_name'])
    
    def run(self):
        with mlflow.start_run() as run:
            # Log parameters
            mlflow.log_params(self.config['model_params'])
            
            # Train model
            model = self.train_model()
            
            # Evaluate model
            metrics = self.evaluate_model(model)
            mlflow.log_metrics(metrics)
            
            # Log model with signature
            signature = infer_signature(self.X_train, model.predict(self.X_train))
            mlflow.sklearn.log_model(
                model,
                "model",
                signature=signature,
                registered_model_name=self.config['model_name']
            )
            
            # Trigger deployment if validation passes
            if self.validate_model(model, metrics):
                self.trigger_deployment(run.info.run_id)
    
    def trigger_deployment(self, run_id):
        """Trigger deployment pipeline"""
        deployment_config = {
            'run_id': run_id,
            'environment': 'staging',
            'deployment_strategy': 'blue_green'
        }
        
        # Call deployment API or trigger CI/CD pipeline
        requests.post(
            f"{self.config['deployment_api']}/deploy",
            json=deployment_config
        )
```

6.2 Kubeflow Pipelines Integration
----------------------------------
```python
import kfp
from kfp import dsl
from kfp.components import func_to_container_op

@func_to_container_op
def train_model_component(
    data_path: str,
    model_output_path: str,
    hyperparameters: dict
) -> str:
    """Training component for Kubeflow Pipeline"""
    import pickle
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    
    # Load data
    data = pd.read_csv(data_path)
    X = data.drop('target', axis=1)
    y = data['target']
    
    # Train model
    model = RandomForestClassifier(**hyperparameters)
    model.fit(X, y)
    
    # Save model
    with open(model_output_path, 'wb') as f:
        pickle.dump(model, f)
    
    return model_output_path

@func_to_container_op
def validate_model_component(
    model_path: str,
    test_data_path: str,
    metrics_output_path: str
) -> str:
    """Validation component for Kubeflow Pipeline"""
    import pickle
    import json
    import pandas as pd
    from sklearn.metrics import accuracy_score, precision_score, recall_score
    
    # Load model and test data
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    test_data = pd.read_csv(test_data_path)
    X_test = test_data.drop('target', axis=1)
    y_test = test_data['target']
    
    # Make predictions and calculate metrics
    predictions = model.predict(X_test)
    metrics = {
        'accuracy': accuracy_score(y_test, predictions),
        'precision': precision_score(y_test, predictions, average='weighted'),
        'recall': recall_score(y_test, predictions, average='weighted')
    }
    
    # Save metrics
    with open(metrics_output_path, 'w') as f:
        json.dump(metrics, f)
    
    return metrics_output_path

@dsl.pipeline(
    name='ML Training Pipeline',
    description='End-to-end ML training pipeline'
)
def ml_training_pipeline(
    data_path: str = '/data/train.csv',
    test_data_path: str = '/data/test.csv',
    hyperparameters: dict = {'n_estimators': 100, 'max_depth': 10}
):
    # Training step
    train_task = train_model_component(
        data_path=data_path,
        model_output_path='/models/trained_model.pkl',
        hyperparameters=hyperparameters
    )
    
    # Validation step
    validate_task = validate_model_component(
        model_path=train_task.output,
        test_data_path=test_data_path,
        metrics_output_path='/metrics/validation_metrics.json'
    ).after(train_task)
    
    # Deployment step (conditional on validation)
    deploy_task = deploy_model_component(
        model_path=train_task.output,
        metrics_path=validate_task.output
    ).after(validate_task)
```

================================================================================
7. MONITORING AND ROLLBACK PROCEDURES
================================================================================

7.1 Deployment Monitoring
-------------------------
```python
class DeploymentMonitor:
    def __init__(self, metrics_backend, alert_manager):
        self.metrics_backend = metrics_backend
        self.alert_manager = alert_manager
        self.monitoring_rules = self._setup_monitoring_rules()
    
    def monitor_deployment(self, deployment_id, duration_minutes=60):
        """Monitor deployment health for specified duration"""
        start_time = datetime.now()
        end_time = start_time + timedelta(minutes=duration_minutes)
        
        monitoring_results = []
        
        while datetime.now() < end_time:
            # Collect metrics
            current_metrics = self._collect_metrics(deployment_id)
            
            # Evaluate rules
            rule_results = self._evaluate_rules(current_metrics)
            
            # Log results
            monitoring_results.append({
                'timestamp': datetime.now(),
                'metrics': current_metrics,
                'rule_results': rule_results
            })
            
            # Check for failures
            if any(not result['passed'] for result in rule_results.values()):
                self._trigger_alert(deployment_id, rule_results)
                return {'status': 'failed', 'results': monitoring_results}
            
            time.sleep(60)  # Check every minute
        
        return {'status': 'success', 'results': monitoring_results}
    
    def _setup_monitoring_rules(self):
        return {
            'error_rate': {
                'threshold': 0.05,
                'operator': 'less_than',
                'description': 'Error rate should be less than 5%'
            },
            'latency_p99': {
                'threshold': 1000,
                'operator': 'less_than',
                'description': 'P99 latency should be less than 1000ms'
            },
            'throughput': {
                'threshold': 100,
                'operator': 'greater_than',
                'description': 'Throughput should be greater than 100 RPS'
            }
        }
```

7.2 Automated Rollback
----------------------
```python
class AutoRollbackManager:
    def __init__(self, deployment_manager, monitoring_system):
        self.deployment_manager = deployment_manager
        self.monitoring_system = monitoring_system
        self.rollback_conditions = self._setup_rollback_conditions()
    
    def setup_auto_rollback(self, deployment_id):
        """Setup automatic rollback monitoring"""
        
        # Start monitoring thread
        monitor_thread = threading.Thread(
            target=self._monitor_for_rollback,
            args=(deployment_id,)
        )
        monitor_thread.daemon = True
        monitor_thread.start()
    
    def _monitor_for_rollback(self, deployment_id):
        """Monitor deployment and trigger rollback if needed"""
        rollback_triggered = False
        monitoring_duration = 60  # Monitor for 60 minutes
        
        for minute in range(monitoring_duration):
            if rollback_triggered:
                break
                
            metrics = self.monitoring_system.get_deployment_metrics(deployment_id)
            
            for condition_name, condition in self.rollback_conditions.items():
                if self._evaluate_rollback_condition(metrics, condition):
                    logging.critical(f"Rollback condition {condition_name} triggered for {deployment_id}")
                    self._execute_rollback(deployment_id, condition_name)
                    rollback_triggered = True
                    break
            
            time.sleep(60)
    
    def _execute_rollback(self, deployment_id, trigger_reason):
        """Execute rollback procedure"""
        try:
            # Get previous stable version
            previous_version = self.deployment_manager.get_previous_version(deployment_id)
            
            # Perform rollback
            rollback_result = self.deployment_manager.rollback(
                deployment_id, 
                previous_version
            )
            
            # Notify stakeholders
            self._send_rollback_notification(deployment_id, trigger_reason, rollback_result)
            
        except Exception as e:
            logging.error(f"Rollback failed for {deployment_id}: {e}")
            self._send_rollback_failure_alert(deployment_id, e)
    
    def _setup_rollback_conditions(self):
        return {
            'high_error_rate': {
                'metric': 'error_rate',
                'threshold': 0.1,
                'duration_minutes': 5,
                'operator': 'greater_than'
            },
            'extreme_latency': {
                'metric': 'latency_p99',
                'threshold': 5000,
                'duration_minutes': 3,
                'operator': 'greater_than'
            },
            'zero_throughput': {
                'metric': 'throughput',
                'threshold': 1,
                'duration_minutes': 2,
                'operator': 'less_than'
            }
        }
```

================================================================================
8. BEST PRACTICES AND IMPLEMENTATION
================================================================================

8.1 CI/CD Pipeline Best Practices
---------------------------------
**Pipeline Design Principles:**
```yaml
# Best practices checklist
pipeline_design:
  - fail_fast: "Fail early in the pipeline to save resources"
  - parallel_execution: "Run independent steps in parallel"
  - caching: "Cache dependencies and intermediate results"
  - environment_parity: "Keep dev/staging/prod environments similar"
  - rollback_capability: "Always maintain rollback capability"

testing_strategy:
  - unit_tests: "Test individual components"
  - integration_tests: "Test component interactions"
  - model_validation: "Validate model performance and fairness"
  - load_testing: "Test performance under expected load"
  - security_scanning: "Scan for vulnerabilities"

deployment_strategy:
  - blue_green: "Zero-downtime deployments"
  - canary: "Gradual rollout with monitoring"
  - feature_flags: "Control feature rollout independently"
  - monitoring: "Comprehensive monitoring from deployment"
  - documentation: "Document deployment procedures"
```

8.2 Implementation Checklist
----------------------------
**Pre-Implementation:**
- [ ] Define environment promotion strategy
- [ ] Set up model validation thresholds
- [ ] Implement comprehensive testing suite
- [ ] Design monitoring and alerting rules
- [ ] Plan rollback procedures

**Pipeline Implementation:**
- [ ] Automated data validation
- [ ] Model training automation
- [ ] Model validation gates
- [ ] Deployment automation
- [ ] Performance monitoring

**Post-Implementation:**
- [ ] Monitor pipeline performance
- [ ] Regularly review and update thresholds
- [ ] Conduct rollback drills
- [ ] Gather team feedback
- [ ] Continuous improvement

================================================================================
SUMMARY AND KEY TAKEAWAYS
================================================================================

CI/CD for ML systems requires specialized approaches beyond traditional software:

**Key Considerations:**
- Data as a dependency requiring validation and versioning
- Model-specific testing including performance and fairness
- Gradual deployment strategies for risk mitigation
- Comprehensive monitoring and automated rollback

**Essential Components:**
- Multi-layer testing strategy (unit, integration, model validation)
- Automated model validation with performance thresholds
- Deployment strategies (blue-green, canary) with monitoring
- GitOps workflows for infrastructure and configuration management

**Success Factors:**
- Balance automation with human oversight
- Implement comprehensive monitoring from day one
- Design for rollback and disaster recovery
- Maintain environment parity across dev/staging/production
- Continuously monitor and improve pipeline performance

The goal is to enable rapid, reliable deployment of ML models while maintaining quality and reducing risk through automation and monitoring. 